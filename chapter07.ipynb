{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Agents\n",
    "\n",
    "## Planning, Delegation, Reasoning, and Reflection\n",
    "\n",
    "This notebook explores the architecture of **deep agents** — agents that go beyond single-turn responses to plan, delegate, reason through multi-step problems, reflect on their own progress, and backtrack when stuck.\n",
    "\n",
    "We cover the four key elements that transform a basic agent into a deep agent:\n",
    "\n",
    "```\n",
    "    ┌─────────────────────────────────────────────────────────────┐\n",
    "    │                    Deep Agent Coordinator                   │\n",
    "    │                                                             │\n",
    "    │   ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │\n",
    "    │   │ Planning │  │ Context  │  │Delegation│  │Reflection│    │\n",
    "    │   │  & Task  │  │ Manage-  │  │& Sub-    │  │& Back-   │    │\n",
    "    │   │  Decomp  │  │  ment    │  │ agents   │  │ tracking │    │\n",
    "    │   └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘    │\n",
    "    │        │             │             │             │          │\n",
    "    │        v             v             v             v          │\n",
    "    │   Break goals   Prioritize    Spawn focused  Evaluate       │\n",
    "    │   into tasks    & compress    workers for    progress &     │\n",
    "    │   with deps     context       subtasks       adjust plans   │\n",
    "    └─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Each section builds up reusable classes and functions that come together in the final **\"Bringing It All Together\"** section, where we build a complete Deep Research Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Make sure you have the required packages installed:\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, TypedDict\n",
    "from enum import Enum\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning and Task Decomposition\n",
    "\n",
    "Every complex project starts with a plan. Without explicit planning, agents tend to tackle whatever seems most immediate, which often isn't what's most important. Planning gives agents direction.\n",
    "\n",
    "The core idea: take a high-level goal and break it down into a sequence of smaller, achievable tasks with explicit dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: str\n",
    "    description: str\n",
    "    depends_on: list[str] = []\n",
    "    estimated_hours: float\n",
    "    status: str = \"pending\"\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    goal: str\n",
    "    tasks: list[Task]\n",
    "\n",
    "offsite_plan = Plan(\n",
    "    goal=\"Plan team offsite for 30 people\",\n",
    "    tasks=[\n",
    "        Task(id=\"t1\", description=\"Determine budget and dates\", depends_on=[], estimated_hours=1),\n",
    "        Task(id=\"t2\", description=\"Survey team for location preferences\", depends_on=[\"t1\"], estimated_hours=0.5),\n",
    "        Task(id=\"t3\", description=\"Research venue options matching budget\", depends_on=[\"t1\", \"t2\"], estimated_hours=3),\n",
    "        Task(id=\"t4\", description=\"Get quotes from top 3 venues\", depends_on=[\"t3\"], estimated_hours=2),\n",
    "        Task(id=\"t5\", description=\"Book selected venue\", depends_on=[\"t4\"], estimated_hours=0.5),\n",
    "        Task(id=\"t6\", description=\"Plan agenda and activities\", depends_on=[\"t5\"], estimated_hours=4),\n",
    "        Task(id=\"t7\", description=\"Arrange transportation and logistics\", depends_on=[\"t5\"], estimated_hours=2),\n",
    "        Task(id=\"t8\", description=\"Send invitations with details\", depends_on=[\"t6\", \"t7\"], estimated_hours=1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the plan\n",
    "print(f\"Goal: {offsite_plan.goal}\")\n",
    "print(f\"Tasks: {len(offsite_plan.tasks)}\")\n",
    "for task in offsite_plan.tasks:\n",
    "    deps = f\" (depends on: {', '.join(task.depends_on)})\" if task.depends_on else \"\"\n",
    "    print(f\"  {task.id}: {task.description}{deps} [{task.estimated_hours}h]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Planning vs. Backward Chaining\n",
    "\n",
    "There are two main strategies for generating plans:\n",
    "\n",
    "- **Forward planning** starts from the current state and asks, \"What's the first thing I need to do? Then what?\"\n",
    "- **Backward chaining** starts from the goal and works backwards: \"To achieve X, what needs to be true? To make that true, what needs to happen first?\"\n",
    "\n",
    "For learning-related goals, backward chaining often works beautifully. The implementation below adds a `max_depth` parameter to prevent infinite recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_chain(goal: str, llm, max_depth: int = 3, depth: int = 0) -> list[str]:\n",
    "    \"\"\"Generate prerequisites by working backward from goal.\n",
    "\n",
    "    Args:\n",
    "        goal: The goal to decompose.\n",
    "        llm: The language model to use for generating prerequisites.\n",
    "        max_depth: Maximum recursion depth to prevent infinite loops.\n",
    "        depth: Current recursion depth (internal use).\n",
    "\n",
    "    Returns:\n",
    "        Ordered list of tasks from earliest prerequisite to final goal.\n",
    "    \"\"\"\n",
    "    if depth >= max_depth:\n",
    "        return [goal]\n",
    "\n",
    "    prompt = f\"\"\"Given this goal: {goal}\n",
    "    \n",
    "What must be true or completed IMMEDIATELY BEFORE this goal can be achieved?\n",
    "List only the direct prerequisites, not earlier steps.\n",
    "Be specific and actionable.\n",
    "\n",
    "Format: One prerequisite per line, no numbering.\"\"\"\n",
    "    \n",
    "    prerequisites = llm.invoke(prompt).content.strip().split(\"\\n\")\n",
    "    \n",
    "    all_tasks = []\n",
    "    for prereq in prerequisites:\n",
    "        prereq = prereq.strip()\n",
    "        if prereq:\n",
    "            # Recursively find prerequisites of prerequisites\n",
    "            # Note: backward_chain already includes prereq as its final element\n",
    "            sub_prereqs = backward_chain(prereq, llm, max_depth=max_depth, depth=depth + 1)\n",
    "            all_tasks.extend(sub_prereqs)\n",
    "    \n",
    "    all_tasks.append(goal)\n",
    "    return all_tasks\n",
    "\n",
    "# Test it with a concrete goal\n",
    "tasks = backward_chain(\"Pass the AWS Solutions Architect certification exam\", llm, max_depth=2)\n",
    "print(\"Backward-chained tasks (from earliest to final goal):\")\n",
    "for i, task in enumerate(tasks, 1):\n",
    "    print(f\"  {i}. {task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Management in Deep Agents\n",
    "\n",
    "When an agent operates over extended periods, information accumulates. Eventually you hit the context window limit, and even before that, performance degrades as important information gets lost in noise.\n",
    "\n",
    "Deep agents need explicit strategies for managing context: **prioritization** (what's important right now?), **compression** (summarize what you can), and **progressive summarization** (maintain multiple levels of detail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant(item: dict, current_task: str) -> bool:\n",
    "    \"\"\"Check if a context item is relevant to the current task.\n",
    "\n",
    "    Uses simple keyword overlap between the item's content/description\n",
    "    and the current task description.\n",
    "    \"\"\"\n",
    "    item_text = json.dumps(item).lower()\n",
    "    task_words = set(current_task.lower().split())\n",
    "    # Remove common stop words\n",
    "    stop_words = {\"the\", \"a\", \"an\", \"is\", \"to\", \"and\", \"or\", \"for\", \"in\", \"on\", \"of\", \"with\"}\n",
    "    task_words -= stop_words\n",
    "    overlap = sum(1 for word in task_words if word in item_text)\n",
    "    return overlap >= 2  # At least 2 keyword matches\n",
    "\n",
    "\n",
    "def prioritize_context(context_items: list[dict], current_task: str) -> list[dict]:\n",
    "    \"\"\"Rank context items by relevance to current task.\"\"\"\n",
    "    \n",
    "    categories = {\n",
    "        \"goal\": 10,          # Original objective - always keep\n",
    "        \"plan\": 9,           # Current plan - critical\n",
    "        \"progress\": 8,       # What's done/pending - important\n",
    "        \"findings\": 7,       # Key results - valuable\n",
    "        \"recent\": 6,         # Last few messages - continuity\n",
    "        \"reference\": 3,      # Supporting details - nice to have\n",
    "        \"historical\": 1,     # Old interactions - low priority\n",
    "    }\n",
    "    \n",
    "    scored = []\n",
    "    for item in context_items:\n",
    "        base_score = categories.get(item.get(\"type\"), 2)\n",
    "        \n",
    "        # Boost if relevant to current task\n",
    "        if is_relevant(item, current_task):\n",
    "            base_score *= 1.5\n",
    "            \n",
    "        scored.append((base_score, item))\n",
    "    \n",
    "    # Sort by score descending, return items\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [item for _, item in scored]\n",
    "\n",
    "\n",
    "# Demonstrate context prioritization\n",
    "context_items = [\n",
    "    {\"type\": \"goal\", \"content\": \"Research best practices for fine-tuning LLMs\"},\n",
    "    {\"type\": \"historical\", \"content\": \"Last week we discussed prompt engineering basics\"},\n",
    "    {\"type\": \"findings\", \"content\": \"LoRA reduces fine-tuning compute by 10x\"},\n",
    "    {\"type\": \"recent\", \"content\": \"User asked about dataset preparation for fine-tuning\"},\n",
    "    {\"type\": \"reference\", \"content\": \"Python documentation for asyncio module\"},\n",
    "    {\"type\": \"progress\", \"content\": \"Completed literature review on fine-tuning methods\"},\n",
    "    {\"type\": \"plan\", \"content\": \"Next: evaluate LoRA vs full fine-tuning on sample dataset\"},\n",
    "]\n",
    "\n",
    "prioritized = prioritize_context(context_items, \"evaluate fine-tuning approaches for LLMs\")\n",
    "print(\"Prioritized context (highest priority first):\")\n",
    "for item in prioritized:\n",
    "    print(f\"  [{item['type']:>12}] {item['content'][:70]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Compression and Summarization\n",
    "\n",
    "When you can't keep everything, summarize. The trick is summarizing at the right level of detail: enough to be useful, concise enough to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compress_context(full_context: str, max_tokens: int, llm) -> str:\n",
    "    \"\"\"Compress context while preserving essential information.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Summarize the following context, preserving:\n",
    "1. The main goal or objective\n",
    "2. Key decisions made and their rationale\n",
    "3. Important findings or results\n",
    "4. Current status and next steps\n",
    "5. Any critical constraints or requirements\n",
    "\n",
    "Remove:\n",
    "- Redundant information\n",
    "- Detailed intermediate reasoning\n",
    "- Verbose explanations where a summary suffices\n",
    "\n",
    "Context to compress:\n",
    "{full_context}\n",
    "\n",
    "Provide a compressed summary in under {max_tokens} tokens:\"\"\"\n",
    "    \n",
    "    summary = await llm.ainvoke(prompt)\n",
    "    return summary.content\n",
    "\n",
    "\n",
    "# Test compression\n",
    "long_context = \"\"\"\n",
    "We are researching the best approaches to fine-tune large language models for a \n",
    "customer service chatbot. Our goal is to create a model that handles support tickets \n",
    "accurately and empathetically.\n",
    "\n",
    "In our first phase, we reviewed 15 papers on fine-tuning techniques. We found that \n",
    "LoRA (Low-Rank Adaptation) is the most cost-effective approach for our scale, requiring \n",
    "only 10% of the compute of full fine-tuning while achieving 95% of the quality. We also \n",
    "considered QLoRA but decided against it due to quantization artifacts in our specific \n",
    "domain. Full fine-tuning was ruled out due to GPU cost constraints.\n",
    "\n",
    "We collected 50,000 support ticket examples, cleaned and formatted them. The dataset \n",
    "covers billing issues (40%), technical support (35%), and general inquiries (25%).\n",
    "\n",
    "Current status: We have trained a LoRA adapter on the dataset and initial results show \n",
    "82% accuracy on our test set. Next steps include hyperparameter tuning, testing with \n",
    "real support agents, and deploying to a staging environment.\n",
    "\"\"\"\n",
    "\n",
    "compressed = await compress_context(long_context, 100, llm)\n",
    "print(f\"Original length: {len(long_context)} chars\")\n",
    "print(f\"Compressed length: {len(compressed)} chars\")\n",
    "print(f\"\\nCompressed:\\n{compressed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subagent Spawning and Delegation\n",
    "\n",
    "A single agent trying to do everything is like a one-person company trying to scale. Subagent spawning is the pattern where a coordinating agent creates specialized helper agents to handle specific subtasks. Each subagent has a focused role, relevant tools, and just the context it needs for its job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubagentStatus(Enum):\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "@dataclass\n",
    "class Subagent:\n",
    "    id: str\n",
    "    role: str\n",
    "    task: str\n",
    "    status: SubagentStatus\n",
    "    result: Any = None\n",
    "    error: str = None\n",
    "\n",
    "class SubagentManager:\n",
    "    \"\"\"Manages the lifecycle of subagents: spawning, monitoring, and collecting results.\"\"\"\n",
    "\n",
    "    def __init__(self, llm=None):\n",
    "        self.subagents: dict[str, Subagent] = {}\n",
    "        self.shared_context: dict[str, Any] = {}\n",
    "        self.llm = llm\n",
    "\n",
    "    def spawn(self, role: str, task: str, context: dict) -> str:\n",
    "        \"\"\"Create and execute a new subagent.\"\"\"\n",
    "        agent_id = f\"{role}_{len(self.subagents)}\"\n",
    "\n",
    "        subagent = Subagent(\n",
    "            id=agent_id,\n",
    "            role=role,\n",
    "            task=task,\n",
    "            status=SubagentStatus.PENDING\n",
    "        )\n",
    "        self.subagents[agent_id] = subagent\n",
    "\n",
    "        # In practice, you'd actually create and run the agent here\n",
    "        self._execute_subagent(subagent, context)\n",
    "\n",
    "        return agent_id\n",
    "\n",
    "    def _execute_subagent(self, subagent: Subagent, context: dict):\n",
    "        \"\"\"Execute a subagent's task using the LLM.\n",
    "\n",
    "        In a production system, this would create an independent agent\n",
    "        with its own tools and context. Here we simulate it with a\n",
    "        focused LLM call.\n",
    "        \"\"\"\n",
    "        subagent.status = SubagentStatus.RUNNING\n",
    "        try:\n",
    "            context_str = json.dumps(context, indent=2) if context else \"No additional context.\"\n",
    "            prompt = f\"\"\"You are a specialized {subagent.role} agent.\n",
    "\n",
    "Your task: {subagent.task}\n",
    "\n",
    "Context: {context_str}\n",
    "\n",
    "Complete this task thoroughly and provide your results.\"\"\"\n",
    "\n",
    "            response = self.llm.invoke(prompt)\n",
    "            subagent.result = response.content\n",
    "            subagent.status = SubagentStatus.COMPLETED\n",
    "        except Exception as e:\n",
    "            subagent.error = str(e)\n",
    "            subagent.status = SubagentStatus.FAILED\n",
    "\n",
    "    def get_status(self, agent_id: str) -> SubagentStatus:\n",
    "        return self.subagents[agent_id].status\n",
    "\n",
    "    def get_result(self, agent_id: str) -> Any:\n",
    "        subagent = self.subagents[agent_id]\n",
    "        if subagent.status != SubagentStatus.COMPLETED:\n",
    "            raise ValueError(f\"Subagent {agent_id} not yet complete\")\n",
    "        return subagent.result\n",
    "\n",
    "    def get_error(self, agent_id: str) -> str:\n",
    "        \"\"\"Get the error message from a failed subagent.\"\"\"\n",
    "        return self.subagents[agent_id].error\n",
    "\n",
    "    def terminate(self, agent_id: str):\n",
    "        \"\"\"Terminate a running subagent.\"\"\"\n",
    "        subagent = self.subagents[agent_id]\n",
    "        subagent.status = SubagentStatus.FAILED\n",
    "        subagent.error = \"Terminated by manager\"\n",
    "\n",
    "    def update_shared_context(self, role: str, result: Any):\n",
    "        \"\"\"Update shared context with a subagent's results.\"\"\"\n",
    "        self.shared_context[role] = result\n",
    "\n",
    "    def collect_all(self) -> dict[str, Any]:\n",
    "        \"\"\"Collect results from all completed subagents.\"\"\"\n",
    "        return {\n",
    "            agent_id: sa.result \n",
    "            for agent_id, sa in self.subagents.items()\n",
    "            if sa.status == SubagentStatus.COMPLETED\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstrate SubagentManager\n",
    "manager = SubagentManager(llm=llm)\n",
    "agent_id = manager.spawn(\n",
    "    role=\"researcher\",\n",
    "    task=\"List 3 key benefits of LoRA for fine-tuning large language models\",\n",
    "    context={\"domain\": \"AI/ML\", \"audience\": \"engineers\"}\n",
    ")\n",
    "\n",
    "print(f\"Agent ID: {agent_id}\")\n",
    "print(f\"Status: {manager.get_status(agent_id)}\")\n",
    "print(f\"Result:\\n{manager.get_result(agent_id)[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel vs. Sequential Delegation\n",
    "\n",
    "Some subtasks can run in parallel (independent of each other), while others must run sequentially (each builds on the previous). Deep agents need to handle both patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parallel_delegation(tasks: list[dict], manager: SubagentManager) -> list[Any]:\n",
    "    \"\"\"Execute independent tasks in parallel.\"\"\"\n",
    "    \n",
    "    async def run_task(task):\n",
    "        agent_id = manager.spawn(\n",
    "            role=task[\"role\"],\n",
    "            task=task[\"description\"],\n",
    "            context=task.get(\"context\", {})\n",
    "        )\n",
    "        # Wait for completion (simplified)\n",
    "        while manager.get_status(agent_id) == SubagentStatus.RUNNING:\n",
    "            await asyncio.sleep(0.1)\n",
    "        return manager.get_result(agent_id)\n",
    "    \n",
    "    results = await asyncio.gather(*[run_task(t) for t in tasks])\n",
    "    return results\n",
    "\n",
    "\n",
    "def sequential_delegation(tasks: list[dict], manager: SubagentManager) -> list[Any]:\n",
    "    \"\"\"Execute dependent tasks sequentially, passing results forward.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    accumulated_context = {}\n",
    "    \n",
    "    for task in tasks:\n",
    "        # Include results from previous tasks in context\n",
    "        task_context = {**task.get(\"context\", {}), **accumulated_context}\n",
    "        \n",
    "        agent_id = manager.spawn(\n",
    "            role=task[\"role\"],\n",
    "            task=task[\"description\"],\n",
    "            context=task_context\n",
    "        )\n",
    "        \n",
    "        # Wait and get result\n",
    "        result = manager.get_result(agent_id)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Add to accumulated context for next task\n",
    "        accumulated_context[f\"{task['role']}_result\"] = result\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test parallel delegation\n",
    "parallel_tasks = [\n",
    "    {\"role\": \"researcher\", \"description\": \"Find 3 benefits of RAG systems\", \"context\": {}},\n",
    "    {\"role\": \"analyst\", \"description\": \"List 3 challenges of RAG systems\", \"context\": {}},\n",
    "]\n",
    "\n",
    "manager_parallel = SubagentManager(llm=llm)\n",
    "parallel_results = await parallel_delegation(parallel_tasks, manager_parallel)\n",
    "for task, result in zip(parallel_tasks, parallel_results):\n",
    "    print(f\"\\n[{task['role']}] {task['description']}\")\n",
    "    print(f\"  {result[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Delegation\n",
    "\n",
    "Real workflows mix parallel and sequential execution. The research phase might run three searches in parallel, but the writing phase must wait for all research to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def hybrid_delegation(workflow: dict, manager: SubagentManager) -> dict:\n",
    "    \"\"\"Execute a workflow with both parallel and sequential phases.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for phase in workflow[\"phases\"]:\n",
    "        phase_tasks = phase[\"tasks\"]\n",
    "        \n",
    "        if phase.get(\"parallel\", False):\n",
    "            # Run all tasks in this phase concurrently\n",
    "            phase_results = await parallel_delegation(phase_tasks, manager)\n",
    "        else:\n",
    "            # Run tasks sequentially\n",
    "            phase_results = sequential_delegation(phase_tasks, manager)\n",
    "        \n",
    "        results[phase[\"name\"]] = phase_results\n",
    "        \n",
    "        # Add phase results to context for next phase\n",
    "        for task, result in zip(phase_tasks, phase_results):\n",
    "            manager.update_shared_context(task[\"role\"], result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test hybrid delegation\n",
    "workflow = {\n",
    "    \"phases\": [\n",
    "        {\n",
    "            \"name\": \"research\",\n",
    "            \"parallel\": True,\n",
    "            \"tasks\": [\n",
    "                {\"role\": \"topic_researcher\", \"description\": \"Summarize key concepts of LoRA fine-tuning in 2 sentences\"},\n",
    "                {\"role\": \"comparison_researcher\", \"description\": \"Compare LoRA vs full fine-tuning in 2 sentences\"},\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"synthesis\",\n",
    "            \"parallel\": False,\n",
    "            \"tasks\": [\n",
    "                {\"role\": \"synthesizer\", \"description\": \"Combine the research findings into a brief recommendation\"},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "manager_hybrid = SubagentManager(llm=llm)\n",
    "hybrid_results = await hybrid_delegation(workflow, manager_hybrid)\n",
    "for phase_name, phase_results in hybrid_results.items():\n",
    "    print(f\"\\n--- Phase: {phase_name} ---\")\n",
    "    for result in phase_results:\n",
    "        print(f\"  {result[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting and Integrating Results\n",
    "\n",
    "The coordinator's job is to both delegate tasks *and* make sense of what comes back. Subagent results often need to be synthesized, conflicts resolved, and gaps identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(results: list[dict]) -> str:\n",
    "    \"\"\"Format research results for LLM synthesis.\n",
    "\n",
    "    Each result dict should have 'source' and 'content' keys.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for i, r in enumerate(results, 1):\n",
    "        source = r.get(\"source\", f\"Source {i}\")\n",
    "        content = r.get(\"content\", str(r))\n",
    "        parts.append(f\"[{source}]:\\n{content}\")\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "def integrate_research_results(results: list[dict], llm) -> dict:\n",
    "    \"\"\"Synthesize results from multiple research subagents.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You have research results from multiple specialists:\n",
    "\n",
    "{format_results(results)}\n",
    "\n",
    "Synthesize these into a coherent summary:\n",
    "1. Identify key themes that appear across multiple sources\n",
    "2. Note any contradictions or disagreements between sources\n",
    "3. Highlight gaps where more research may be needed\n",
    "4. Provide an overall conclusion\n",
    "\n",
    "Be specific and cite which source each finding came from.\"\"\"\n",
    "    \n",
    "    synthesis = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"synthesis\": synthesis.content,\n",
    "        \"source_count\": len(results),\n",
    "        \"sources\": [r.get(\"source\") for r in results]\n",
    "    }\n",
    "\n",
    "\n",
    "# Test integration\n",
    "research_results = [\n",
    "    {\"source\": \"Topic Researcher\", \"content\": \"LoRA adds low-rank matrices to attention layers, achieving near full fine-tuning quality at 10% of the compute cost.\"},\n",
    "    {\"source\": \"Comparison Analyst\", \"content\": \"Full fine-tuning modifies all parameters and achieves marginally better results, but costs 10x more in compute. LoRA is preferred for most use cases.\"},\n",
    "    {\"source\": \"Practitioner\", \"content\": \"In production, LoRA adapters can be hot-swapped, allowing one base model to serve multiple specialized tasks.\"},\n",
    "]\n",
    "\n",
    "integrated = integrate_research_results(research_results, llm)\n",
    "print(f\"Sources integrated: {integrated['source_count']}\")\n",
    "print(f\"\\nSynthesis:\\n{integrated['synthesis'][:600]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Step Reasoning\n",
    "\n",
    "Deep agents reason through complex problems step by step, exploring multiple angles when needed. This section covers sequential chains, parallel exploration, and adaptive stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_reasoning(problem: str, max_steps: int, llm) -> dict:\n",
    "    \"\"\"Reason through a problem step by step.\"\"\"\n",
    "    \n",
    "    steps = []\n",
    "    current_state = f\"Problem: {problem}\\n\\nLet me think through this step by step.\"\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        prompt = f\"\"\"{current_state}\n",
    "\n",
    "Step {i + 1}: What's the next logical step in solving this problem?\n",
    "Consider what we know, what we need to find out, and what follows logically.\n",
    "\n",
    "If we have enough information to give a final answer, say \\\"CONCLUSION: [answer]\\\"\n",
    "Otherwise, describe your reasoning for this step.\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt).content\n",
    "        steps.append({\"step\": i + 1, \"reasoning\": response})\n",
    "        \n",
    "        if response.startswith(\"CONCLUSION:\"):\n",
    "            return {\n",
    "                \"conclusion\": response.replace(\"CONCLUSION:\", \"\").strip(),\n",
    "                \"steps\": steps,\n",
    "                \"total_steps\": i + 1\n",
    "            }\n",
    "        \n",
    "        current_state += f\"\\n\\nStep {i + 1}: {response}\"\n",
    "    \n",
    "    return {\n",
    "        \"conclusion\": None,\n",
    "        \"steps\": steps,\n",
    "        \"total_steps\": max_steps,\n",
    "        \"status\": \"max_steps_reached\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Test sequential reasoning\n",
    "result = sequential_reasoning(\n",
    "    \"Should a startup with a 5-person team use microservices or a monolith architecture?\",\n",
    "    max_steps=4,\n",
    "    llm=llm\n",
    ")\n",
    "print(f\"Completed in {result['total_steps']} steps\")\n",
    "if result.get(\"conclusion\"):\n",
    "    print(f\"Conclusion: {result['conclusion'][:300]}\")\n",
    "else:\n",
    "    print(\"Final step reasoning:\")\n",
    "    print(f\"  {result['steps'][-1]['reasoning'][:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Reasoning Paths\n",
    "\n",
    "For some problems, multiple reasoning approaches might lead to an answer. Rather than committing to one path, deep agents can explore several in parallel and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reasoning_results(results: list[dict]) -> str:\n",
    "    \"\"\"Format parallel reasoning results for comparison.\"\"\"\n",
    "    parts = []\n",
    "    for r in results:\n",
    "        parts.append(f\"Approach: {r['approach']}\\nReasoning: {r['reasoning']}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "async def parallel_reasoning(problem: str, approaches: list[str], llm) -> dict:\n",
    "    \"\"\"Try multiple reasoning approaches in parallel.\"\"\"\n",
    "    \n",
    "    async def try_approach(approach: str):\n",
    "        prompt = f\"\"\"Problem: {problem}\n",
    "\n",
    "Approach this problem using: {approach}\n",
    "\n",
    "Think through the problem using this approach and provide your conclusion.\n",
    "Explain your reasoning clearly.\"\"\"\n",
    "        \n",
    "        response = await llm.ainvoke(prompt)\n",
    "        return {\"approach\": approach, \"reasoning\": response.content}\n",
    "    \n",
    "    results = await asyncio.gather(*[try_approach(a) for a in approaches])\n",
    "    \n",
    "    # Compare results\n",
    "    comparison_prompt = f\"\"\"Problem: {problem}\n",
    "\n",
    "Multiple reasoning approaches were tried:\n",
    "\n",
    "{format_reasoning_results(results)}\n",
    "\n",
    "Compare these approaches:\n",
    "1. Do they reach the same conclusion?\n",
    "2. Which approach seems most rigorous?\n",
    "3. Are there any errors in reasoning?\n",
    "4. What is the best final answer?\"\"\"\n",
    "    \n",
    "    comparison = llm.invoke(comparison_prompt)\n",
    "    \n",
    "    return {\n",
    "        \"approaches\": results,\n",
    "        \"comparison\": comparison.content\n",
    "    }\n",
    "\n",
    "\n",
    "# Test parallel reasoning\n",
    "result = await parallel_reasoning(\n",
    "    \"What is the most effective way to reduce hallucinations in LLM applications?\",\n",
    "    approaches=[\n",
    "        \"Engineering perspective (technical solutions)\",\n",
    "        \"Process perspective (workflow and validation)\",\n",
    "        \"User experience perspective (design patterns)\",\n",
    "    ],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(\"Approaches tested:\")\n",
    "for approach in result[\"approaches\"]:\n",
    "    print(f\"\\n  [{approach['approach']}]\")\n",
    "    print(f\"  {approach['reasoning'][:150]}...\")\n",
    "\n",
    "print(f\"\\nComparison:\\n{result['comparison'][:400]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Stopping\n",
    "\n",
    "Deep reasoning is powerful but expensive. Every additional reasoning step costs time and tokens. Adaptive reasoning uses confidence tracking and plateau detection to know when to stop:\n",
    "\n",
    "- **Confidence threshold**: The agent believes it has a reliable answer\n",
    "- **Diminishing returns**: Recent steps aren't adding new insights\n",
    "- **Step limit**: Hard cap to prevent runaway reasoning\n",
    "- **Convergence**: Multiple approaches reach the same conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reasoning_history(steps: list[dict]) -> str:\n",
    "    \"\"\"Format the reasoning history for inclusion in prompts.\"\"\"\n",
    "    if not steps:\n",
    "        return \"\"\n",
    "    parts = []\n",
    "    for s in steps:\n",
    "        parts.append(f\"Step {s['step']}: {s['reasoning']}\")\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "def parse_reasoning_response(response: str) -> tuple[str, float, str | None]:\n",
    "    \"\"\"Parse a reasoning response into (reasoning, confidence, optional_answer).\n",
    "\n",
    "    Expected format in response:\n",
    "        REASONING: ...\n",
    "        CONFIDENCE: 0.X\n",
    "        FINAL_ANSWER: ... (optional)\n",
    "    \"\"\"\n",
    "    reasoning = response\n",
    "    confidence = 0.5\n",
    "    answer = None\n",
    "\n",
    "    if \"REASONING:\" in response:\n",
    "        reasoning = response.split(\"REASONING:\")[-1].split(\"CONFIDENCE:\")[0].strip()\n",
    "    if \"CONFIDENCE:\" in response:\n",
    "        try:\n",
    "            conf_str = response.split(\"CONFIDENCE:\")[-1].split(\"\\n\")[0].strip()\n",
    "            confidence = float(conf_str)\n",
    "        except (ValueError, IndexError):\n",
    "            confidence = 0.5\n",
    "    if \"FINAL_ANSWER:\" in response:\n",
    "        answer = response.split(\"FINAL_ANSWER:\")[-1].strip()\n",
    "\n",
    "    return reasoning, confidence, answer\n",
    "\n",
    "\n",
    "def get_final_answer(problem: str, steps: list[dict], llm) -> str:\n",
    "    \"\"\"Ask the LLM for a final answer given the reasoning history.\"\"\"\n",
    "    history = format_reasoning_history(steps)\n",
    "    prompt = f\"\"\"Problem: {problem}\n",
    "\n",
    "Based on this reasoning:\n",
    "{history}\n",
    "\n",
    "Provide a clear, definitive final answer in 2-3 sentences.\"\"\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "\n",
    "def adaptive_reasoning(problem: str, llm, max_steps: int = 10) -> dict:\n",
    "    \"\"\"Reason with adaptive stopping based on confidence and progress.\"\"\"\n",
    "    \n",
    "    steps = []\n",
    "    confidence_history = []\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        # Generate next reasoning step\n",
    "        context = format_reasoning_history(steps)\n",
    "        \n",
    "        prompt = f\"\"\"Problem: {problem}\n",
    "\n",
    "Reasoning so far:\n",
    "{context if context else \"None yet - this is step 1\"}\n",
    "\n",
    "Continue reasoning. After your analysis, rate your confidence (0.0-1.0)\n",
    "that you can now give a correct final answer.\n",
    "\n",
    "Format:\n",
    "REASONING: Your next step of analysis\n",
    "CONFIDENCE: 0.X\n",
    "FINAL_ANSWER: [only if confidence >= 0.8] Your answer\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt).content\n",
    "        reasoning, confidence, answer = parse_reasoning_response(response)\n",
    "        \n",
    "        steps.append({\"step\": i + 1, \"reasoning\": reasoning})\n",
    "        confidence_history.append(confidence)\n",
    "        \n",
    "        # Check stopping conditions\n",
    "        if answer:  # Model provided final answer\n",
    "            return {\"answer\": answer, \"steps\": steps, \"confidence\": confidence}\n",
    "        \n",
    "        if confidence >= 0.85:  # High confidence - prompt for answer\n",
    "            final = get_final_answer(problem, steps, llm)\n",
    "            return {\"answer\": final, \"steps\": steps, \"confidence\": confidence}\n",
    "        \n",
    "        # Check for plateau (no confidence improvement in 3 steps)\n",
    "        if len(confidence_history) >= 3:\n",
    "            recent = confidence_history[-3:]\n",
    "            if max(recent) - min(recent) < 0.05:\n",
    "                final = get_final_answer(problem, steps, llm)\n",
    "                return {\"answer\": final, \"steps\": steps, \"confidence\": confidence, \n",
    "                        \"note\": \"Stopped due to confidence plateau\"}\n",
    "    \n",
    "    # Max steps reached\n",
    "    final = get_final_answer(problem, steps, llm)\n",
    "    return {\"answer\": final, \"steps\": steps, \"confidence\": confidence_history[-1],\n",
    "            \"note\": \"Max steps reached\"}\n",
    "\n",
    "\n",
    "# Test adaptive reasoning\n",
    "result = adaptive_reasoning(\n",
    "    \"Is it better to use RAG or fine-tuning for a customer support chatbot with 10,000 FAQ entries?\",\n",
    "    llm=llm,\n",
    "    max_steps=5\n",
    ")\n",
    "\n",
    "print(f\"Steps taken: {len(result['steps'])}\")\n",
    "print(f\"Final confidence: {result.get('confidence', 'N/A')}\")\n",
    "if result.get(\"note\"):\n",
    "    print(f\"Note: {result['note']}\")\n",
    "print(f\"\\nAnswer: {result['answer'][:400]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Reflection and Metacognition\n",
    "\n",
    "Deep agents can think about their own thinking. They evaluate whether they're making progress, critique their own outputs, and adjust strategies when something isn't working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_plan(plan: Plan) -> str:\n",
    "    \"\"\"Format a Plan object for display in prompts.\"\"\"\n",
    "    lines = [f\"Goal: {plan.goal}\", \"Tasks:\"]\n",
    "    for t in plan.tasks:\n",
    "        deps = f\" (depends on: {', '.join(t.depends_on)})\" if t.depends_on else \"\"\n",
    "        lines.append(f\"  [{t.status}] {t.id}: {t.description}{deps}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def format_completed(completed_tasks: list[str]) -> str:\n",
    "    \"\"\"Format a list of completed task descriptions for prompts.\"\"\"\n",
    "    if not completed_tasks:\n",
    "        return \"(none yet)\"\n",
    "    return \"\\n\".join(f\"  - {task}\" for task in completed_tasks)\n",
    "\n",
    "\n",
    "def parse_plan(plan_text: str) -> Plan:\n",
    "    \"\"\"Parse LLM-generated plan text into a Plan object.\n",
    "\n",
    "    Creates tasks from numbered or bulleted lines in the LLM's response.\n",
    "    \"\"\"\n",
    "    lines = plan_text.strip().split(\"\\n\")\n",
    "    tasks = []\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip().lstrip(\"0123456789.-) \")\n",
    "        if line and len(line) > 5:\n",
    "            tasks.append(Task(\n",
    "                id=f\"t{i+1}\",\n",
    "                description=line,\n",
    "                estimated_hours=1.0\n",
    "            ))\n",
    "    if not tasks:\n",
    "        tasks = [Task(id=\"t1\", description=\"Re-evaluate approach\", estimated_hours=1.0)]\n",
    "    return Plan(goal=\"Revised plan\", tasks=tasks)\n",
    "\n",
    "\n",
    "def evaluate_progress(goal: str, plan: Plan, completed_tasks: list[str], llm) -> dict:\n",
    "    \"\"\"Assess progress toward the overall goal.\"\"\"\n",
    "    \n",
    "    total_tasks = len(plan.tasks)\n",
    "    completed_count = len(completed_tasks)\n",
    "    \n",
    "    prompt = f\"\"\"Goal: {goal}\n",
    "\n",
    "Original plan:\n",
    "{format_plan(plan)}\n",
    "\n",
    "Completed tasks:\n",
    "{format_completed(completed_tasks)}\n",
    "\n",
    "Progress: {completed_count}/{total_tasks} tasks complete\n",
    "\n",
    "Evaluate:\n",
    "1. Are we on track to achieve the goal?\n",
    "2. Have completed tasks actually moved us closer to the goal?\n",
    "3. Are there any warning signs or blockers?\n",
    "4. Should we adjust the remaining plan?\n",
    "\n",
    "Be honest and specific.\"\"\"\n",
    "    \n",
    "    evaluation = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"progress_fraction\": completed_count / total_tasks if total_tasks > 0 else 0,\n",
    "        \"evaluation\": evaluation.content,\n",
    "        \"status\": \"on_track\" if completed_count / total_tasks > 0.3 else \"needs_review\"\n",
    "    }\n",
    "\n",
    "\n",
    "def adjust_strategy(current_plan: Plan, evaluation: dict, llm) -> Plan:\n",
    "    \"\"\"Modify plan based on progress evaluation.\"\"\"\n",
    "    \n",
    "    if \"on_track\" in evaluation.get(\"status\", \"\"):\n",
    "        return current_plan  # No adjustment needed\n",
    "    \n",
    "    prompt = f\"\"\"Current plan:\n",
    "{format_plan(current_plan)}\n",
    "\n",
    "Progress evaluation:\n",
    "{evaluation['evaluation']}\n",
    "\n",
    "The current approach isn't working well. Propose adjustments:\n",
    "1. Which remaining tasks should be modified or replaced?\n",
    "2. Are there new tasks that should be added?\n",
    "3. Should any tasks be removed or deprioritized?\n",
    "4. What's the revised strategy?\n",
    "\n",
    "Provide a revised task list (one task per line).\"\"\"\n",
    "    \n",
    "    revised = llm.invoke(prompt)\n",
    "    \n",
    "    # In practice, you'd parse this into a new Plan object\n",
    "    return parse_plan(revised.content)\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "progress = evaluate_progress(\n",
    "    goal=offsite_plan.goal,\n",
    "    plan=offsite_plan,\n",
    "    completed_tasks=[\"Determine budget and dates\", \"Survey team for location preferences\"],\n",
    "    llm=llm\n",
    ")\n",
    "print(f\"Progress: {progress['progress_fraction']:.0%}\")\n",
    "print(f\"Status: {progress['status']}\")\n",
    "print(f\"\\nEvaluation:\\n{progress['evaluation'][:400]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reflection Loop with LangGraph\n",
    "\n",
    "Reflection is most powerful when built into the agent's core loop. After completing significant work, the agent evaluates and potentially revises before moving on.\n",
    "\n",
    "The following uses LangGraph's `StateGraph` to create a structured execute-reflect-replan cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_task(task: dict) -> str:\n",
    "    \"\"\"Execute a single task using the LLM.\n",
    "\n",
    "    In a real system, this might call tools, APIs, or spawn subagents.\n",
    "    Here we simulate execution with an LLM call.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are executing this task: {task.get('description', task.get('id', 'unknown'))}\n",
    "\n",
    "Complete this task and provide the result. Be concise (2-3 sentences).\"\"\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "\n",
    "class ReflectiveState(TypedDict):\n",
    "    goal: str\n",
    "    plan: dict\n",
    "    current_task_idx: int\n",
    "    task_results: dict\n",
    "    reflection: str\n",
    "    needs_replanning: bool\n",
    "\n",
    "\n",
    "def execute_task_node(state: ReflectiveState) -> ReflectiveState:\n",
    "    \"\"\"Execute the current task.\"\"\"\n",
    "    task = state[\"plan\"][\"tasks\"][state[\"current_task_idx\"]]\n",
    "    result = execute_task(task)\n",
    "    \n",
    "    new_results = {**state[\"task_results\"], task[\"id\"]: result}\n",
    "    return {\"task_results\": new_results, \"current_task_idx\": state[\"current_task_idx\"] + 1}\n",
    "\n",
    "\n",
    "def reflect_node(state: ReflectiveState) -> ReflectiveState:\n",
    "    \"\"\"Reflect on progress after task completion.\"\"\"\n",
    "    task_idx = state[\"current_task_idx\"] - 1  # Just completed\n",
    "    \n",
    "    reflection_prompt = f\"\"\"Goal: {state['goal']}\n",
    "    \n",
    "Completed task: {state['plan']['tasks'][task_idx]['description']}\n",
    "Result: {list(state['task_results'].values())[-1]}\n",
    "\n",
    "Reflect:\n",
    "1. Did this task achieve what we expected?\n",
    "2. Are we still on track for the overall goal?\n",
    "3. Should we adjust the plan?\n",
    "\n",
    "If the plan needs significant changes, say \\\"REPLAN NEEDED\\\".\n",
    "Otherwise, say \\\"CONTINUE\\\".\"\"\"\n",
    "    \n",
    "    reflection = llm.invoke(reflection_prompt).content\n",
    "    \n",
    "    needs_replan = \"REPLAN NEEDED\" in reflection\n",
    "    return {\"reflection\": reflection, \"needs_replanning\": needs_replan}\n",
    "\n",
    "\n",
    "def replan_node(state: ReflectiveState) -> ReflectiveState:\n",
    "    \"\"\"Create a new plan based on reflection.\"\"\"\n",
    "    plan_obj = Plan(\n",
    "        goal=state[\"goal\"],\n",
    "        tasks=[Task(**t) for t in state[\"plan\"][\"tasks\"]]\n",
    "    )\n",
    "    new_plan = adjust_strategy(\n",
    "        plan_obj,\n",
    "        {\"evaluation\": state[\"reflection\"], \"status\": \"needs_review\"},\n",
    "        llm\n",
    "    )\n",
    "    new_plan_dict = {\n",
    "        \"goal\": new_plan.goal,\n",
    "        \"tasks\": [t.model_dump() for t in new_plan.tasks]\n",
    "    }\n",
    "    return {\"plan\": new_plan_dict, \"needs_replanning\": False, \"current_task_idx\": 0}\n",
    "\n",
    "\n",
    "def route_after_reflection(state: ReflectiveState) -> str:\n",
    "    \"\"\"Decide whether to continue, replan, or finish.\"\"\"\n",
    "    if state[\"needs_replanning\"]:\n",
    "        return \"replan\"\n",
    "    if state[\"current_task_idx\"] >= len(state[\"plan\"][\"tasks\"]):\n",
    "        return \"finish\"\n",
    "    return \"continue\"\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "graph = StateGraph(ReflectiveState)\n",
    "graph.add_node(\"execute\", execute_task_node)\n",
    "graph.add_node(\"reflect\", reflect_node)\n",
    "graph.add_node(\"replan\", replan_node)\n",
    "\n",
    "graph.set_entry_point(\"execute\")\n",
    "graph.add_edge(\"execute\", \"reflect\")\n",
    "graph.add_conditional_edges(\"reflect\", route_after_reflection, {\n",
    "    \"continue\": \"execute\",\n",
    "    \"replan\": \"replan\",\n",
    "    \"finish\": END\n",
    "})\n",
    "graph.add_edge(\"replan\", \"execute\")\n",
    "\n",
    "reflective_agent = graph.compile()\n",
    "\n",
    "\n",
    "# Test with a small plan (2 tasks for speed)\n",
    "small_plan = Plan(\n",
    "    goal=\"Prepare a presentation on LoRA fine-tuning\",\n",
    "    tasks=[\n",
    "        Task(id=\"t1\", description=\"Research key concepts of LoRA\", depends_on=[], estimated_hours=1),\n",
    "        Task(id=\"t2\", description=\"Create outline with 3 main sections\", depends_on=[\"t1\"], estimated_hours=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = reflective_agent.invoke({\n",
    "    \"goal\": small_plan.goal,\n",
    "    \"plan\": {\"goal\": small_plan.goal, \"tasks\": [t.model_dump() for t in small_plan.tasks]},\n",
    "    \"current_task_idx\": 0,\n",
    "    \"task_results\": {},\n",
    "    \"reflection\": \"\",\n",
    "    \"needs_replanning\": False,\n",
    "})\n",
    "\n",
    "print(f\"Goal: {result['goal']}\")\n",
    "print(f\"Tasks completed: {len(result['task_results'])}\")\n",
    "for task_id, task_result in result[\"task_results\"].items():\n",
    "    print(f\"\\n  [{task_id}]: {task_result[:200]}\")\n",
    "print(f\"\\nFinal reflection: {result['reflection'][:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtracking and Recovery\n",
    "\n",
    "Even with good planning and reflection, agents sometimes go down wrong paths. Deep agents need the ability to recognize dead ends and backtrack to try alternative approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dead_end(state: dict, history: list[dict]) -> dict:\n",
    "    \"\"\"Check if the agent is stuck.\n",
    "\n",
    "    Detects three patterns: repeated failures, task cycling,\n",
    "    and lack of progress.\n",
    "    \"\"\"\n",
    "    # Check for repeated failures\n",
    "    recent_failures = [h for h in history[-5:] if h.get(\"status\") == \"failed\"]\n",
    "    if len(recent_failures) >= 3:\n",
    "        return {\"is_stuck\": True, \"reason\": \"repeated_failures\"}\n",
    "    \n",
    "    # Check for cycling (same task attempted multiple times)\n",
    "    recent_tasks = [h.get(\"task_id\") for h in history[-10:]]\n",
    "    for task_id in set(recent_tasks):\n",
    "        if task_id and recent_tasks.count(task_id) >= 3:\n",
    "            return {\"is_stuck\": True, \"reason\": \"task_cycling\", \"task\": task_id}\n",
    "    \n",
    "    # Check for lack of progress\n",
    "    if len(history) > 10:\n",
    "        recent_progress = sum(1 for h in history[-10:] if h.get(\"progress_made\"))\n",
    "        if recent_progress < 2:\n",
    "            return {\"is_stuck\": True, \"reason\": \"no_progress\"}\n",
    "    \n",
    "    return {\"is_stuck\": False}\n",
    "\n",
    "\n",
    "# Test dead end detection\n",
    "history_stuck = [\n",
    "    {\"task_id\": \"t3\", \"status\": \"failed\", \"progress_made\": False},\n",
    "    {\"task_id\": \"t3\", \"status\": \"failed\", \"progress_made\": False},\n",
    "    {\"task_id\": \"t3\", \"status\": \"failed\", \"progress_made\": False},\n",
    "]\n",
    "print(f\"Repeated failures: {detect_dead_end({}, history_stuck)}\")\n",
    "\n",
    "history_cycling = [{\"task_id\": \"t1\"}, {\"task_id\": \"t2\"}, {\"task_id\": \"t1\"}, \n",
    "                   {\"task_id\": \"t2\"}, {\"task_id\": \"t1\"}, {\"task_id\": \"t2\"}]\n",
    "print(f\"Task cycling: {detect_dead_end({}, history_cycling)}\")\n",
    "\n",
    "history_ok = [{\"task_id\": \"t1\", \"status\": \"completed\", \"progress_made\": True}]\n",
    "print(f\"Normal progress: {detect_dead_end({}, history_ok)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExplorationState:\n",
    "    id: str\n",
    "    task_state: dict\n",
    "    decision_made: str\n",
    "    alternatives: list[str]\n",
    "    timestamp: float\n",
    "\n",
    "\n",
    "class ExplorationHistory:\n",
    "    \"\"\"Maintains checkpoints at decision points for backtracking.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.states: list[ExplorationState] = []\n",
    "        self.current_idx: int = -1\n",
    "\n",
    "    def checkpoint(self, state: dict, decision: str, alternatives: list[str]):\n",
    "        \"\"\"Save a decision point we might want to return to.\"\"\"\n",
    "        exploration_state = ExplorationState(\n",
    "            id=f\"checkpoint_{len(self.states)}\",\n",
    "            task_state=state.copy(),\n",
    "            decision_made=decision,\n",
    "            alternatives=alternatives,\n",
    "            timestamp=time.time()\n",
    "        )\n",
    "        self.states.append(exploration_state)\n",
    "        self.current_idx = len(self.states) - 1\n",
    "\n",
    "    def backtrack(self) -> tuple[dict, str] | None:\n",
    "        \"\"\"Return to most recent checkpoint with untried alternatives.\"\"\"\n",
    "        for i in range(self.current_idx, -1, -1):\n",
    "            state = self.states[i]\n",
    "            if state.alternatives:\n",
    "                # Try next alternative\n",
    "                next_alternative = state.alternatives.pop(0)\n",
    "                self.current_idx = i\n",
    "                return state.task_state, next_alternative\n",
    "\n",
    "        return None  # No alternatives left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_dead_end(state: dict, history: ExplorationHistory, llm) -> dict:\n",
    "    \"\"\"Recover from a dead end by backtracking and trying alternatives.\"\"\"\n",
    "    \n",
    "    backtrack_result = history.backtrack()\n",
    "    \n",
    "    if backtrack_result is None:\n",
    "        # No alternatives left - report failure\n",
    "        return {\n",
    "            \"status\": \"failed\",\n",
    "            \"reason\": \"All alternatives exhausted\"\n",
    "        }\n",
    "    \n",
    "    restored_state, alternative = backtrack_result\n",
    "    \n",
    "    # Log the backtrack\n",
    "    print(f\"Backtracking to try alternative: {alternative}\")\n",
    "    \n",
    "    # Continue from restored state with new approach\n",
    "    return {\n",
    "        \"status\": \"backtracked\",\n",
    "        \"restored_state\": restored_state,\n",
    "        \"new_approach\": alternative\n",
    "    }\n",
    "\n",
    "\n",
    "# Demonstrate backtracking\n",
    "history = ExplorationHistory()\n",
    "\n",
    "# Checkpoint 1: choosing a venue approach\n",
    "history.checkpoint(\n",
    "    state={\"task\": \"find_venue\", \"progress\": 0.2},\n",
    "    decision=\"Search online databases\",\n",
    "    alternatives=[\"Ask local contacts\", \"Hire an event planner\"]\n",
    ")\n",
    "\n",
    "# Checkpoint 2: choosing a specific database\n",
    "history.checkpoint(\n",
    "    state={\"task\": \"find_venue\", \"progress\": 0.3, \"approach\": \"online\"},\n",
    "    decision=\"Use Eventbrite\",\n",
    "    alternatives=[\"Use Google Maps\", \"Use Yelp\"]\n",
    ")\n",
    "\n",
    "# Simulate hitting dead ends\n",
    "print(\"Dead end detected! Online search yielded no results.\")\n",
    "result = handle_dead_end({}, history, llm)\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "print(\"\\nAnother dead end!\")\n",
    "result = handle_dead_end({}, history, llm)\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "print(\"\\nYet another dead end!\")\n",
    "result = handle_dead_end({}, history, llm)\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing It All Together\n",
    "\n",
    "Now let's combine planning, delegation, reasoning, reflection, and backtracking into a single cohesive pipeline. We'll build a **Deep Research Agent** that:\n",
    "\n",
    "1. **Plans** — Decomposes a research question into sub-questions\n",
    "2. **Delegates** — Spawns subagents to research each sub-question in parallel\n",
    "3. **Reasons** — Synthesizes findings using parallel reasoning approaches\n",
    "4. **Reflects** — Evaluates whether the research is complete and identifies gaps\n",
    "5. **Backtracks** — Checks if we're stuck and tries alternatives if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def deep_research_agent(question: str, llm, max_iterations: int = 3) -> dict:\n",
    "    \"\"\"A deep agent that researches a question using all four pillars.\n",
    "\n",
    "    Combines planning, delegation, reasoning, reflection, and backtracking\n",
    "    to produce a comprehensive research report.\n",
    "    \"\"\"\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"DEEP RESEARCH AGENT\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # PHASE 1: PLANNING -- Decompose the question\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- Phase 1: Planning ---\")\n",
    "    planning_prompt = f\"\"\"Break this research question into 3 specific sub-questions\n",
    "that, when answered, would provide a comprehensive answer:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Format each sub-question on its own line, no numbering.\"\"\"\n",
    "\n",
    "    sub_questions_raw = llm.invoke(planning_prompt).content.strip().split(\"\\n\")\n",
    "    sub_questions = [q.strip() for q in sub_questions_raw if q.strip()][:3]\n",
    "\n",
    "    plan = Plan(\n",
    "        goal=question,\n",
    "        tasks=[\n",
    "            Task(id=f\"research_{i+1}\", description=sq, estimated_hours=1)\n",
    "            for i, sq in enumerate(sub_questions)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"Plan created with {len(plan.tasks)} sub-questions:\")\n",
    "    for task in plan.tasks:\n",
    "        print(f\"  {task.id}: {task.description}\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # PHASE 2: DELEGATION -- Research each sub-question\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- Phase 2: Delegation (parallel) ---\")\n",
    "    manager = SubagentManager(llm=llm)\n",
    "\n",
    "    research_tasks = [\n",
    "        {\n",
    "            \"role\": f\"researcher_{i+1}\",\n",
    "            \"description\": f\"Research this question thoroughly and provide key findings: {sq}\",\n",
    "            \"context\": {\"main_question\": question}\n",
    "        }\n",
    "        for i, sq in enumerate(sub_questions)\n",
    "    ]\n",
    "\n",
    "    research_results_raw = await parallel_delegation(research_tasks, manager)\n",
    "\n",
    "    research_results = [\n",
    "        {\"source\": f\"Researcher {i+1}: {sub_questions[i][:50]}\", \"content\": result}\n",
    "        for i, result in enumerate(research_results_raw)\n",
    "    ]\n",
    "\n",
    "    print(f\"Received {len(research_results)} research reports\")\n",
    "    for r in research_results:\n",
    "        print(f\"  [{r['source'][:40]}...] {r['content'][:80]}...\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # PHASE 3: REASONING -- Synthesize findings\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- Phase 3: Reasoning (parallel approaches) ---\")\n",
    "\n",
    "    findings_text = format_results(research_results)\n",
    "    synthesis_problem = f\"\"\"Given these research findings about '{question}':\n",
    "\n",
    "{findings_text}\n",
    "\n",
    "What is the comprehensive answer to the original question?\"\"\"\n",
    "\n",
    "    reasoning_result = await parallel_reasoning(\n",
    "        synthesis_problem,\n",
    "        approaches=[\n",
    "            \"Analytical synthesis (identify patterns and themes)\",\n",
    "            \"Critical evaluation (assess strength of evidence)\",\n",
    "        ],\n",
    "        llm=llm\n",
    "    )\n",
    "\n",
    "    print(f\"Reasoning complete with {len(reasoning_result['approaches'])} approaches\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # PHASE 4: REFLECTION -- Evaluate completeness\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- Phase 4: Reflection ---\")\n",
    "\n",
    "    completed_tasks = [t.description for t in plan.tasks]\n",
    "    progress = evaluate_progress(\n",
    "        goal=question,\n",
    "        plan=plan,\n",
    "        completed_tasks=completed_tasks,\n",
    "        llm=llm\n",
    "    )\n",
    "\n",
    "    print(f\"Progress: {progress['progress_fraction']:.0%}\")\n",
    "    print(f\"Status: {progress['status']}\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # PHASE 5: BACKTRACKING CHECK -- Are we stuck?\n",
    "    # -------------------------------------------------------\n",
    "    print(\"\\n--- Phase 5: Backtracking check ---\")\n",
    "\n",
    "    exec_history = [\n",
    "        {\"task_id\": t.id, \"status\": \"completed\", \"progress_made\": True}\n",
    "        for t in plan.tasks\n",
    "    ]\n",
    "    dead_end_check = detect_dead_end({}, exec_history)\n",
    "    print(f\"Stuck? {dead_end_check['is_stuck']}\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # FINAL OUTPUT\n",
    "    # -------------------------------------------------------\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RESEARCH COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    final_report = {\n",
    "        \"question\": question,\n",
    "        \"sub_questions\": sub_questions,\n",
    "        \"research_findings\": [r[\"content\"][:300] for r in research_results],\n",
    "        \"synthesis\": reasoning_result[\"comparison\"],\n",
    "        \"progress_evaluation\": progress[\"evaluation\"],\n",
    "        \"is_stuck\": dead_end_check[\"is_stuck\"],\n",
    "    }\n",
    "\n",
    "    return final_report\n",
    "\n",
    "\n",
    "# Run the deep research agent\n",
    "report = await deep_research_agent(\n",
    "    \"What are the most effective strategies for reducing hallucinations in production LLM applications?\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL SYNTHESIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(report[\"synthesis\"][:800])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
