{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Evaluation Infrastructure\n",
    "\n",
    "## Systematic Evaluation for AI Systems\n",
    "\n",
    "This notebook explores evaluation methodology for RAG and agentic systems. We cover the complete infrastructure needed to move from \"vibe checks\" to systematic, metrics-driven development.\n",
    "\n",
    "```\n",
    "    ┌─────────────────────────────────────────────────────────────┐\n",
    "    │                  Evaluation Infrastructure                  │\n",
    "    │                                                             │\n",
    "    │   ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │\n",
    "    │   │ Synthetic│  │  RAGAS   │  │ Quality  │  │ Metrics  │    │\n",
    "    │   │   Data   │  │  Metrics │  │Validation│  │ Tracking │    │\n",
    "    │   │Generation│  │          │  │          │  │          │    │\n",
    "    │   └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘    │\n",
    "    │        │             │             │             │          │\n",
    "    │        v             v             v             v          │\n",
    "    │   Test cases    Faithfulness   Filter bad    Track over     │\n",
    "    │   at scale      Relevancy      test cases    time           │\n",
    "    │                 Precision                                   │\n",
    "    │                 Recall                                      │\n",
    "    └─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Topics covered:\n",
    "- Synthetic data generation for test cases\n",
    "- RAGAS core metrics (faithfulness, answer relevancy, context precision, context recall)\n",
    "- RAGAS testset generation from documents\n",
    "- Difficulty-based and adversarial test case generation\n",
    "- Data quality validation and coverage verification\n",
    "- Metrics-driven development practices\n",
    "- LangSmith integration for dataset management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Make sure you have the required packages installed:\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "Note: This notebook requires RAGAS 0.4.x. The `nest_asyncio` package is needed for RAGAS to work in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from datetime import datetime, UTC\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RAGAS requires nest_asyncio in Jupyter environments\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def extract_json(content: str) -> str:\n",
    "    \"\"\"Extract JSON from LLM response, handling markdown code blocks.\"\"\"\n",
    "    if \"```json\" in content:\n",
    "        content = content.split(\"```json\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in content:\n",
    "        content = content.split(\"```\")[1].split(\"```\")[0]\n",
    "    return content.strip()\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation\n",
    "\n",
    "The cold-start problem: systematic evaluation requires test data, but you don't have users yet (or enough users, or users who hit edge cases). Synthetic test data lets you achieve coverage of scenarios you've identified without waiting for production traffic.\n",
    "\n",
    "The key insight: use an LLM to generate the test cases that will evaluate your system. This works because the generation model and evaluation target can be different, and generated data can be reviewed and filtered by humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_cases(\n",
    "    topic: str, \n",
    "    num_cases: int,\n",
    "    difficulty_distribution: dict,\n",
    "    llm: ChatOpenAI\n",
    ") -> list[dict]:\n",
    "    \"\"\"Generate synthetic test cases for a topic.\"\"\"\n",
    "    \n",
    "    test_cases = []\n",
    "    \n",
    "    for difficulty, count in difficulty_distribution.items():\n",
    "        prompt = f\"\"\"Generate {count} question-answer pairs about {topic}.\n",
    "\n",
    "Difficulty level: {difficulty}\n",
    "- easy: Basic factual questions with straightforward answers\n",
    "- medium: Questions requiring explanation or connection of concepts  \n",
    "- hard: Questions requiring synthesis, edge cases, or subtle distinctions\n",
    "\n",
    "For each pair, provide:\n",
    "- question: The question to ask\n",
    "- expected_answer: A reference answer (doesn't need to be exact)\n",
    "- key_concepts: List of concepts the answer should mention\n",
    "- difficulty: {difficulty}\n",
    "\n",
    "Format as JSON array: [{{\"question\": \"...\", \"expected_answer\": \"...\", \"key_concepts\": [...], \"difficulty\": \"{difficulty}\"}}]\"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        content = extract_json(response.content)\n",
    "        \n",
    "        try:\n",
    "            cases = json.loads(content)\n",
    "            test_cases.extend(cases)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not parse response for {difficulty}\")\n",
    "    \n",
    "    return test_cases\n",
    "\n",
    "\n",
    "# Generate a small test set\n",
    "test_data = generate_test_cases(\n",
    "    topic=\"retrieval-augmented generation\",\n",
    "    num_cases=6,\n",
    "    difficulty_distribution={\"easy\": 2, \"medium\": 3, \"hard\": 1},\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(test_data)} test cases:\")\n",
    "for tc in test_data[:3]:\n",
    "    print(f\"  [{tc.get('difficulty', 'unknown')}] {tc.get('question', 'N/A')[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## The RAGAS Framework\n",
    "\n",
    "RAGAS (RAG Assessment) provides automated metrics specifically designed for evaluating RAG systems. It measures retrieval quality and generation quality separately, so you can identify exactly where problems occur.\n",
    "\n",
    "### The Four Core Metrics\n",
    "\n",
    "| Metric | What It Measures | Good | Acceptable | Needs Work |\n",
    "|--------|------------------|------|------------|------------|\n",
    "| **Faithfulness** | Does the answer stick to retrieved context? | > 0.9 | 0.7-0.9 | < 0.7 |\n",
    "| **Answer Relevancy** | Does the response address what was asked? | > 0.8 | 0.6-0.8 | < 0.6 |\n",
    "| **Context Precision** | Of retrieved docs, what proportion were relevant? | > 0.8 | 0.6-0.8 | < 0.6 |\n",
    "| **Context Recall** | Does retrieved context contain needed info? | > 0.8 | 0.6-0.8 | < 0.6 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics._faithfulness import Faithfulness\n",
    "from ragas.metrics._answer_relevance import ResponseRelevancy\n",
    "from ragas.metrics._context_precision import LLMContextPrecisionWithoutReference\n",
    "from ragas.metrics._context_recall import LLMContextRecall\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from datasets import Dataset\n",
    "\n",
    "# Prepare evaluation data - this simulates running your RAG system on test questions\n",
    "eval_data = {\n",
    "    \"question\": [\n",
    "        \"What is the ReAct pattern?\",\n",
    "        \"How do embeddings capture semantic meaning?\",\n",
    "        \"When should you use multi-agent systems?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"ReAct combines reasoning and acting in a loop. The agent thinks about what to do, takes an action, observes the result, and then thinks again. This allows for iterative problem solving.\",\n",
    "        \"Embeddings map text to dense vectors in a high-dimensional space where similar meanings are positioned close together. The model learns these representations during training on large text corpora.\",\n",
    "        \"Multi-agent systems work best when you have distinct, specialized tasks that can be handled by focused agents. They're useful when tasks decompose naturally into parallel or sequential components.\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        [\"The ReAct pattern alternates between reasoning steps and action steps. In the reasoning phase, the agent analyzes the situation. In the action phase, it executes a tool or operation.\"],\n",
    "        [\"Embeddings are dense vector representations that capture semantic relationships. Words with similar meanings have similar embedding vectors.\", \"Semantic similarity is measured by cosine distance between embedding vectors.\"],\n",
    "        [\"Multi-agent architectures provide benefits when tasks are complex and decomposable. Each agent can specialize in a specific capability.\"]\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        \"ReAct (Reasoning + Acting) is a pattern where the agent alternates between thinking and taking actions, allowing iterative refinement.\",\n",
    "        \"Embeddings capture meaning by mapping text to vectors where semantically similar content has similar vector representations.\",\n",
    "        \"Use multi-agent systems when tasks decompose naturally into specialized subtasks that different agents can handle.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(eval_data)\n",
    "\n",
    "# Configure RAGAS using LangChain wrappers\n",
    "ragas_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = [\n",
    "    Faithfulness(llm=ragas_llm),\n",
    "    ResponseRelevancy(llm=ragas_llm, embeddings=ragas_embeddings),\n",
    "    LLMContextPrecisionWithoutReference(llm=ragas_llm),\n",
    "    LLMContextRecall(llm=ragas_llm),\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running RAGAS evaluation (this may take a minute)...\")\n",
    "results = evaluate(\n",
    "    dataset,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "# RAGAS 0.4.x returns per-row results - convert to DataFrame and compute means\n",
    "results_df = results.to_pandas()\n",
    "\n",
    "# Get only numeric columns for metric results\n",
    "metric_columns = results_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "print(f\"\\nResults (mean across {len(results_df)} samples):\")\n",
    "for col in metric_columns:\n",
    "    print(f\"  {col}: {results_df[col].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Interpreting Results and Diagnosing Issues\n",
    "\n",
    "Each metric points to different failure modes:\n",
    "\n",
    "- **Low faithfulness** → Model is hallucinating beyond the context. Fix: stronger grounding prompts, lower temperature, more instruction-following model.\n",
    "- **Low relevancy** → Model isn't understanding or addressing the question. Fix: check prompt structure, add examples.\n",
    "- **Low precision** → Retrieval is pulling irrelevant documents. Fix: better embeddings, improved chunking, add reranking.\n",
    "- **Low recall** → Missing information that should be retrieved. Fix: more documents, better search, query expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## RAGAS Testset Generation\n",
    "\n",
    "RAGAS packages the knowledge graph approach into a convenient API. It automatically extracts entities and relationships from your documents and generates diverse question types.\n",
    "\n",
    "Note: RAGAS 0.4.x requires wrapped LangChain models via `LangchainLLMWrapper` and `LangchainEmbeddingsWrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def load_reference_documents(docs_path: str = \"documents\", pattern: str = \"ref-*.md\") -> list[Document]:\n",
    "    \"\"\"Load and chunk reference documents for test generation.\"\"\"\n",
    "    \n",
    "    documents_dir = Path(docs_path)\n",
    "    guide_files = sorted(documents_dir.glob(pattern))\n",
    "    \n",
    "    if not guide_files:\n",
    "        print(f\"No files found matching {pattern} in {docs_path}\")\n",
    "        return []\n",
    "    \n",
    "    # Use larger chunks for test generation\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    for filepath in guide_files[:3]:  # Limit to 3 files for demo\n",
    "        loader = TextLoader(str(filepath))\n",
    "        documents = loader.load()\n",
    "        chunks = splitter.split_documents(documents)\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"  Loaded {len(chunks)} chunks from {filepath.name}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def create_testset_generator() -> TestsetGenerator:\n",
    "    \"\"\"Initialize RAGAS testset generator.\n",
    "    \n",
    "    Uses gpt-4o-mini to avoid rate limits - RAGAS makes many LLM calls during\n",
    "    knowledge graph extraction.\n",
    "    \"\"\"\n",
    "    # Use LangChain wrappers (same approach as assignment notebook)\n",
    "    generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "    generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "    \n",
    "    return TestsetGenerator(\n",
    "        llm=generator_llm,\n",
    "        embedding_model=generator_embeddings\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"RAGAS testset generator functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "print(\"Loading reference documents...\")\n",
    "documents = load_reference_documents(\"documents\", \"ref-*.md\")\n",
    "print(f\"Total chunks: {len(documents)}\")\n",
    "\n",
    "if documents:\n",
    "    print(\"\\nGenerating testset (this may take a few minutes)...\")\n",
    "    generator = create_testset_generator()\n",
    "    \n",
    "    # Generate a small testset for demo\n",
    "    testset = generator.generate_with_langchain_docs(\n",
    "        documents=documents[:10],  # Limit chunks for speed\n",
    "        testset_size=5,  # Small for demo\n",
    "    )\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    test_df = testset.to_pandas()\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    if \"user_input\" in test_df.columns:\n",
    "        test_df = test_df.rename(columns={\"user_input\": \"question\"})\n",
    "    if \"reference\" in test_df.columns:\n",
    "        test_df = test_df.rename(columns={\"reference\": \"ground_truth\"})\n",
    "    \n",
    "    print(f\"\\nGenerated {len(test_df)} test cases:\")\n",
    "    display_cols = [\"question\", \"synthesizer_name\"] if \"synthesizer_name\" in test_df.columns else [\"question\"]\n",
    "    print(test_df[display_cols].head())\n",
    "else:\n",
    "    print(\"No documents loaded. Skipping testset generation.\")\n",
    "    test_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Difficulty-Based Test Generation\n",
    "\n",
    "Different difficulty levels stress different parts of your system:\n",
    "- **Easy**: Direct lookup questions where the answer appears almost verbatim\n",
    "- **Medium**: Requires understanding, paraphrasing, connecting ideas\n",
    "- **Hard**: Edge cases, exceptions, synthesis across multiple concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_difficulty_spectrum(\n",
    "    topic: str,\n",
    "    documents: str,\n",
    "    llm: ChatOpenAI\n",
    ") -> dict[str, list]:\n",
    "    \"\"\"Generate questions across difficulty levels.\"\"\"\n",
    "    \n",
    "    test_cases = {\"easy\": [], \"medium\": [], \"hard\": []}\n",
    "    doc_sample = documents[:2000] if isinstance(documents, str) else \"\\n\\n\".join(str(d) for d in documents)[:2000]\n",
    "    \n",
    "    # Easy: Direct lookup questions\n",
    "    easy_prompt = f\"\"\"Based on these documents about {topic}, generate 3 questions \n",
    "where the answer appears almost verbatim in the text. These should be \n",
    "straightforward retrieval tasks.\n",
    "\n",
    "Documents:\n",
    "{doc_sample}\n",
    "\n",
    "Format as JSON: [{{\"question\": \"...\", \"answer\": \"...\", \"source_quote\": \"...\"}}]\"\"\"\n",
    "    \n",
    "    response = llm.invoke(easy_prompt)\n",
    "    content = extract_json(response.content)\n",
    "    try:\n",
    "        test_cases[\"easy\"] = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        test_cases[\"easy\"] = []\n",
    "    \n",
    "    # Medium: Requires understanding and paraphrasing\n",
    "    medium_prompt = f\"\"\"Generate 3 questions about {topic} where answering requires:\n",
    "- Understanding concepts, not just finding text\n",
    "- Paraphrasing information from the documents\n",
    "- Connecting two related ideas\n",
    "\n",
    "The answer shouldn't be copy-pasteable from the documents.\n",
    "\n",
    "Documents:\n",
    "{doc_sample}\n",
    "\n",
    "Format as JSON: [{{\"question\": \"...\", \"answer\": \"...\", \"reasoning\": \"...\"}}]\"\"\"\n",
    "    \n",
    "    response = llm.invoke(medium_prompt)\n",
    "    content = extract_json(response.content)\n",
    "    try:\n",
    "        test_cases[\"medium\"] = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        test_cases[\"medium\"] = []\n",
    "    \n",
    "    # Hard: Edge cases, exceptions, nuanced understanding\n",
    "    hard_prompt = f\"\"\"Generate 2 challenging questions about {topic}:\n",
    "- Questions about edge cases or exceptions to general rules\n",
    "- Questions requiring synthesis across multiple concepts\n",
    "- Questions where naive answers would be incomplete or wrong\n",
    "\n",
    "Documents:\n",
    "{doc_sample}\n",
    "\n",
    "Format as JSON: [{{\"question\": \"...\", \"answer\": \"...\", \"why_hard\": \"...\"}}]\"\"\"\n",
    "    \n",
    "    response = llm.invoke(hard_prompt)\n",
    "    content = extract_json(response.content)\n",
    "    try:\n",
    "        test_cases[\"hard\"] = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        test_cases[\"hard\"] = []\n",
    "    \n",
    "    return test_cases\n",
    "\n",
    "\n",
    "# Test difficulty spectrum generation\n",
    "sample_text = \"\"\"RAG (Retrieval-Augmented Generation) combines retrieval with generation.\n",
    "The indexing phase involves: Load, Chunk, Embed, Store.\n",
    "The query phase involves: Embed query, Search, Retrieve, Generate.\n",
    "Chunking strategies include fixed-size chunks, semantic chunking, and sentence-based chunking.\n",
    "Vector databases like Qdrant store embeddings for similarity search.\"\"\"\n",
    "\n",
    "difficulty_cases = generate_difficulty_spectrum(\"RAG systems\", sample_text, llm)\n",
    "print(f\"Generated test cases by difficulty:\")\n",
    "for level, cases in difficulty_cases.items():\n",
    "    print(f\"  {level}: {len(cases)} questions\")\n",
    "    for c in cases[:1]:\n",
    "        print(f\"    - {c.get('question', 'N/A')[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Adversarial Test Cases\n",
    "\n",
    "Adversarial cases reveal how your system handles the unexpected:\n",
    "- **Unanswerable**: Questions that seem related but can't be answered from the documents\n",
    "- **Ambiguous**: Questions that could be interpreted multiple ways\n",
    "- **Misleading**: Questions with false premises or incorrect assumptions\n",
    "- **Out-of-scope**: Questions related to the topic but outside document coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversarial_cases(\n",
    "    topic: str,\n",
    "    documents: str,\n",
    "    llm: ChatOpenAI\n",
    ") -> list[dict]:\n",
    "    \"\"\"Generate test cases designed to find weaknesses.\"\"\"\n",
    "    \n",
    "    doc_sample = documents[:1000] if isinstance(documents, str) else \"\\n\\n\".join(str(d) for d in documents)[:1000]\n",
    "    \n",
    "    prompt = f\"\"\"Generate adversarial test cases for a Q&A system about {topic}.\n",
    "\n",
    "Include:\n",
    "\n",
    "1. UNANSWERABLE questions (2 examples)\n",
    "   Questions that seem related but can't actually be answered from the documents.\n",
    "   The system should recognize it doesn't have the information.\n",
    "\n",
    "2. AMBIGUOUS questions (2 examples)\n",
    "   Questions that could be interpreted multiple ways.\n",
    "   The system should ask for clarification or acknowledge ambiguity.\n",
    "\n",
    "3. MISLEADING questions (2 examples)\n",
    "   Questions with false premises or incorrect assumptions.\n",
    "   The system should correct the assumption, not just answer.\n",
    "\n",
    "4. OUT_OF_SCOPE questions (2 examples)\n",
    "   Questions related to {topic} but outside what the documents cover.\n",
    "   The system should acknowledge limitations.\n",
    "\n",
    "Documents summary: {doc_sample}\n",
    "\n",
    "For each case, specify:\n",
    "- question: The adversarial question\n",
    "- category: Which type (unanswerable, ambiguous, misleading, out_of_scope)\n",
    "- expected_behavior: How a good system should respond\n",
    "- failure_mode: What a bad response would look like\n",
    "\n",
    "Return as JSON array.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    content = extract_json(response.content)\n",
    "    \n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Warning: Could not parse adversarial cases\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Generate adversarial cases\n",
    "adversarial_cases = generate_adversarial_cases(\"RAG systems\", sample_text, llm)\n",
    "print(f\"Generated {len(adversarial_cases)} adversarial test cases:\")\n",
    "for case in adversarial_cases[:4]:\n",
    "    print(f\"  [{case.get('category', 'unknown')}] {case.get('question', 'N/A')[:50]}...\")\n",
    "    print(f\"    Expected: {case.get('expected_behavior', 'N/A')[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Coverage Verification\n",
    "\n",
    "After generating test cases, verify you have adequate coverage across topics and question types. Gaps indicate areas where you need additional test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_coverage(\n",
    "    test_cases: list[dict],\n",
    "    expected_topics: list[str],\n",
    "    expected_types: list[str]\n",
    ") -> dict:\n",
    "    \"\"\"Analyze whether test cases cover expected dimensions.\"\"\"\n",
    "    \n",
    "    # Track what's covered\n",
    "    topic_coverage = {topic: 0 for topic in expected_topics}\n",
    "    type_coverage = {t: 0 for t in expected_types}\n",
    "    \n",
    "    for case in test_cases:\n",
    "        # Check topic coverage\n",
    "        question = case.get(\"question\", \"\").lower()\n",
    "        for topic in expected_topics:\n",
    "            if topic.lower() in question:\n",
    "                topic_coverage[topic] += 1\n",
    "        \n",
    "        # Check type coverage\n",
    "        case_type = case.get(\"type\", case.get(\"category\", \"unknown\"))\n",
    "        if case_type in type_coverage:\n",
    "            type_coverage[case_type] += 1\n",
    "    \n",
    "    # Identify gaps\n",
    "    gaps = {\n",
    "        \"missing_topics\": [t for t, c in topic_coverage.items() if c == 0],\n",
    "        \"underrepresented_topics\": [t for t, c in topic_coverage.items() if 0 < c < 3],\n",
    "        \"missing_types\": [t for t, c in type_coverage.items() if c == 0]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"topic_coverage\": topic_coverage,\n",
    "        \"type_coverage\": type_coverage,\n",
    "        \"gaps\": gaps,\n",
    "        \"total_cases\": len(test_cases)\n",
    "    }\n",
    "\n",
    "\n",
    "# Collect all test cases generated so far\n",
    "all_test_cases = []\n",
    "\n",
    "# Add adversarial cases\n",
    "if adversarial_cases:\n",
    "    all_test_cases.extend(adversarial_cases)\n",
    "\n",
    "# Add difficulty-based cases\n",
    "for level in [\"easy\", \"medium\", \"hard\"]:\n",
    "    for c in difficulty_cases.get(level, []):\n",
    "        all_test_cases.append({\"type\": level, **c})\n",
    "\n",
    "if all_test_cases:\n",
    "    coverage = analyze_coverage(\n",
    "        all_test_cases,\n",
    "        expected_topics=[\"RAG\", \"retrieval\", \"embedding\", \"chunking\", \"generation\"],\n",
    "        expected_types=[\"easy\", \"medium\", \"hard\", \"unanswerable\", \"ambiguous\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Coverage Analysis ({coverage['total_cases']} total cases):\")\n",
    "    print(f\"\\nTopic coverage:\")\n",
    "    for topic, count in coverage[\"topic_coverage\"].items():\n",
    "        print(f\"  {topic}: {count}\")\n",
    "    \n",
    "    print(f\"\\nType coverage:\")\n",
    "    for qtype, count in coverage[\"type_coverage\"].items():\n",
    "        print(f\"  {qtype}: {count}\")\n",
    "    \n",
    "    print(f\"\\nGaps identified:\")\n",
    "    print(f\"  Missing topics: {coverage['gaps']['missing_topics']}\")\n",
    "    print(f\"  Underrepresented: {coverage['gaps']['underrepresented_topics']}\")\n",
    "    print(f\"  Missing types: {coverage['gaps']['missing_types']}\")\n",
    "else:\n",
    "    print(\"No test cases available for coverage analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Data Quality Validation\n",
    "\n",
    "Not every generated test case is good. Build a validation pipeline to filter out low-quality cases: questions that don't make sense, incorrect expected answers, or near-duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_test_case(case: dict, documents: str, llm: ChatOpenAI) -> dict:\n",
    "    \"\"\"Validate a generated test case.\"\"\"\n",
    "    \n",
    "    doc_sample = documents[:2000] if isinstance(documents, str) else \"\\n\\n\".join(str(d) for d in documents)[:2000]\n",
    "    \n",
    "    validation_prompt = f\"\"\"Evaluate this test case for quality:\n",
    "\n",
    "Question: {case.get('question', 'N/A')}\n",
    "Expected Answer: {case.get('answer', case.get('expected_answer', 'N/A'))}\n",
    "\n",
    "Reference Documents:\n",
    "{doc_sample}\n",
    "\n",
    "Evaluate (score 1-5 each):\n",
    "1. Is the question clear and unambiguous?\n",
    "2. Is the expected answer actually correct based on the documents?\n",
    "3. Can the question be reasonably answered from the documents?\n",
    "4. Is this a useful test case (not trivial, not impossible)?\n",
    "\n",
    "Return as JSON: {{\"scores\": {{\"clarity\": N, \"correctness\": N, \"answerability\": N, \"usefulness\": N}}, \"issues\": [...], \"recommendation\": \"keep|revise|discard\"}}\"\"\"\n",
    "    \n",
    "    response = llm.invoke(validation_prompt)\n",
    "    content = extract_json(response.content)\n",
    "    \n",
    "    try:\n",
    "        validation = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        validation = {\"scores\": {\"clarity\": 3, \"correctness\": 3, \"answerability\": 3, \"usefulness\": 3}, \n",
    "                      \"issues\": [\"Could not parse validation\"], \"recommendation\": \"revise\"}\n",
    "    \n",
    "    scores = validation.get(\"scores\", {})\n",
    "    quality_score = sum(scores.values()) / len(scores) if scores else 0\n",
    "    \n",
    "    return {\n",
    "        **case,\n",
    "        \"validation\": validation,\n",
    "        \"quality_score\": quality_score\n",
    "    }\n",
    "\n",
    "\n",
    "def filter_test_cases(\n",
    "    cases: list[dict], \n",
    "    min_quality: float = 3.5\n",
    ") -> list[dict]:\n",
    "    \"\"\"Keep only high-quality test cases.\"\"\"\n",
    "    return [c for c in cases if c.get(\"quality_score\", 0) >= min_quality]\n",
    "\n",
    "\n",
    "# Validate a sample test case\n",
    "if all_test_cases:\n",
    "    sample_case = all_test_cases[0]\n",
    "    validated = validate_test_case(sample_case, sample_text, llm)\n",
    "    print(f\"Validation result:\")\n",
    "    print(f\"  Question: {validated.get('question', 'N/A')[:50]}...\")\n",
    "    print(f\"  Quality score: {validated.get('quality_score', 0):.2f}\")\n",
    "    print(f\"  Recommendation: {validated.get('validation', {}).get('recommendation', 'N/A')}\")\n",
    "    print(f\"  Issues: {validated.get('validation', {}).get('issues', [])}\")\n",
    "else:\n",
    "    print(\"No test cases to validate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Metrics-Driven Development\n",
    "\n",
    "Evaluation isn't a one-time activity—it's about using measurements to drive systematic improvement. The loop:\n",
    "\n",
    "1. **Measure**: Run your evaluation suite and record scores\n",
    "2. **Analyze**: Identify what's causing low scores\n",
    "3. **Hypothesize**: Form a theory about what would improve things\n",
    "4. **Change**: Implement your proposed improvement\n",
    "5. **Measure again**: See if it actually helped\n",
    "6. **Iterate**: Keep going until you hit your targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationTracker:\n",
    "    \"\"\"Track evaluation results over time.\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name: str):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.runs = []\n",
    "    \n",
    "    def record_run(\n",
    "        self, \n",
    "        version: str, \n",
    "        metrics: dict, \n",
    "        config: dict,\n",
    "        notes: str = \"\"\n",
    "    ):\n",
    "        \"\"\"Record an evaluation run.\"\"\"\n",
    "        self.runs.append({\n",
    "            \"timestamp\": datetime.now(UTC).isoformat(),\n",
    "            \"version\": version,\n",
    "            \"metrics\": metrics,\n",
    "            \"config\": config,\n",
    "            \"notes\": notes\n",
    "        })\n",
    "    \n",
    "    def compare_versions(self, v1: str, v2: str) -> dict:\n",
    "        \"\"\"Compare metrics between two versions.\"\"\"\n",
    "        run1 = next((r for r in self.runs if r[\"version\"] == v1), None)\n",
    "        run2 = next((r for r in self.runs if r[\"version\"] == v2), None)\n",
    "        \n",
    "        if not run1 or not run2:\n",
    "            return {\"error\": \"Version not found\"}\n",
    "        \n",
    "        comparison = {}\n",
    "        for metric in run1[\"metrics\"]:\n",
    "            old = run1[\"metrics\"][metric]\n",
    "            new = run2[\"metrics\"][metric]\n",
    "            comparison[metric] = {\n",
    "                \"old\": old,\n",
    "                \"new\": new,\n",
    "                \"change\": new - old,\n",
    "                \"percent_change\": (new - old) / old * 100 if old != 0 else float('inf')\n",
    "            }\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def get_trend(self, metric: str) -> list:\n",
    "        \"\"\"Get trend of a metric over time.\"\"\"\n",
    "        return [\n",
    "            {\"version\": r[\"version\"], \"value\": r[\"metrics\"].get(metric)}\n",
    "            for r in sorted(self.runs, key=lambda x: x[\"timestamp\"])\n",
    "        ]\n",
    "    \n",
    "    def check_regression(self, metric: str, threshold: float = 0.05) -> bool:\n",
    "        \"\"\"Check if recent changes caused regression beyond threshold.\"\"\"\n",
    "        if len(self.runs) < 2:\n",
    "            return False\n",
    "        \n",
    "        recent = self.runs[-1][\"metrics\"].get(metric, 0)\n",
    "        previous = self.runs[-2][\"metrics\"].get(metric, 0)\n",
    "        \n",
    "        return (previous - recent) / previous > threshold if previous > 0 else False\n",
    "\n",
    "\n",
    "# Demonstrate the tracker\n",
    "tracker = EvaluationTracker(\"rag-evaluation\")\n",
    "\n",
    "# Simulate recording some runs\n",
    "tracker.record_run(\n",
    "    version=\"v1.0\",\n",
    "    metrics={\"faithfulness\": 0.78, \"relevancy\": 0.82, \"precision\": 0.71},\n",
    "    config={\"model\": \"gpt-4o-mini\", \"k\": 3},\n",
    "    notes=\"Baseline\"\n",
    ")\n",
    "\n",
    "tracker.record_run(\n",
    "    version=\"v1.1\",\n",
    "    metrics={\"faithfulness\": 0.85, \"relevancy\": 0.84, \"precision\": 0.75},\n",
    "    config={\"model\": \"gpt-4o-mini\", \"k\": 5},\n",
    "    notes=\"Increased retrieval k\"\n",
    ")\n",
    "\n",
    "tracker.record_run(\n",
    "    version=\"v1.2\",\n",
    "    metrics={\"faithfulness\": 0.88, \"relevancy\": 0.86, \"precision\": 0.79},\n",
    "    config={\"model\": \"gpt-4o-mini\", \"k\": 5, \"rerank\": True},\n",
    "    notes=\"Added reranking\"\n",
    ")\n",
    "\n",
    "print(\"Evaluation Tracker Demo:\")\n",
    "print(f\"\\nRuns recorded: {len(tracker.runs)}\")\n",
    "\n",
    "comparison = tracker.compare_versions(\"v1.0\", \"v1.2\")\n",
    "print(f\"\\nComparison v1.0 -> v1.2:\")\n",
    "for metric, data in comparison.items():\n",
    "    print(f\"  {metric}: {data['old']:.3f} -> {data['new']:.3f} ({data['percent_change']:+.1f}%)\")\n",
    "\n",
    "print(f\"\\nFaithfulness trend:\")\n",
    "for point in tracker.get_trend(\"faithfulness\"):\n",
    "    print(f\"  {point['version']}: {point['value']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityThresholds:\n",
    "    \"\"\"Define quality thresholds for the system.\"\"\"\n",
    "    \n",
    "    # Minimum acceptable scores (system is broken below this)\n",
    "    MINIMUM = {\n",
    "        \"faithfulness\": 0.7,\n",
    "        \"relevance\": 0.6,\n",
    "        \"retrieval_precision\": 0.5\n",
    "    }\n",
    "    \n",
    "    # Target scores (what we're aiming for)\n",
    "    TARGET = {\n",
    "        \"faithfulness\": 0.9,\n",
    "        \"relevance\": 0.85,\n",
    "        \"retrieval_precision\": 0.8\n",
    "    }\n",
    "    \n",
    "    # Stretch goals (excellence)\n",
    "    STRETCH = {\n",
    "        \"faithfulness\": 0.95,\n",
    "        \"relevance\": 0.92,\n",
    "        \"retrieval_precision\": 0.9\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def assess(cls, metrics: dict) -> str:\n",
    "        \"\"\"Assess current metrics against thresholds.\"\"\"\n",
    "        \n",
    "        # Check for any below minimum\n",
    "        for metric, min_val in cls.MINIMUM.items():\n",
    "            if metrics.get(metric, 0) < min_val:\n",
    "                return f\"CRITICAL: {metric} below minimum threshold ({metrics.get(metric, 0):.3f} < {min_val})\"\n",
    "        \n",
    "        # Check if meeting targets\n",
    "        meeting_targets = all(\n",
    "            metrics.get(m, 0) >= t \n",
    "            for m, t in cls.TARGET.items()\n",
    "        )\n",
    "        \n",
    "        if meeting_targets:\n",
    "            return \"GOOD: Meeting all targets\"\n",
    "        else:\n",
    "            below_target = [\n",
    "                m for m, t in cls.TARGET.items() \n",
    "                if metrics.get(m, 0) < t\n",
    "            ]\n",
    "            return f\"IMPROVING: Below target on {', '.join(below_target)}\"\n",
    "\n",
    "\n",
    "# Test threshold assessment\n",
    "test_metrics = {\"faithfulness\": 0.85, \"relevance\": 0.88, \"retrieval_precision\": 0.75}\n",
    "assessment = QualityThresholds.assess(test_metrics)\n",
    "print(f\"Metrics: {test_metrics}\")\n",
    "print(f\"Assessment: {assessment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## LangSmith Integration\n",
    "\n",
    "LangSmith provides infrastructure for managing evaluation datasets, running evaluations at scale, and tracking results over time. This section demonstrates graceful handling when LangSmith is not configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_langsmith_client():\n",
    "    \"\"\"Get LangSmith client with graceful error handling.\"\"\"\n",
    "    try:\n",
    "        from langsmith import Client\n",
    "        client = Client()\n",
    "        # Test connection\n",
    "        list(client.list_datasets(limit=1))\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"LangSmith not available: {e}\")\n",
    "        print(\"Skipping LangSmith-specific functionality.\")\n",
    "        print(\"To use LangSmith, set LANGSMITH_API_KEY in your environment.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Check LangSmith availability\n",
    "langsmith_client = get_langsmith_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dataset(client, dataset_name: str, examples: list[dict]):\n",
    "    \"\"\"Create a LangSmith dataset for evaluation.\"\"\"\n",
    "    if client is None:\n",
    "        print(\"LangSmith not available. Returning examples as local dataset.\")\n",
    "        return {\"name\": dataset_name, \"examples\": examples, \"local\": True}\n",
    "    \n",
    "    try:\n",
    "        # Create dataset\n",
    "        dataset = client.create_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            description=\"Synthetic test cases for RAG evaluation\"\n",
    "        )\n",
    "        \n",
    "        # Add examples\n",
    "        client.create_examples(\n",
    "            inputs=[e[\"inputs\"] for e in examples],\n",
    "            outputs=[e[\"outputs\"] for e in examples],\n",
    "            dataset_id=dataset.id\n",
    "        )\n",
    "        \n",
    "        return {\"name\": dataset_name, \"id\": dataset.id, \"local\": False}\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataset: {e}\")\n",
    "        return {\"name\": dataset_name, \"examples\": examples, \"local\": True, \"error\": str(e)}\n",
    "\n",
    "\n",
    "# Prepare example data\n",
    "example_data = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is RAG?\"},\n",
    "        \"outputs\": {\"answer\": \"RAG (Retrieval-Augmented Generation) combines retrieval with generation...\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do embeddings work?\"},\n",
    "        \"outputs\": {\"answer\": \"Embeddings map text to vectors in a space where similar meanings are close together...\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create dataset (will use local fallback if LangSmith unavailable)\n",
    "dataset_result = create_evaluation_dataset(\n",
    "    langsmith_client,\n",
    "    f\"rag-evaluation-demo-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    example_data\n",
    ")\n",
    "\n",
    "print(f\"Dataset created: {dataset_result['name']}\")\n",
    "print(f\"Local mode: {dataset_result.get('local', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_langsmith_evaluation(client, dataset_name: str, system_func, evaluators: list):\n",
    "    \"\"\"Run evaluation using LangSmith.\"\"\"\n",
    "    if client is None:\n",
    "        print(\"LangSmith not available. Simulating evaluation locally...\")\n",
    "        # Simulate local evaluation\n",
    "        return {\n",
    "            \"status\": \"simulated\",\n",
    "            \"message\": \"LangSmith not configured. In production, this would run against the dataset.\",\n",
    "            \"dataset\": dataset_name\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        from langsmith.evaluation import evaluate\n",
    "        \n",
    "        results = evaluate(\n",
    "            system_func,\n",
    "            data=dataset_name,\n",
    "            evaluators=evaluators,\n",
    "            experiment_prefix=\"demo\"\n",
    "        )\n",
    "        \n",
    "        return {\"status\": \"completed\", \"results\": results}\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {e}\")\n",
    "        return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "\n",
    "# Define a simple system function for evaluation\n",
    "def simple_rag_system(inputs: dict) -> dict:\n",
    "    \"\"\"Simple RAG system for demonstration.\"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "    # In production, this would call your actual RAG pipeline\n",
    "    response = llm.invoke(f\"Answer briefly: {question}\")\n",
    "    return {\"answer\": response.content, \"contexts\": [\"[Demo context]\"]}\n",
    "\n",
    "\n",
    "# Run evaluation (will simulate if LangSmith unavailable)\n",
    "eval_result = run_langsmith_evaluation(\n",
    "    langsmith_client,\n",
    "    dataset_result['name'],\n",
    "    simple_rag_system,\n",
    "    []  # Evaluators would be defined here\n",
    ")\n",
    "\n",
    "print(f\"Evaluation status: {eval_result['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Bringing It All Together\n",
    "\n",
    "We've covered the complete evaluation infrastructure:\n",
    "\n",
    "1. **Synthetic Data Generation** - Creating test cases at scale\n",
    "2. **RAGAS Framework** - Automated metrics for RAG systems (faithfulness, relevancy, precision, recall)\n",
    "3. **RAGAS Testset Generation** - Automatic test generation from documents using knowledge graphs\n",
    "4. **Difficulty Spectrum** - Testing across easy, medium, and hard cases\n",
    "5. **Adversarial Cases** - Finding system weaknesses\n",
    "6. **Coverage Analysis** - Ensuring comprehensive testing\n",
    "7. **Quality Validation** - Filtering low-quality test cases\n",
    "8. **Metrics-Driven Development** - Using measurements to drive improvement\n",
    "9. **LangSmith Integration** - Production-grade evaluation infrastructure\n",
    "\n",
    "The key insight: **evaluation transforms intuition into data**. Instead of \"I think it's working,\" you can say \"faithfulness is 0.88 and improving.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE EVALUATION PIPELINE DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Load documents\n",
    "print(\"\\n1. Loading documents...\")\n",
    "docs = load_reference_documents(\"documents\", \"ref-*.md\")\n",
    "print(f\"   Loaded {len(docs)} document chunks\")\n",
    "\n",
    "# 2. Generate test cases\n",
    "print(\"\\n2. Generating test cases...\")\n",
    "if docs:\n",
    "    # Generate a few test cases\n",
    "    simple_cases = generate_test_cases(\n",
    "        topic=\"AI Engineering\",\n",
    "        num_cases=4,\n",
    "        difficulty_distribution={\"easy\": 2, \"medium\": 2},\n",
    "        llm=llm\n",
    "    )\n",
    "    print(f\"   Generated {len(simple_cases)} test cases\")\n",
    "else:\n",
    "    simple_cases = []\n",
    "    print(\"   No documents available\")\n",
    "\n",
    "# 3. Analyze coverage\n",
    "print(\"\\n3. Analyzing coverage...\")\n",
    "if simple_cases:\n",
    "    coverage = analyze_coverage(\n",
    "        simple_cases,\n",
    "        expected_topics=[\"RAG\", \"embedding\", \"retrieval\"],\n",
    "        expected_types=[\"easy\", \"medium\"]\n",
    "    )\n",
    "    print(f\"   Total cases: {coverage['total_cases']}\")\n",
    "    print(f\"   Missing topics: {coverage['gaps']['missing_topics']}\")\n",
    "\n",
    "# 4. Track metrics\n",
    "print(\"\\n4. Tracking metrics...\")\n",
    "demo_tracker = EvaluationTracker(\"demo-experiment\")\n",
    "demo_tracker.record_run(\n",
    "    version=\"demo-v1\",\n",
    "    metrics={\"faithfulness\": 0.82, \"relevancy\": 0.85},\n",
    "    config={\"model\": \"gpt-4o-mini\"},\n",
    "    notes=\"Demo run\"\n",
    ")\n",
    "print(f\"   Recorded run: {demo_tracker.runs[-1]['version']}\")\n",
    "\n",
    "# 5. Assess against thresholds\n",
    "print(\"\\n5. Assessing against thresholds...\")\n",
    "assessment = QualityThresholds.assess({\n",
    "    \"faithfulness\": 0.82,\n",
    "    \"relevance\": 0.85,\n",
    "    \"retrieval_precision\": 0.78\n",
    "})\n",
    "print(f\"   {assessment}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluation infrastructure ready for production use!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
