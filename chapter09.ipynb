{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Retrieval\n",
    "\n",
    "## Beyond Basic Vector Search\n",
    "\n",
    "This notebook explores advanced retrieval techniques that address the limitations of basic dense vector search, and evaluates them with concrete metrics.\n",
    "\n",
    "```\n",
    "    ┌─────────────────────────────────────────────────────────────┐\n",
    "    │              Advanced Retrieval Techniques                  │\n",
    "    │                                                             │\n",
    "    │   ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │\n",
    "    │   │ Hybrid   │  │Reranking │  │  Query   │  │   RAG    │    │\n",
    "    │   │ Search   │  │ (Cohere) │  │ Expansion│  │  Fusion  │    │\n",
    "    │   │ BM25+Vec │  │          │  │          │  │          │    │\n",
    "    │   └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘    │\n",
    "    │        │             │             │             │          │\n",
    "    │        v             v             v             v          │\n",
    "    │   Exact + dense   Precision    Vocabulary    Comprehensive  │\n",
    "    │   matching        reordering   bridging      coverage       │\n",
    "    │                                                             │\n",
    "    │   ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │\n",
    "    │   │ Semantic │  │Contextual│  │Relevance │  │Systematic│    │\n",
    "    │   │ Chunking │  │Compress  │  │ Filter   │  │Comparison│    │\n",
    "    │   └──────────┘  └──────────┘  └──────────┘  └──────────┘    │\n",
    "    └─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Topics covered:\n",
    "- Reciprocal Rank Fusion (RRF) for combining ranked lists.\n",
    "- Hybrid search (BM25 + dense vectors).\n",
    "- Reranking with Cohere cross-encoders.\n",
    "- Query expansion and RAG-Fusion.\n",
    "- Semantic chunking and structure-aware splitting.\n",
    "- Contextual compression and relevance filtering.\n",
    "- Hierarchical retrieval.\n",
    "- Systematic retriever comparison with RAGAS evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Make sure you have the required packages installed:\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "Note: This notebook uses `nest_asyncio` for RAGAS compatibility in Jupyter. Some sections require optional dependencies (`cohere`, `langchain-experimental`, `rank-bm25`). The notebook handles missing optional packages gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "# RAGAS requires nest_asyncio in Jupyter environments\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "# Initialize the LLM and embeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "def extract_json(content: str) -> str:\n",
    "    \"\"\"Extract JSON from LLM response, handling markdown code blocks.\"\"\"\n",
    "    if \"```json\" in content:\n",
    "        content = content.split(\"```json\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in content:\n",
    "        content = content.split(\"```\")[1].split(\"```\")[0]\n",
    "    return content.strip()\n",
    "\n",
    "\n",
    "# Check optional dependencies\n",
    "HAS_RANK_BM25 = False\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi  # noqa: F401\n",
    "    HAS_RANK_BM25 = True\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "HAS_COHERE = False\n",
    "try:\n",
    "    import cohere  # noqa: F401\n",
    "    HAS_COHERE = True\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "HAS_SEMANTIC_CHUNKER = False\n",
    "try:\n",
    "    from langchain_experimental.text_splitter import SemanticChunker  # noqa: F401\n",
    "    HAS_SEMANTIC_CHUNKER = True\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "print(\"Optional dependencies:\")\n",
    "print(f\"  rank_bm25 (for BM25Retriever): {'Available' if HAS_RANK_BM25 else 'Not installed - run: uv add rank-bm25'}\")\n",
    "print(f\"  cohere (for reranking):         {'Available' if HAS_COHERE else 'Not installed - run: uv add cohere'}\")\n",
    "print(f\"  langchain-experimental:         {'Available' if HAS_SEMANTIC_CHUNKER else 'Not installed - run: uv add langchain-experimental'}\")\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Indexing Documents\n",
    "\n",
    "Before exploring retrieval techniques, we need a document corpus and a vector store. We load the reference guides from the `documents/` directory, chunk them, embed them, and store them in an in-memory Qdrant vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference documents\n",
    "documents_dir = Path(\"documents\")\n",
    "guide_files = sorted(documents_dir.glob(\"ref-*.md\"))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "raw_documents = []  # Keep full docs for BM25\n",
    "\n",
    "for filepath in guide_files:\n",
    "    loader = TextLoader(str(filepath))\n",
    "    docs = loader.load()\n",
    "    raw_documents.extend(docs)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"  Loaded {len(chunks):3d} chunks from {filepath.name}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_chunks)} chunks from {len(guide_files)} documents\")\n",
    "\n",
    "# Create in-memory Qdrant vector store\n",
    "COLLECTION_NAME = \"chapter9_retrieval\"\n",
    "\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# Index all chunks\n",
    "vector_store.add_documents(all_chunks)\n",
    "print(f\"Indexed {len(all_chunks)} chunks in Qdrant vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "Hybrid search combines results from two completely different scoring systems. Dense search returns cosine similarity (0-1). BM25 returns scores on a completely different scale. You cannot just average them.\n",
    "\n",
    "RRF solves this by combining **rankings** instead of scores. If a document appears highly ranked in multiple result lists, it is probably relevant. The formula:\n",
    "\n",
    "```\n",
    "score(doc) = sum( 1 / (k + rank) )  for each list where doc appears\n",
    "```\n",
    "\n",
    "The constant `k` (typically 60) prevents the top-ranked document from dominating too heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(\n",
    "    result_lists: list[list],\n",
    "    k: int = 60\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Combine multiple ranked result lists using RRF.\n",
    "    \n",
    "    Args:\n",
    "        result_lists: List of ranked result lists, each containing (doc, score) tuples\n",
    "        k: Ranking constant (default 60, higher values reduce impact of rank differences)\n",
    "    \n",
    "    Returns:\n",
    "        Combined ranked list of documents\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    for results in result_lists:\n",
    "        for rank, (doc, _) in enumerate(results):\n",
    "            # Use content hash as document ID\n",
    "            doc_id = hash(doc.page_content)\n",
    "            \n",
    "            if doc_id not in scores:\n",
    "                scores[doc_id] = {\"doc\": doc, \"score\": 0}\n",
    "            \n",
    "            # RRF formula: 1 / (k + rank + 1)\n",
    "            scores[doc_id][\"score\"] += 1 / (k + rank + 1)\n",
    "    \n",
    "    # Sort by combined score\n",
    "    sorted_results = sorted(\n",
    "        scores.values(),\n",
    "        key=lambda x: x[\"score\"],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    return [item[\"doc\"] for item in sorted_results]\n",
    "\n",
    "\n",
    "print(\"reciprocal_rank_fusion() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: RRF with two mock result lists\n",
    "doc_a = Document(page_content=\"Document about embeddings and vector search\")\n",
    "doc_b = Document(page_content=\"Document about BM25 keyword matching\")\n",
    "doc_c = Document(page_content=\"Document about hybrid search approaches\")\n",
    "doc_d = Document(page_content=\"Document about search engine optimization\")\n",
    "\n",
    "# List 1: dense search ranking\n",
    "list1 = [(doc_a, 0.95), (doc_c, 0.88), (doc_b, 0.82), (doc_d, 0.70)]\n",
    "# List 2: BM25 ranking (different order, different score scale)\n",
    "list2 = [(doc_b, 15.2), (doc_c, 12.1), (doc_d, 8.3), (doc_a, 5.0)]\n",
    "\n",
    "fused = reciprocal_rank_fusion([list1, list2])\n",
    "print(\"RRF fused ranking:\")\n",
    "for i, doc in enumerate(fused):\n",
    "    print(f\"  {i+1}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search\n",
    "\n",
    "Hybrid search combines dense vector retrieval with BM25 sparse keyword matching. Dense search finds semantically similar content. BM25 finds exact term matches. Together they cover each other's blind spots.\n",
    "\n",
    "- **Dense search** handles: paraphrasing, synonyms, conceptual similarity\n",
    "- **BM25** handles: exact identifiers, codes, names, technical terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str,\n",
    "    vector_store: QdrantVectorStore,\n",
    "    documents: list,\n",
    "    k: int = 5,\n",
    "    alpha: float = 0.5\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Combine dense vector search with BM25 keyword search.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        vector_store: Qdrant vector store for dense search\n",
    "        documents: Original documents for BM25 (needs raw text)\n",
    "        k: Number of results to return\n",
    "        alpha: Weight for dense results (1-alpha for sparse)\n",
    "    \"\"\"\n",
    "    # Dense search\n",
    "    dense_results = vector_store.similarity_search_with_score(query, k=k*2)\n",
    "    \n",
    "    # Sparse search with BM25\n",
    "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "    bm25_retriever.k = k * 2\n",
    "    sparse_docs = bm25_retriever.invoke(query)\n",
    "    # Convert to (doc, score) format for RRF\n",
    "    sparse_results = [(doc, 1.0 / (i + 1)) for i, doc in enumerate(sparse_docs)]\n",
    "    \n",
    "    # Combine with RRF\n",
    "    combined = reciprocal_rank_fusion([dense_results, sparse_results])\n",
    "    \n",
    "    return combined[:k]\n",
    "\n",
    "\n",
    "print(\"hybrid_search() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_RANK_BM25:\n",
    "    query = \"How does BM25 keyword matching work?\"\n",
    "    print(f\"Query: {query}\\n\")\n",
    "\n",
    "    # Compare: dense-only vs hybrid\n",
    "    dense_only = vector_store.similarity_search(query, k=5)\n",
    "    print(\"Dense search results:\")\n",
    "    for i, doc in enumerate(dense_only):\n",
    "        source = Path(doc.metadata.get(\"source\", \"unknown\")).name\n",
    "        print(f\"  {i+1}. [{source}] {doc.page_content[:80]}...\")\n",
    "\n",
    "    hybrid_results = hybrid_search(query, vector_store, all_chunks, k=5)\n",
    "    print(f\"\\nHybrid search results:\")\n",
    "    for i, doc in enumerate(hybrid_results):\n",
    "        source = Path(doc.metadata.get(\"source\", \"unknown\")).name\n",
    "        print(f\"  {i+1}. [{source}] {doc.page_content[:80]}...\")\n",
    "else:\n",
    "    print(\"Skipping hybrid search demo: rank_bm25 not installed.\")\n",
    "    print(\"Install with: uv add rank-bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking with Cohere\n",
    "\n",
    "Initial retrieval casts a wide net for recall. Reranking is a second pass that reorders results using a more powerful cross-encoder model for precision.\n",
    "\n",
    "- **Bi-encoder** (embedding model): processes query and document independently -- fast\n",
    "- **Cross-encoder** (reranker): processes query+document jointly -- accurate but slow\n",
    "\n",
    "Reranking 20 candidates takes 100-300ms. It pays off when precision matters more than latency.\n",
    "\n",
    "> Note: This section requires a Cohere API key. Set `COHERE_API_KEY` in your `.env` file. Get a free trial key at https://dashboard.cohere.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_reranking(\n",
    "    query: str,\n",
    "    vector_store,\n",
    "    k: int = 5,\n",
    "    initial_k: int = 20\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Two-stage retrieval: broad search then precise reranking.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        vector_store: Vector store for initial retrieval\n",
    "        k: Final number of results\n",
    "        initial_k: Candidates to fetch for reranking\n",
    "    \"\"\"\n",
    "    # Stage 1: Broad retrieval\n",
    "    candidates = vector_store.similarity_search(query, k=initial_k)\n",
    "    \n",
    "    if not candidates:\n",
    "        return []\n",
    "    \n",
    "    # Check for Cohere availability\n",
    "    if not HAS_COHERE:\n",
    "        print(\"Cohere not installed. Returning unranked results.\")\n",
    "        return candidates[:k]\n",
    "    \n",
    "    api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "    if not api_key or api_key == \"your_cohere_api_key_here\":\n",
    "        print(\"COHERE_API_KEY not set. Returning unranked results.\")\n",
    "        return candidates[:k]\n",
    "    \n",
    "    try:\n",
    "        # Stage 2: Rerank with Cohere\n",
    "        import cohere\n",
    "        co = cohere.Client(api_key=api_key)\n",
    "        \n",
    "        rerank_response = co.rerank(\n",
    "            query=query,\n",
    "            documents=[doc.page_content for doc in candidates],\n",
    "            top_n=k,\n",
    "            model=\"rerank-english-v3.0\"\n",
    "        )\n",
    "        \n",
    "        # Return reranked documents\n",
    "        reranked = []\n",
    "        for result in rerank_response.results:\n",
    "            doc = candidates[result.index]\n",
    "            doc.metadata[\"rerank_score\"] = result.relevance_score\n",
    "            reranked.append(doc)\n",
    "        \n",
    "        return reranked\n",
    "    except Exception as e:\n",
    "        print(f\"Cohere reranking failed: {e}\")\n",
    "        return candidates[:k]\n",
    "\n",
    "\n",
    "print(\"search_with_reranking() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the attention mechanism in transformers?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "reranked = search_with_reranking(query, vector_store, k=5, initial_k=20)\n",
    "print(f\"Results ({len(reranked)} docs):\")\n",
    "for i, doc in enumerate(reranked):\n",
    "    source = Path(doc.metadata.get(\"source\", \"unknown\")).name\n",
    "    score = doc.metadata.get(\"rerank_score\", \"N/A\")\n",
    "    print(f\"  {i+1}. [{source}] (score: {score}) {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Expansion\n",
    "\n",
    "Users don't always use the same words as your documents. Query expansion generates multiple query variations to cover different phrasings of the same intent.\n",
    "\n",
    "- **Synonym-based**: \"laptop won't start\" -> \"notebook fails to boot\"\n",
    "- **Specificity variation**: general to specific and back\n",
    "- **LLM-generated**: use the model to rephrase the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(query: str, llm) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generate query variations using an LLM.\n",
    "    \n",
    "    Returns the original query plus variations.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Generate 3 alternative search queries for:\n",
    "\n",
    "Original: {query}\n",
    "\n",
    "Create variations using:\n",
    "1. Synonyms and related terms\n",
    "2. More specific phrasing\n",
    "3. More general phrasing\n",
    "\n",
    "Return as a JSON array of strings. Only the array, no explanation.\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    content = extract_json(response.content)\n",
    "\n",
    "    try:\n",
    "        variations = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        variations = []\n",
    "    \n",
    "    # Always include original query\n",
    "    return [query] + variations\n",
    "\n",
    "\n",
    "def search_with_expansion(\n",
    "    query: str,\n",
    "    vector_store,\n",
    "    llm,\n",
    "    k: int = 5\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Search with query expansion for better coverage.\n",
    "    \"\"\"\n",
    "    queries = expand_query(query, llm)\n",
    "    \n",
    "    all_results = []\n",
    "    seen_content = set()\n",
    "    \n",
    "    for q in queries:\n",
    "        results = vector_store.similarity_search(q, k=k)\n",
    "        for doc in results:\n",
    "            content_hash = hash(doc.page_content)\n",
    "            if content_hash not in seen_content:\n",
    "                seen_content.add(content_hash)\n",
    "                all_results.append(doc)\n",
    "    \n",
    "    # Return top k unique results\n",
    "    return all_results[:k]\n",
    "\n",
    "\n",
    "print(\"expand_query() and search_with_expansion() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I make my chatbot remember things?\"\n",
    "print(f\"Original query: {query}\\n\")\n",
    "\n",
    "expanded = expand_query(query, llm)\n",
    "print(\"Expanded queries:\")\n",
    "for i, q in enumerate(expanded):\n",
    "    print(f\"  {i+1}. {q}\")\n",
    "\n",
    "print(f\"\\nSearch with expansion:\")\n",
    "expanded_results = search_with_expansion(query, vector_store, llm, k=5)\n",
    "for i, doc in enumerate(expanded_results):\n",
    "    source = Path(doc.metadata.get(\"source\", \"unknown\")).name\n",
    "    print(f\"  {i+1}. [{source}] {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-Fusion\n",
    "\n",
    "RAG-Fusion takes query expansion further by systematically generating diverse queries and combining their results with Reciprocal Rank Fusion. Different query phrasings activate different regions of the embedding space. By searching with multiple phrasings and fusing results, you cover more ground.\n",
    "\n",
    "Best for: research-style queries, complex multi-faceted questions, situations where missing relevant content is worse than including some irrelevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_fusion(\n",
    "    query: str,\n",
    "    vector_store,\n",
    "    llm,\n",
    "    k: int = 5,\n",
    "    num_queries: int = 4\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    RAG-Fusion: multi-query retrieval with RRF combination.\n",
    "    \n",
    "    Generates diverse queries, searches with each, combines results.\n",
    "    \"\"\"\n",
    "    # Generate diverse queries\n",
    "    prompt = f\"\"\"Generate {num_queries} different search queries to find information for:\n",
    "\\\"{query}\\\"\n",
    "\n",
    "Make queries diverse:\n",
    "- Different angles on the topic\n",
    "- Varying levels of specificity  \n",
    "- Alternative phrasings and synonyms\n",
    "\n",
    "Return as a JSON array of strings.\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    content = extract_json(response.content)\n",
    "    \n",
    "    try:\n",
    "        queries = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        queries = [query]\n",
    "    \n",
    "    # Include original\n",
    "    queries = [query] + queries\n",
    "    \n",
    "    # Search with each query\n",
    "    all_ranked_results = []\n",
    "    for q in queries:\n",
    "        results = vector_store.similarity_search_with_score(q, k=k*2)\n",
    "        all_ranked_results.append(results)\n",
    "    \n",
    "    # Combine with RRF\n",
    "    combined = reciprocal_rank_fusion(all_ranked_results)\n",
    "    \n",
    "    return combined[:k]\n",
    "\n",
    "\n",
    "print(\"rag_fusion() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Compare embeddings and keyword search for different use cases\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "fusion_results = rag_fusion(query, vector_store, llm, k=5, num_queries=3)\n",
    "print(f\"RAG-Fusion results ({len(fusion_results)} docs):\")\n",
    "for i, doc in enumerate(fusion_results):\n",
    "    source = Path(doc.metadata.get(\"source\", \"unknown\")).name\n",
    "    print(f\"  {i+1}. [{source}] {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Chunking\n",
    "\n",
    "Fixed-size chunking ignores document structure. A chunk might split a code example in half or separate a concept from its explanation.\n",
    "\n",
    "**Semantic chunking** uses embeddings to identify natural topic boundaries -- it embeds each sentence, then looks for points where consecutive sentences have significantly different embeddings, indicating a topic shift.\n",
    "\n",
    "For structured documents like markdown, **structure-aware chunking** respects headers and sections explicitly.\n",
    "\n",
    "> Note: `SemanticChunker` requires `langchain-experimental`. Install with: `uv add langchain-experimental`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunk_documents(documents: list, threshold: float = 95) -> list:\n",
    "    \"\"\"\n",
    "    Split documents at semantic boundaries rather than fixed sizes.\n",
    "    \n",
    "    Args:\n",
    "        documents: Documents to chunk\n",
    "        threshold: Percentile threshold for detecting topic shifts (higher = fewer splits)\n",
    "    \"\"\"\n",
    "    if not HAS_SEMANTIC_CHUNKER:\n",
    "        print(\"langchain-experimental not installed. Using RecursiveCharacterTextSplitter fallback.\")\n",
    "        fallback = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        chunks = []\n",
    "        for doc in documents:\n",
    "            doc_chunks = fallback.split_documents([doc])\n",
    "            chunks.extend(doc_chunks)\n",
    "        return chunks\n",
    "    \n",
    "    from langchain_experimental.text_splitter import SemanticChunker\n",
    "    \n",
    "    chunker_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    \n",
    "    chunker = SemanticChunker(\n",
    "        embeddings=chunker_embeddings,\n",
    "        breakpoint_threshold_type=\"percentile\",\n",
    "        breakpoint_threshold_amount=threshold\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        doc_chunks = chunker.create_documents([doc.page_content])\n",
    "        \n",
    "        # Preserve original metadata\n",
    "        for chunk in doc_chunks:\n",
    "            chunk.metadata.update(doc.metadata)\n",
    "        \n",
    "        chunks.extend(doc_chunks)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "def chunk_markdown_by_structure(markdown_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Split markdown documents by header structure.\n",
    "    \"\"\"\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"h1\"),\n",
    "            (\"##\", \"h2\"),\n",
    "            (\"###\", \"h3\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(markdown_text)\n",
    "    # Each chunk includes header hierarchy in metadata\n",
    "    return chunks\n",
    "\n",
    "\n",
    "print(\"semantic_chunk_documents() and chunk_markdown_by_structure() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: markdown structure-aware chunking on a single document\n",
    "sample_doc = raw_documents[0]\n",
    "source_name = Path(sample_doc.metadata.get(\"source\", \"unknown\")).name\n",
    "print(f\"Structure-aware chunking of: {source_name}\")\n",
    "print(f\"  Original length: {len(sample_doc.page_content)} characters\\n\")\n",
    "\n",
    "structure_chunks = chunk_markdown_by_structure(sample_doc.page_content)\n",
    "print(f\"  Structure chunks: {len(structure_chunks)}\")\n",
    "for i, chunk in enumerate(structure_chunks[:5]):\n",
    "    headers = {k: v for k, v in chunk.metadata.items() if k.startswith(\"h\")}\n",
    "    print(f\"    {i+1}. {headers} -> {chunk.page_content[:60]}...\")\n",
    "\n",
    "# Demo: semantic chunking on a small sample (limit to 2 docs to save API calls)\n",
    "print(f\"\\nSemantic chunking (2 documents):\")\n",
    "semantic_chunks = semantic_chunk_documents(raw_documents[:2])\n",
    "print(f\"  Produced {len(semantic_chunks)} semantic chunks\")\n",
    "for i, chunk in enumerate(semantic_chunks[:3]):\n",
    "    print(f\"    {i+1}. ({len(chunk.page_content)} chars) {chunk.page_content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Compression\n",
    "\n",
    "Sometimes you retrieve the right document but it is too long. A 2000-token chunk might contain only 200 tokens of relevant content. Contextual compression extracts only the relevant portions, saving context window space and reducing noise.\n",
    "\n",
    "The trade-off: additional LLM calls per retrieved document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "\n",
    "def create_compression_retriever(base_retriever, llm):\n",
    "    \"\"\"\n",
    "    Wrap a retriever with contextual compression.\n",
    "    \n",
    "    Retrieved documents are filtered to only include relevant content.\n",
    "    \"\"\"\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    \n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "    \n",
    "    return compression_retriever\n",
    "\n",
    "\n",
    "print(\"create_compression_retriever() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a compression retriever wrapping basic vector search\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "compressed_retriever = create_compression_retriever(base_retriever, llm)\n",
    "\n",
    "query = \"What is the attention mechanism?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Uncompressed\n",
    "print(\"Uncompressed results (first 3):\")\n",
    "uncompressed = base_retriever.invoke(query)\n",
    "for i, doc in enumerate(uncompressed[:3]):\n",
    "    print(f\"  {i+1}. ({len(doc.page_content)} chars) {doc.page_content[:100]}...\")\n",
    "\n",
    "# Compressed\n",
    "print(f\"\\nCompressed results:\")\n",
    "compressed_docs = compressed_retriever.invoke(query)\n",
    "for i, doc in enumerate(compressed_docs[:3]):\n",
    "    print(f\"  {i+1}. ({len(doc.page_content)} chars) {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Filtering\n",
    "\n",
    "A lighter-weight alternative to full compression: score each chunk's cosine similarity to the query and drop low-scoring chunks before sending to the LLM. Faster than LLM compression but less precise -- it drops clearly irrelevant documents but cannot extract relevant portions from partially relevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_relevance(\n",
    "    query: str,\n",
    "    documents: list,\n",
    "    embeddings,\n",
    "    threshold: float = 0.7\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Filter documents below a relevance threshold.\n",
    "    \"\"\"\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    \n",
    "    filtered = []\n",
    "    for doc in documents:\n",
    "        doc_embedding = embeddings.embed_query(doc.page_content)\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = sum(a * b for a, b in zip(query_embedding, doc_embedding))\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            filtered.append(doc)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "print(\"filter_by_relevance() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do vector databases store embeddings?\"\n",
    "candidates = vector_store.similarity_search(query, k=10)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Candidates before filtering: {len(candidates)}\")\n",
    "\n",
    "# Use a moderate threshold -- text-embedding-3-small cosine similarities\n",
    "# tend to be in the 0.2-0.6 range for related content\n",
    "filtered = filter_by_relevance(query, candidates, embeddings, threshold=0.3)\n",
    "print(f\"After filtering (threshold=0.3): {len(filtered)}\")\n",
    "\n",
    "for i, doc in enumerate(filtered[:5]):\n",
    "    source = Path(doc.metadata.get(\"source\", \"unknown\")).name\n",
    "    print(f\"  {i+1}. [{source}] {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Retrieval\n",
    "\n",
    "For large corpora, flat retrieval can be inefficient. Hierarchical retrieval uses document summaries as a first-stage filter:\n",
    "\n",
    "1. **Stage 1:** Search document summaries to find relevant documents\n",
    "2. **Stage 2:** Search chunks within those relevant documents\n",
    "\n",
    "Like using a table of contents before reading individual chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "\n",
    "\n",
    "def hierarchical_search(\n",
    "    query: str,\n",
    "    summary_store,\n",
    "    chunk_store,\n",
    "    k: int = 5,\n",
    "    top_docs: int = 3\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Two-stage hierarchical retrieval.\n",
    "    \n",
    "    Stage 1: Search document summaries to find relevant documents\n",
    "    Stage 2: Search chunks within those documents\n",
    "    \"\"\"\n",
    "    # Stage 1: Find relevant documents via summaries\n",
    "    summaries = summary_store.similarity_search(query, k=top_docs)\n",
    "    \n",
    "    # Get document IDs from summaries\n",
    "    doc_ids = [s.metadata[\"document_id\"] for s in summaries]\n",
    "    \n",
    "    # Stage 2: Search chunks filtered to those documents\n",
    "    all_chunks = []\n",
    "    for doc_id in doc_ids:\n",
    "        qdrant_filter = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.document_id\",\n",
    "                    match=MatchValue(value=doc_id)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        chunks = chunk_store.similarity_search(\n",
    "            query,\n",
    "            k=k,\n",
    "            filter=qdrant_filter\n",
    "        )\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    # Re-rank combined chunks\n",
    "    all_chunks.sort(\n",
    "        key=lambda x: x.metadata.get(\"relevance_score\", 0),\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    return all_chunks[:k]\n",
    "\n",
    "\n",
    "print(\"hierarchical_search() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a summary store for demonstration\n",
    "SUMMARY_COLLECTION = \"chapter9_summaries\"\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=SUMMARY_COLLECTION,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "summary_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=SUMMARY_COLLECTION,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "print(\"Generating document summaries...\")\n",
    "summary_docs = []\n",
    "for doc in raw_documents[:5]:  # Limit to 5 for speed\n",
    "    source = Path(doc.metadata.get(\"source\", \"unknown\")).name\n",
    "    summary_response = llm.invoke(\n",
    "        f\"Summarize this document in 2-3 sentences:\\n\\n{doc.page_content[:3000]}\"\n",
    "    )\n",
    "    summary_doc = Document(\n",
    "        page_content=summary_response.content,\n",
    "        metadata={\"document_id\": source, \"source\": source}\n",
    "    )\n",
    "    summary_docs.append(summary_doc)\n",
    "    print(f\"  Summarized: {source}\")\n",
    "\n",
    "summary_store.add_documents(summary_docs)\n",
    "\n",
    "# Build a chunk store with document_id metadata\n",
    "CHUNK_COLLECTION = \"chapter9_hier_chunks\"\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=CHUNK_COLLECTION,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "chunk_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=CHUNK_COLLECTION,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "hier_chunks = []\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "for doc in raw_documents[:5]:\n",
    "    source = Path(doc.metadata.get(\"source\", \"unknown\")).name\n",
    "    chunks = splitter.split_documents([doc])\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"document_id\"] = source\n",
    "    hier_chunks.extend(chunks)\n",
    "\n",
    "chunk_store.add_documents(hier_chunks)\n",
    "print(f\"\\nIndexed {len(hier_chunks)} chunks with document_id metadata\")\n",
    "\n",
    "# Test hierarchical search\n",
    "query = \"What are the best practices for chunking documents?\"\n",
    "print(f\"\\nHierarchical search: {query}\")\n",
    "results = hierarchical_search(query, summary_store, chunk_store, k=5, top_docs=2)\n",
    "print(f\"Results: {len(results)} chunks\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"  {i+1}. [{doc.metadata.get('document_id', '?')}] {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic Comparison of Retrievers\n",
    "\n",
    "With multiple techniques available, the answer to \"which should I use?\" is: **measurement**. Build a comparison harness, run your evaluation dataset through different configurations, and let the numbers guide your decisions.\n",
    "\n",
    "The key to good comparisons: control variables. Same documents, same chunking, same embedding model, same k. Only the retrieval strategy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RetrieverConfig:\n",
    "    name: str\n",
    "    retrieve_fn: Callable\n",
    "    description: str\n",
    "\n",
    "\n",
    "def compare_retrievers(\n",
    "    configs: list[RetrieverConfig],\n",
    "    test_queries: list[dict],\n",
    "    evaluate_fn: Callable\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Systematically compare multiple retrieval configurations.\n",
    "    \n",
    "    Args:\n",
    "        configs: List of retriever configurations to test\n",
    "        test_queries: List of {query, ground_truth} dicts\n",
    "        evaluate_fn: Function that scores retrieval results\n",
    "    \n",
    "    Returns:\n",
    "        Comparison results with metrics for each config\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for config in configs:\n",
    "        config_results = {\n",
    "            \"scores\": [],\n",
    "            \"latencies\": [],\n",
    "            \"name\": config.name,\n",
    "            \"description\": config.description\n",
    "        }\n",
    "        \n",
    "        for test_case in test_queries:\n",
    "            query = test_case[\"query\"]\n",
    "            ground_truth = test_case[\"ground_truth\"]\n",
    "            \n",
    "            # Measure retrieval\n",
    "            start = time.time()\n",
    "            retrieved = config.retrieve_fn(query)\n",
    "            latency = time.time() - start\n",
    "            \n",
    "            # Score results\n",
    "            score = evaluate_fn(retrieved, ground_truth, query)\n",
    "            \n",
    "            config_results[\"scores\"].append(score)\n",
    "            config_results[\"latencies\"].append(latency)\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        config_results[\"mean_score\"] = sum(config_results[\"scores\"]) / len(config_results[\"scores\"])\n",
    "        config_results[\"mean_latency\"] = sum(config_results[\"latencies\"]) / len(config_results[\"latencies\"])\n",
    "        config_results[\"p95_latency\"] = sorted(config_results[\"latencies\"])[int(len(config_results[\"latencies\"]) * 0.95)]\n",
    "        \n",
    "        results[config.name] = config_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def segment_analysis(\n",
    "    results: dict,\n",
    "    test_queries: list[dict],\n",
    "    segment_fn: Callable\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Analyze retriever performance across query segments.\n",
    "    \n",
    "    Args:\n",
    "        results: Raw results from compare_retrievers\n",
    "        test_queries: Original test queries with metadata\n",
    "        segment_fn: Function that returns segment name for a query\n",
    "    \"\"\"\n",
    "    segmented = {}\n",
    "    \n",
    "    for config_name, config_results in results.items():\n",
    "        segmented[config_name] = {}\n",
    "        \n",
    "        for i, query in enumerate(test_queries):\n",
    "            segment = segment_fn(query)\n",
    "            \n",
    "            if segment not in segmented[config_name]:\n",
    "                segmented[config_name][segment] = {\"scores\": [], \"latencies\": []}\n",
    "            \n",
    "            segmented[config_name][segment][\"scores\"].append(\n",
    "                config_results[\"scores\"][i]\n",
    "            )\n",
    "            segmented[config_name][segment][\"latencies\"].append(\n",
    "                config_results[\"latencies\"][i]\n",
    "            )\n",
    "        \n",
    "        # Calculate segment averages\n",
    "        for segment in segmented[config_name]:\n",
    "            scores = segmented[config_name][segment][\"scores\"]\n",
    "            latencies = segmented[config_name][segment][\"latencies\"]\n",
    "            segmented[config_name][segment][\"mean_score\"] = sum(scores) / len(scores)\n",
    "            segmented[config_name][segment][\"mean_latency\"] = sum(latencies) / len(latencies)\n",
    "    \n",
    "    return segmented\n",
    "\n",
    "\n",
    "def query_complexity_segment(query: dict) -> str:\n",
    "    \"\"\"Segment queries by complexity.\"\"\"\n",
    "    words = query[\"query\"].split()\n",
    "    if len(words) <= 5:\n",
    "        return \"simple\"\n",
    "    elif len(words) <= 15:\n",
    "        return \"moderate\"\n",
    "    else:\n",
    "        return \"complex\"\n",
    "\n",
    "\n",
    "print(\"Comparison framework defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Retriever Comparison\n",
    "\n",
    "Let's compare basic vector search against advanced techniques using test queries with ground truth context. We use a simple relevance scoring function first, then upgrade to RAGAS metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries with ground truth (key phrases that should appear in good retrievals)\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"What is RAG?\",\n",
    "        \"ground_truth\": \"retrieval-augmented generation combines retrieval with generation\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do embeddings capture semantic meaning?\",\n",
    "        \"ground_truth\": \"embeddings map text to vectors where similar meanings are close together\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the different chunking strategies?\",\n",
    "        \"ground_truth\": \"fixed-size chunking recursive character splitting semantic chunking\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Compare vector search with keyword search\",\n",
    "        \"ground_truth\": \"dense vector search finds semantic similarity BM25 finds exact term matches\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"error handling in agents\",\n",
    "        \"ground_truth\": \"agents need error handling retry logic fallback strategies\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How does the ReAct pattern work?\",\n",
    "        \"ground_truth\": \"ReAct alternates between reasoning and acting observation loop\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is prompt engineering and why does it matter?\",\n",
    "        \"ground_truth\": \"prompt engineering designs inputs to get better outputs from language models\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain the supervisor pattern in multi-agent systems\",\n",
    "        \"ground_truth\": \"supervisor agent coordinates delegates tasks to specialized worker agents\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def simple_relevance_score(retrieved: list, ground_truth: str, query: str) -> float:\n",
    "    \"\"\"Score retrieval quality by checking ground truth keyword overlap.\"\"\"\n",
    "    if not retrieved:\n",
    "        return 0.0\n",
    "    \n",
    "    retrieved_text = \" \".join([doc.page_content.lower() for doc in retrieved])\n",
    "    truth_terms = ground_truth.lower().split()\n",
    "    \n",
    "    matches = sum(1 for term in truth_terms if term in retrieved_text)\n",
    "    return matches / len(truth_terms) if truth_terms else 0.0\n",
    "\n",
    "\n",
    "print(f\"Defined {len(test_queries)} test queries with ground truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define retriever configurations\n",
    "configs = [\n",
    "    RetrieverConfig(\n",
    "        name=\"basic_vector\",\n",
    "        retrieve_fn=lambda q: vector_store.similarity_search(q, k=5),\n",
    "        description=\"Basic dense vector search\"\n",
    "    ),\n",
    "    RetrieverConfig(\n",
    "        name=\"rag_fusion\",\n",
    "        retrieve_fn=lambda q: rag_fusion(q, vector_store, llm, k=5, num_queries=3),\n",
    "        description=\"RAG-Fusion with multi-query + RRF\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Add hybrid search if rank_bm25 is available\n",
    "if HAS_RANK_BM25:\n",
    "    configs.append(RetrieverConfig(\n",
    "        name=\"hybrid\",\n",
    "        retrieve_fn=lambda q: hybrid_search(q, vector_store, all_chunks, k=5),\n",
    "        description=\"Hybrid dense + BM25 with RRF\"\n",
    "    ))\n",
    "\n",
    "print(\"Comparing retrievers...\\n\")\n",
    "comparison_results = compare_retrievers(configs, test_queries, simple_relevance_score)\n",
    "\n",
    "# Display results\n",
    "print(f\"{'Retriever':<20} {'Mean Score':>12} {'Mean Latency':>14} {'P95 Latency':>13}\")\n",
    "print(\"-\" * 62)\n",
    "for name, data in comparison_results.items():\n",
    "    print(f\"{name:<20} {data['mean_score']:>12.3f} {data['mean_latency']*1000:>12.1f}ms {data['p95_latency']*1000:>11.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by query complexity\n",
    "segmented = segment_analysis(comparison_results, test_queries, query_complexity_segment)\n",
    "\n",
    "print(\"Performance by query complexity:\\n\")\n",
    "for config_name, segments in segmented.items():\n",
    "    print(f\"  {config_name}:\")\n",
    "    for segment, data in segments.items():\n",
    "        print(f\"    {segment:>10}: score={data['mean_score']:.3f}, latency={data['mean_latency']*1000:.1f}ms\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS Evaluation of Retrieval Quality\n",
    "\n",
    "For a more rigorous evaluation, we use RAGAS metrics to measure context precision and context recall. This tells us not just whether keywords match, but whether the retrieved context actually supports answering the question correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics._context_precision import LLMContextPrecisionWithoutReference\n",
    "from ragas.metrics._context_recall import LLMContextRecall\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from datasets import Dataset\n",
    "\n",
    "# Configure RAGAS using LangChain wrappers\n",
    "ragas_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "metrics = [\n",
    "    LLMContextPrecisionWithoutReference(llm=ragas_llm),\n",
    "    LLMContextRecall(llm=ragas_llm),\n",
    "]\n",
    "\n",
    "\n",
    "def build_ragas_dataset(retriever_fn, test_queries):\n",
    "    \"\"\"Run retriever on test queries, generate answers, and build RAGAS-compatible dataset.\"\"\"\n",
    "    questions = []\n",
    "    responses = []\n",
    "    contexts_list = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    for tq in test_queries:\n",
    "        retrieved = retriever_fn(tq[\"query\"])\n",
    "        context_texts = [doc.page_content for doc in retrieved]\n",
    "        \n",
    "        # Generate a response using the retrieved context\n",
    "        context_str = \"\\n\\n\".join(context_texts[:3])\n",
    "        answer = llm.invoke(\n",
    "            f\"Based on the following context, answer the question briefly.\\n\\n\"\n",
    "            f\"Context:\\n{context_str}\\n\\nQuestion: {tq['query']}\"\n",
    "        )\n",
    "        \n",
    "        questions.append(tq[\"query\"])\n",
    "        responses.append(answer.content)\n",
    "        contexts_list.append(context_texts)\n",
    "        ground_truths.append(tq[\"ground_truth\"])\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        \"question\": questions,\n",
    "        \"answer\": responses,\n",
    "        \"contexts\": contexts_list,\n",
    "        \"ground_truth\": ground_truths,\n",
    "    })\n",
    "\n",
    "\n",
    "# Evaluate basic vector search (limit to 4 queries to save API calls)\n",
    "ragas_test_queries = test_queries[:4]\n",
    "\n",
    "print(\"Evaluating basic vector search with RAGAS...\")\n",
    "basic_dataset = build_ragas_dataset(\n",
    "    lambda q: vector_store.similarity_search(q, k=5),\n",
    "    ragas_test_queries\n",
    ")\n",
    "basic_results = evaluate(basic_dataset, metrics=metrics)\n",
    "basic_df = basic_results.to_pandas()\n",
    "\n",
    "# Evaluate RAG-Fusion\n",
    "print(\"Evaluating RAG-Fusion with RAGAS...\")\n",
    "fusion_dataset = build_ragas_dataset(\n",
    "    lambda q: rag_fusion(q, vector_store, llm, k=5, num_queries=3),\n",
    "    ragas_test_queries\n",
    ")\n",
    "fusion_results = evaluate(fusion_dataset, metrics=metrics)\n",
    "fusion_df = fusion_results.to_pandas()\n",
    "\n",
    "# Compare\n",
    "metric_cols = basic_df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "print(f\"\\n{'Metric':<40} {'Basic Vector':>14} {'RAG-Fusion':>14}\")\n",
    "print(\"-\" * 70)\n",
    "for col in metric_cols:\n",
    "    basic_mean = basic_df[col].mean()\n",
    "    fusion_mean = fusion_df[col].mean()\n",
    "    delta = fusion_mean - basic_mean\n",
    "    arrow = \"^\" if delta > 0.01 else \"v\" if delta < -0.01 else \"=\"\n",
    "    print(f\"{col:<40} {basic_mean:>14.3f} {fusion_mean:>13.3f} {arrow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We explored advanced retrieval techniques and a systematic evaluation framework:\n",
    "\n",
    "| Technique | When to Use | Typical Latency |\n",
    "|-----------|------------|----------------|\n",
    "| **Hybrid search** (BM25 + dense) | Queries with specific terms, codes, names | 40-80ms |\n",
    "| **Reranking** (Cohere cross-encoder) | Precision is critical | 100-250ms |\n",
    "| **Query expansion** (LLM-generated) | Vocabulary mismatch is common | 100-300ms |\n",
    "| **RAG-Fusion** (multi-query + RRF) | Complex, multi-faceted queries | 200-500ms |\n",
    "| **Semantic chunking** | Structured documents, code examples | Indexing-time |\n",
    "| **Contextual compression** | Long chunks, precious context window | 200-500ms per doc |\n",
    "| **Relevance filtering** | Quick noise removal | 50-100ms |\n",
    "| **Hierarchical retrieval** | Very large corpora | Varies |\n",
    "\n",
    "**Decision framework:**\n",
    "1. Start simple -- basic vector search with good chunking.\n",
    "2. Add hybrid search if you have specific identifiers users search for.\n",
    "3. Add reranking if precision is critical and 200ms extra latency is acceptable.\n",
    "4. Add query expansion if vocabulary mismatch is causing retrieval failures.\n",
    "5. Add RAG-Fusion if comprehensive coverage matters more than speed.\n",
    "\n",
    "The key: **measure before and after each change**. Do not assume more sophisticated equals better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CHAPTER 9: ADVANCED RETRIEVAL & EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDocument corpus: {len(all_chunks)} chunks from {len(guide_files)} files\")\n",
    "print(f\"Vector store: Qdrant in-memory ({COLLECTION_NAME})\")\n",
    "\n",
    "print(f\"\\nTechniques implemented:\")\n",
    "techniques = [\n",
    "    \"Reciprocal Rank Fusion (RRF)\",\n",
    "    \"Hybrid Search (BM25 + Dense)\" + (\" [active]\" if HAS_RANK_BM25 else \" [needs rank-bm25]\"),\n",
    "    \"Reranking (Cohere)\" + (\" [active]\" if HAS_COHERE and os.getenv(\"COHERE_API_KEY\") else \" [needs cohere + API key]\"),\n",
    "    \"Query Expansion (LLM-based)\",\n",
    "    \"RAG-Fusion (multi-query + RRF)\",\n",
    "    \"Semantic Chunking\" + (\" [active]\" if HAS_SEMANTIC_CHUNKER else \" [needs langchain-experimental]\"),\n",
    "    \"Structure-Aware Chunking (Markdown headers)\",\n",
    "    \"Contextual Compression (LLMChainExtractor)\",\n",
    "    \"Relevance Filtering (cosine threshold)\",\n",
    "    \"Hierarchical Retrieval (summary -> chunk)\",\n",
    "    \"Systematic Retriever Comparison Framework\",\n",
    "    \"RAGAS Evaluation (context precision + recall)\",\n",
    "]\n",
    "for t in techniques:\n",
    "    print(f\"  - {t}\")\n",
    "\n",
    "print(f\"\\nComparison results:\")\n",
    "for name, data in comparison_results.items():\n",
    "    print(f\"  {name}: score={data['mean_score']:.3f}, latency={data['mean_latency']*1000:.1f}ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
