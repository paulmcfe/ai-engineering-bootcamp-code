{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Infrastructure\n",
    "\n",
    "## LLM Servers, Open-Source Models, and Agent-to-Agent Communication\n",
    "\n",
    "This notebook explores the infrastructure layer of AI engineering — self-hosted LLM servers, open-source model deployment, and patterns for agents to communicate with each other. We cover the full stack from local development with Ollama to production deployment with Together AI, plus exposing your agents as MCP servers for other agents to consume.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                  Advanced Infrastructure                    │\n",
    "│                                                             │\n",
    "│   ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │\n",
    "│   │  Ollama  │  │  vLLM /  │  │ Together │  │   MCP    │    │\n",
    "│   │  Local   │  │   TGI    │  │    AI    │  │  Server  │    │\n",
    "│   │   Dev    │  │Production│  │  Hosting │  │ Exposure │    │\n",
    "│   └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘    │\n",
    "│        │             │             │             │          │\n",
    "│        v             v             v             v          │\n",
    "│   Fast local     High-throughput  Managed       Agent-to-   │\n",
    "│   iteration      inference        open models   agent APIs  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Topics covered:\n",
    "- LLM server fundamentals (model serving, inference optimization, cost analysis)\n",
    "- Popular LLM servers (vLLM, TGI, Ollama)\n",
    "- Open-source LLMs and embedding models (Llama 4, Mixtral, Qwen3, DeepSeek)\n",
    "- Deploying open models with Together AI\n",
    "- Agent-to-agent communication patterns (request-response, pub-sub, event sourcing)\n",
    "- Exposing agents via MCP (Model Context Protocol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Make sure you have the required packages installed:\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "This notebook uses optional dependencies:\n",
    "- **Ollama** — For local model inference (install from ollama.com)\n",
    "- **Together AI** — For hosted open-source models (set `TOGETHER_API_KEY`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import urllib.request\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Ollama integration (optional - for local development)\n",
    "try:\n",
    "    from langchain_ollama import ChatOllama\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(\"langchain_ollama not installed. Ollama features will be skipped.\")\n",
    "\n",
    "# Together AI integration (optional - for open-source model hosting)\n",
    "try:\n",
    "    from together import Together\n",
    "    TOGETHER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TOGETHER_AVAILABLE = False\n",
    "    print(\"together SDK not installed. Together AI features will be skipped.\")\n",
    "\n",
    "# MCP imports\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check for API keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "\n",
    "def check_ollama_running() -> bool:\n",
    "    \"\"\"Check if Ollama server is running locally.\"\"\"\n",
    "    try:\n",
    "        urllib.request.urlopen(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "OLLAMA_RUNNING = check_ollama_running() if OLLAMA_AVAILABLE else False\n",
    "\n",
    "# Initialize default LLM (OpenAI as fallback)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"  OpenAI: Ready\")\n",
    "print(f\"  Ollama: {'Running' if OLLAMA_RUNNING else 'Not available'}\")\n",
    "print(f\"  Together AI: {'Ready' if TOGETHER_API_KEY else 'Missing API key'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Servers - Model Serving Fundamentals\n",
    "\n",
    "An LLM server is the infrastructure layer that hosts and serves language model inference. Understanding the fundamentals helps you make informed decisions about self-hosting vs. using managed APIs.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Inference Optimization Techniques:**\n",
    "- **Quantization** — Reduce model precision (FP32 → FP16 → INT8 → INT4) to decrease memory usage and increase throughput\n",
    "- **Continuous Batching** — Process multiple requests together, dynamically adding/removing requests as they complete\n",
    "- **KV Cache** — Store computed key-value pairs from attention layers to avoid redundant computation\n",
    "- **PagedAttention** — Manage KV cache memory like virtual memory pages for efficient utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(BaseModel):\n",
    "    \"\"\"Configuration for LLM inference optimization.\"\"\"\n",
    "    max_batch_size: int = 32\n",
    "    max_sequence_length: int = 4096\n",
    "    kv_cache_dtype: str = \"auto\"  # \"auto\", \"fp16\", \"fp8\"\n",
    "    quantization: Optional[str] = None  # \"awq\", \"gptq\", \"fp8\"\n",
    "    tensor_parallel_size: int = 1\n",
    "    gpu_memory_utilization: float = 0.9\n",
    "\n",
    "\n",
    "# Example configurations for different scenarios\n",
    "CONFIGS = {\n",
    "    \"development\": InferenceConfig(\n",
    "        max_batch_size=4,\n",
    "        max_sequence_length=2048,\n",
    "        quantization=\"awq\",\n",
    "        tensor_parallel_size=1,\n",
    "    ),\n",
    "    \"production_single_gpu\": InferenceConfig(\n",
    "        max_batch_size=32,\n",
    "        max_sequence_length=8192,\n",
    "        quantization=\"fp8\",\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.95,\n",
    "    ),\n",
    "    \"production_multi_gpu\": InferenceConfig(\n",
    "        max_batch_size=64,\n",
    "        max_sequence_length=32768,\n",
    "        quantization=None,  # Full precision for quality\n",
    "        tensor_parallel_size=4,\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"Inference Configurations:\")\n",
    "for name, config in CONFIGS.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Batch size: {config.max_batch_size}\")\n",
    "    print(f\"  Max sequence: {config.max_sequence_length}\")\n",
    "    print(f\"  Quantization: {config.quantization or 'None (full precision)'}\")\n",
    "    print(f\"  Tensor parallel: {config.tensor_parallel_size} GPU(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Analysis: Self-Hosting vs. APIs\n",
    "\n",
    "The decision to self-host depends on your volume, latency requirements, and operational capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostComparison(BaseModel):\n",
    "    \"\"\"Compare self-hosting vs API costs.\"\"\"\n",
    "    provider: str\n",
    "    model: str\n",
    "    input_cost_per_1m: float  # Cost per 1M input tokens\n",
    "    output_cost_per_1m: float  # Cost per 1M output tokens\n",
    "    monthly_fixed_cost: float = 0.0  # GPU rental, etc.\n",
    "    \n",
    "    def monthly_cost(self, input_tokens: int, output_tokens: int) -> float:\n",
    "        \"\"\"Calculate monthly cost for given token volumes.\"\"\"\n",
    "        input_cost = (input_tokens / 1_000_000) * self.input_cost_per_1m\n",
    "        output_cost = (output_tokens / 1_000_000) * self.output_cost_per_1m\n",
    "        return input_cost + output_cost + self.monthly_fixed_cost\n",
    "\n",
    "\n",
    "# Cost comparison data (February 2026 pricing)\n",
    "COST_OPTIONS = [\n",
    "    CostComparison(\n",
    "        provider=\"OpenAI\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input_cost_per_1m=0.15,\n",
    "        output_cost_per_1m=0.60,\n",
    "    ),\n",
    "    CostComparison(\n",
    "        provider=\"Together AI\",\n",
    "        model=\"Llama-4-Scout-17B\",\n",
    "        input_cost_per_1m=0.10,\n",
    "        output_cost_per_1m=0.20,\n",
    "    ),\n",
    "    CostComparison(\n",
    "        provider=\"Self-hosted (A100 80GB)\",\n",
    "        model=\"Llama-4-Scout-17B\",\n",
    "        input_cost_per_1m=0.0,\n",
    "        output_cost_per_1m=0.0,\n",
    "        monthly_fixed_cost=2000.0,  # ~$2.78/hr * 720 hrs\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def analyze_costs(monthly_input_tokens: int, monthly_output_tokens: int):\n",
    "    \"\"\"Analyze costs across providers for given token volumes.\"\"\"\n",
    "    print(f\"Cost Analysis for {monthly_input_tokens:,} input + {monthly_output_tokens:,} output tokens/month\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    for option in COST_OPTIONS:\n",
    "        cost = option.monthly_cost(monthly_input_tokens, monthly_output_tokens)\n",
    "        results.append((option.provider, option.model, cost))\n",
    "        print(f\"{option.provider} ({option.model}): ${cost:,.2f}/month\")\n",
    "    \n",
    "    cheapest = min(results, key=lambda x: x[2])\n",
    "    print(f\"\\nBest option: {cheapest[0]} at ${cheapest[2]:,.2f}/month\")\n",
    "\n",
    "\n",
    "# Compare at different scales\n",
    "print(\"=== Low Volume (startup/development) ===\")\n",
    "analyze_costs(1_000_000, 500_000)  # 1M input, 500K output\n",
    "\n",
    "print(\"\\n=== Medium Volume (production app) ===\")\n",
    "analyze_costs(50_000_000, 25_000_000)  # 50M input, 25M output\n",
    "\n",
    "print(\"\\n=== High Volume (enterprise scale) ===\")\n",
    "analyze_costs(500_000_000, 250_000_000)  # 500M input, 250M output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular LLM Servers\n",
    "\n",
    "Three main options dominate the LLM serving landscape:\n",
    "\n",
    "| Server | Best For | Key Features |\n",
    "|--------|----------|-------------|\n",
    "| **vLLM** | High-throughput production | PagedAttention, continuous batching, OpenAI-compatible API |\n",
    "| **TGI** | Hugging Face ecosystem | Multi-backend support, quantization, Rust performance |\n",
    "| **Ollama** | Local development | Simple CLI, easy model management, cross-platform |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vLLM for High-Throughput Serving\n",
    "\n",
    "vLLM (v0.16.0 as of February 2026) is the industry standard for production LLM serving. It uses PagedAttention to achieve 2-4x higher throughput than naive implementations.\n",
    "\n",
    "**Starting a vLLM server:**\n",
    "```bash\n",
    "# Install vLLM\n",
    "pip install vllm\n",
    "\n",
    "# Start server with OpenAI-compatible API\n",
    "vllm serve meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n",
    "    --tensor-parallel-size 2 \\\n",
    "    --max-model-len 8192 \\\n",
    "    --quantization awq \\\n",
    "    --api-key token-abc123\n",
    "```\n",
    "\n",
    "**Key vLLM 0.16 features:**\n",
    "- RTX Blackwell (SM120) support\n",
    "- PyTorch 2.10 compatibility\n",
    "- EAGLE3 speculative decoding\n",
    "- FP8 MoE kernel support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM configuration example (reference only - requires GPU)\n",
    "VLLM_CONFIG = {\n",
    "    \"model\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    \"tensor_parallel_size\": 2,\n",
    "    \"max_model_len\": 8192,\n",
    "    \"quantization\": \"awq\",\n",
    "    \"gpu_memory_utilization\": 0.9,\n",
    "    \"enable_chunked_prefill\": True,\n",
    "    \"max_num_seqs\": 256,\n",
    "}\n",
    "\n",
    "print(\"vLLM Configuration (for production deployment):\")\n",
    "print(json.dumps(VLLM_CONFIG, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"To start vLLM server:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"vllm serve {VLLM_CONFIG['model']} \\\\\")\n",
    "print(f\"    --tensor-parallel-size {VLLM_CONFIG['tensor_parallel_size']} \\\\\")\n",
    "print(f\"    --max-model-len {VLLM_CONFIG['max_model_len']} \\\\\")\n",
    "print(f\"    --quantization {VLLM_CONFIG['quantization']} \\\\\")\n",
    "print(f\"    --api-key your-api-key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation Inference (TGI)\n",
    "\n",
    "TGI (v3.x) from Hugging Face is a production-grade inference server built in Rust and Python.\n",
    "\n",
    "**Starting TGI with Docker:**\n",
    "```bash\n",
    "docker run --gpus all --shm-size 1g -p 8080:80 \\\n",
    "    -v /data:/data \\\n",
    "    ghcr.io/huggingface/text-generation-inference:3.0 \\\n",
    "    --model-id meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n",
    "    --quantize awq\n",
    "```\n",
    "\n",
    "**Key TGI v3 features:**\n",
    "- Multi-backend support (vLLM, TensorRT-LLM)\n",
    "- Zero-config mode for easy deployment\n",
    "- Torch 2.7 and CUDA 12.8 support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TGI Docker command reference\n",
    "TGI_DOCKER_CMD = \"\"\"\n",
    "docker run --gpus all --shm-size 1g -p 8080:80 \\\\\n",
    "    -v /data:/data \\\\\n",
    "    -e HF_TOKEN=$HF_TOKEN \\\\\n",
    "    ghcr.io/huggingface/text-generation-inference:3.0 \\\\\n",
    "    --model-id meta-llama/Llama-4-Scout-17B-16E-Instruct \\\\\n",
    "    --quantize awq \\\\\n",
    "    --max-concurrent-requests 128 \\\\\n",
    "    --max-input-tokens 4096 \\\\\n",
    "    --max-total-tokens 8192\n",
    "\"\"\"\n",
    "\n",
    "print(\"TGI Docker Deployment:\")\n",
    "print(TGI_DOCKER_CMD)\n",
    "\n",
    "print(\"\\nTGI vs vLLM Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"vLLM: Better raw throughput, more quantization options\")\n",
    "print(\"TGI: Easier Hugging Face integration, multi-backend support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama for Local Development\n",
    "\n",
    "Ollama (v0.16.1) makes local LLM development simple. Install from [ollama.com](https://ollama.com) and run models with a single command.\n",
    "\n",
    "**Getting started:**\n",
    "```bash\n",
    "# Start Ollama server\n",
    "ollama serve\n",
    "\n",
    "# Pull a model\n",
    "ollama pull llama3.2\n",
    "\n",
    "# Run interactively\n",
    "ollama run llama3.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ollama_local():\n",
    "    \"\"\"Test Ollama with a local model.\"\"\"\n",
    "    if not OLLAMA_RUNNING:\n",
    "        print(\"Ollama not running.\")\n",
    "        print(\"To use Ollama:\")\n",
    "        print(\"  1. Install from https://ollama.com\")\n",
    "        print(\"  2. Run: ollama serve\")\n",
    "        print(\"  3. Pull a model: ollama pull llama3.2\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Testing Ollama with llama3.2...\")\n",
    "    ollama_llm = ChatOllama(\n",
    "        model=\"llama3.2\",\n",
    "        temperature=0,\n",
    "        base_url=\"http://localhost:11434\"\n",
    "    )\n",
    "    \n",
    "    response = ollama_llm.invoke(\"What is the capital of France? Answer in one word.\")\n",
    "    print(f\"Ollama response: {response.content}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test Ollama if available\n",
    "test_ollama_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI-Compatible APIs\n",
    "\n",
    "All three servers (vLLM, TGI, Ollama) expose OpenAI-compatible APIs. This means you can use the same client code to connect to any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_openai_compatible_client(base_url: str, api_key: str = \"not-needed\") -> OpenAI:\n",
    "    \"\"\"Create a client for any OpenAI-compatible server.\"\"\"\n",
    "    return OpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "\n",
    "def test_openai_compatible(base_url: str, model: str, api_key: str = \"not-needed\"):\n",
    "    \"\"\"Test an OpenAI-compatible endpoint.\"\"\"\n",
    "    client = create_openai_compatible_client(base_url, api_key)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say hello in exactly 3 words.\"}],\n",
    "        max_tokens=20,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Example: Connecting to Ollama via OpenAI SDK\n",
    "if OLLAMA_RUNNING:\n",
    "    print(\"Testing Ollama via OpenAI-compatible API...\")\n",
    "    result = test_openai_compatible(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        model=\"llama3.2\"\n",
    "    )\n",
    "    print(f\"Response: {result}\")\n",
    "else:\n",
    "    print(\"Ollama not running. Example configuration:\")\n",
    "    print(\"  base_url = 'http://localhost:11434/v1'\")\n",
    "    print(\"  model = 'llama3.2'\")\n",
    "\n",
    "print(\"\\nOther OpenAI-compatible endpoints:\")\n",
    "print(\"  vLLM: http://localhost:8000/v1\")\n",
    "print(\"  TGI:  http://localhost:8080/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Open-Source LLMs\n",
    "\n",
    "The open-source LLM landscape has matured significantly. As of February 2026, several models compete with proprietary options on quality while offering significant cost savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInfo(BaseModel):\n",
    "    \"\"\"Information about an open-source model.\"\"\"\n",
    "    name: str\n",
    "    provider: str\n",
    "    parameters: str\n",
    "    context_length: int\n",
    "    strengths: list[str]\n",
    "    best_for: list[str]\n",
    "    license: str\n",
    "\n",
    "\n",
    "# Latest open-source models (February 2026)\n",
    "OPEN_SOURCE_MODELS = [\n",
    "    ModelInfo(\n",
    "        name=\"Llama-4-Scout-17B-16E-Instruct\",\n",
    "        provider=\"Meta\",\n",
    "        parameters=\"17B active / 109B total (MoE)\",\n",
    "        context_length=10_000_000,  # 10M tokens!\n",
    "        strengths=[\"Massive context window\", \"Efficient MoE\", \"Multimodal\"],\n",
    "        best_for=[\"Long documents\", \"Codebase analysis\", \"Research\"],\n",
    "        license=\"Llama 4 Community\"\n",
    "    ),\n",
    "    ModelInfo(\n",
    "        name=\"Llama-4-Maverick-17B-128E-Instruct\",\n",
    "        provider=\"Meta\",\n",
    "        parameters=\"17B active / 400B total (MoE)\",\n",
    "        context_length=128_000,\n",
    "        strengths=[\"Highest quality\", \"Complex reasoning\", \"Multimodal\"],\n",
    "        best_for=[\"Research\", \"Complex analysis\", \"Creative tasks\"],\n",
    "        license=\"Llama 4 Community\"\n",
    "    ),\n",
    "    ModelInfo(\n",
    "        name=\"Mixtral-8x22B-Instruct-v0.1\",\n",
    "        provider=\"Mistral AI\",\n",
    "        parameters=\"39B active / 141B total (MoE)\",\n",
    "        context_length=64_000,\n",
    "        strengths=[\"Multilingual\", \"Cost-effective\", \"Apache 2.0 license\"],\n",
    "        best_for=[\"European languages\", \"Translation\", \"General assistant\"],\n",
    "        license=\"Apache 2.0\"\n",
    "    ),\n",
    "    ModelInfo(\n",
    "        name=\"Qwen3-72B-Instruct\",\n",
    "        provider=\"Alibaba\",\n",
    "        parameters=\"72B dense\",\n",
    "        context_length=128_000,\n",
    "        strengths=[\"Asian languages\", \"Math\", \"Coding\"],\n",
    "        best_for=[\"Multilingual apps\", \"Technical tasks\", \"STEM\"],\n",
    "        license=\"Qwen License\"\n",
    "    ),\n",
    "    ModelInfo(\n",
    "        name=\"DeepSeek-V3.2\",\n",
    "        provider=\"DeepSeek\",\n",
    "        parameters=\"671B total (MoE)\",\n",
    "        context_length=128_000,\n",
    "        strengths=[\"Coding\", \"Math\", \"Reasoning\"],\n",
    "        best_for=[\"Developer tools\", \"Technical Q&A\", \"Code generation\"],\n",
    "        license=\"DeepSeek License\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Open-Source Model Comparison (February 2026)\")\n",
    "print(\"=\" * 70)\n",
    "for model in OPEN_SOURCE_MODELS:\n",
    "    print(f\"\\n{model.name} ({model.provider})\")\n",
    "    print(f\"  Parameters: {model.parameters}\")\n",
    "    print(f\"  Context: {model.context_length:,} tokens\")\n",
    "    print(f\"  License: {model.license}\")\n",
    "    print(f\"  Strengths: {', '.join(model.strengths)}\")\n",
    "    print(f\"  Best for: {', '.join(model.best_for)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model_for_task(\n",
    "    task_type: str,\n",
    "    context_needed: int = 4096,\n",
    "    needs_multilingual: bool = False,\n",
    "    prefer_open_license: bool = False\n",
    ") -> ModelInfo:\n",
    "    \"\"\"Recommend a model based on task requirements.\"\"\"\n",
    "    \n",
    "    candidates = OPEN_SOURCE_MODELS.copy()\n",
    "    \n",
    "    # Filter by context length\n",
    "    candidates = [m for m in candidates if m.context_length >= context_needed]\n",
    "    \n",
    "    # Filter by license if needed\n",
    "    if prefer_open_license:\n",
    "        candidates = [m for m in candidates if \"Apache\" in m.license]\n",
    "    \n",
    "    # Score by task fit\n",
    "    def score(model: ModelInfo) -> int:\n",
    "        s = 0\n",
    "        task_lower = task_type.lower()\n",
    "        for use in model.best_for:\n",
    "            if task_lower in use.lower() or use.lower() in task_lower:\n",
    "                s += 2\n",
    "        if needs_multilingual and \"multilingual\" in \" \".join(model.strengths).lower():\n",
    "            s += 1\n",
    "        return s\n",
    "    \n",
    "    if not candidates:\n",
    "        return OPEN_SOURCE_MODELS[0]  # Default to Llama 4 Scout\n",
    "    \n",
    "    return max(candidates, key=score)\n",
    "\n",
    "\n",
    "# Test model selection\n",
    "print(\"Model Selection Examples:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tasks = [\n",
    "    (\"code generation\", 8000, False, False),\n",
    "    (\"translation\", 4000, True, True),\n",
    "    (\"long document analysis\", 500_000, False, False),\n",
    "    (\"math reasoning\", 4000, False, False),\n",
    "]\n",
    "\n",
    "for task, context, multilingual, open_license in tasks:\n",
    "    model = select_model_for_task(task, context, multilingual, open_license)\n",
    "    print(f\"\\nTask: {task}\")\n",
    "    print(f\"  Context needed: {context:,} tokens\")\n",
    "    print(f\"  Recommended: {model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-Source Embedding Models\n",
    "\n",
    "For RAG applications, embedding model selection is critical. The MTEB (Massive Text Embedding Benchmark) leaderboard tracks performance across tasks like retrieval, classification, and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModelInfo(BaseModel):\n",
    "    \"\"\"Information about an embedding model.\"\"\"\n",
    "    name: str\n",
    "    provider: str\n",
    "    dimensions: int\n",
    "    max_tokens: int\n",
    "    mteb_avg_score: float\n",
    "    best_for: list[str]\n",
    "    license: str\n",
    "\n",
    "\n",
    "EMBEDDING_MODELS = [\n",
    "    EmbeddingModelInfo(\n",
    "        name=\"Qwen3-Embedding-8B\",\n",
    "        provider=\"Alibaba\",\n",
    "        dimensions=1024,\n",
    "        max_tokens=8192,\n",
    "        mteb_avg_score=72.3,\n",
    "        best_for=[\"Multilingual retrieval\", \"Long documents\", \"High accuracy\"],\n",
    "        license=\"Apache 2.0\"\n",
    "    ),\n",
    "    EmbeddingModelInfo(\n",
    "        name=\"BGE-M3\",\n",
    "        provider=\"BAAI\",\n",
    "        dimensions=1024,\n",
    "        max_tokens=8192,\n",
    "        mteb_avg_score=71.8,\n",
    "        best_for=[\"Multi-vector retrieval\", \"Dense + Sparse hybrid\", \"100+ languages\"],\n",
    "        license=\"MIT\"\n",
    "    ),\n",
    "    EmbeddingModelInfo(\n",
    "        name=\"text-embedding-3-large\",\n",
    "        provider=\"OpenAI\",\n",
    "        dimensions=3072,\n",
    "        max_tokens=8191,\n",
    "        mteb_avg_score=70.5,\n",
    "        best_for=[\"OpenAI ecosystem\", \"High quality\", \"Easy integration\"],\n",
    "        license=\"Proprietary\"\n",
    "    ),\n",
    "    EmbeddingModelInfo(\n",
    "        name=\"nomic-embed-text-v2-moe\",\n",
    "        provider=\"Nomic AI\",\n",
    "        dimensions=768,\n",
    "        max_tokens=8192,\n",
    "        mteb_avg_score=69.8,\n",
    "        best_for=[\"Cost-effective\", \"On-device\", \"Open weights\"],\n",
    "        license=\"Apache 2.0\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Embedding Model Comparison (MTEB Leaderboard - February 2026)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<30} {'MTEB':>6} {'Dims':>6} {'Max Tokens':>10}\")\n",
    "print(\"-\" * 70)\n",
    "for model in sorted(EMBEDDING_MODELS, key=lambda x: -x.mteb_avg_score):\n",
    "    print(f\"{model.name:<30} {model.mteb_avg_score:>6.1f} {model.dimensions:>6} {model.max_tokens:>10,}\")\n",
    "\n",
    "print(\"\\nKey insight: Task-specific performance matters more than overall MTEB score.\")\n",
    "print(\"Check the leaderboard at: https://huggingface.co/spaces/mteb/leaderboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Open Models with Together AI\n",
    "\n",
    "Together AI provides managed infrastructure for open-source models with an OpenAI-compatible API. This gives you the cost benefits of open models without the operational overhead of self-hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TogetherAIClient:\n",
    "    \"\"\"Wrapper for Together AI API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        self.api_key = api_key or os.getenv(\"TOGETHER_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"TOGETHER_API_KEY required\")\n",
    "        self.client = Together(api_key=self.api_key)\n",
    "    \n",
    "    def chat(self, model: str, messages: list[dict], **kwargs) -> str:\n",
    "        \"\"\"Send a chat completion request.\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def embed(self, model: str, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Generate embeddings for texts.\"\"\"\n",
    "        response = self.client.embeddings.create(\n",
    "            model=model,\n",
    "            input=texts\n",
    "        )\n",
    "        return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "# Test Together AI if available\n",
    "if TOGETHER_API_KEY:\n",
    "    print(\"Testing Together AI...\")\n",
    "    together_client = TogetherAIClient()\n",
    "    \n",
    "    # Test chat completion\n",
    "    response = together_client.chat(\n",
    "        model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is 2+2? Answer with just the number.\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    print(f\"Chat response: {response}\")\n",
    "else:\n",
    "    print(\"Together AI not configured.\")\n",
    "    print(\"To use Together AI:\")\n",
    "    print(\"  1. Sign up at https://together.ai\")\n",
    "    print(\"  2. Get your API key\")\n",
    "    print(\"  3. Set TOGETHER_API_KEY in your .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fallback Patterns\n",
    "\n",
    "A robust production system should gracefully fall back when the primary model fails. This pattern uses an open-source model as the primary (lower cost) with a proprietary model as fallback (higher reliability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFallbackChain:\n",
    "    \"\"\"Implement fallback from open model to proprietary API.\"\"\"\n",
    "    \n",
    "    def __init__(self, primary_model: str, fallback_model: str):\n",
    "        self.primary_model = primary_model\n",
    "        self.fallback_model = fallback_model\n",
    "        self.primary_client = TogetherAIClient() if TOGETHER_API_KEY else None\n",
    "        self.fallback_llm = ChatOpenAI(model=fallback_model)\n",
    "        self.stats = {\"primary_calls\": 0, \"fallback_calls\": 0}\n",
    "    \n",
    "    def invoke(self, prompt: str) -> tuple[str, str]:\n",
    "        \"\"\"Call primary model, fall back on failure.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (response, model_used)\n",
    "        \"\"\"\n",
    "        # Try primary (Together AI with open-source model)\n",
    "        if self.primary_client:\n",
    "            try:\n",
    "                response = self.primary_client.chat(\n",
    "                    model=self.primary_model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=500\n",
    "                )\n",
    "                self.stats[\"primary_calls\"] += 1\n",
    "                return response, self.primary_model\n",
    "            except Exception as e:\n",
    "                print(f\"Primary model failed: {e}. Falling back...\")\n",
    "        \n",
    "        # Fallback to OpenAI\n",
    "        response = self.fallback_llm.invoke(prompt)\n",
    "        self.stats[\"fallback_calls\"] += 1\n",
    "        return response.content, self.fallback_model\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get usage statistics.\"\"\"\n",
    "        total = self.stats[\"primary_calls\"] + self.stats[\"fallback_calls\"]\n",
    "        return {\n",
    "            **self.stats,\n",
    "            \"total_calls\": total,\n",
    "            \"primary_rate\": self.stats[\"primary_calls\"] / total if total > 0 else 0\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstration\n",
    "fallback_chain = ModelFallbackChain(\n",
    "    primary_model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    fallback_model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "response, model_used = fallback_chain.invoke(\"Explain RAG in one sentence.\")\n",
    "print(f\"Model used: {model_used}\")\n",
    "print(f\"Response: {response[:200]}...\" if len(response) > 200 else f\"Response: {response}\")\n",
    "print(f\"\\nStats: {fallback_chain.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking\n",
    "\n",
    "When comparing models and providers, measure latency, throughput, and cost systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkResult(BaseModel):\n",
    "    \"\"\"Result of a model benchmark.\"\"\"\n",
    "    model: str\n",
    "    provider: str\n",
    "    latency_ms: float\n",
    "    response_length: int\n",
    "    tokens_per_second: float\n",
    "    success: bool = True\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "def benchmark_model(\n",
    "    name: str,\n",
    "    provider: str,\n",
    "    invoke_fn,\n",
    "    prompt: str\n",
    ") -> BenchmarkResult:\n",
    "    \"\"\"Benchmark a single model invocation.\"\"\"\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        response = invoke_fn(prompt)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        \n",
    "        # Estimate tokens (rough: ~4 chars per token)\n",
    "        response_text = response if isinstance(response, str) else response.content\n",
    "        tokens = len(response_text) // 4\n",
    "        \n",
    "        return BenchmarkResult(\n",
    "            model=name,\n",
    "            provider=provider,\n",
    "            latency_ms=elapsed * 1000,\n",
    "            response_length=len(response_text),\n",
    "            tokens_per_second=tokens / elapsed if elapsed > 0 else 0,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return BenchmarkResult(\n",
    "            model=name,\n",
    "            provider=provider,\n",
    "            latency_ms=0,\n",
    "            response_length=0,\n",
    "            tokens_per_second=0,\n",
    "            success=False,\n",
    "            error=str(e)\n",
    "        )\n",
    "\n",
    "\n",
    "# Run benchmarks\n",
    "BENCHMARK_PROMPT = \"Explain the concept of PagedAttention in LLM inference in 3 sentences.\"\n",
    "\n",
    "results = []\n",
    "\n",
    "# OpenAI benchmark\n",
    "print(\"Benchmarking models...\")\n",
    "result = benchmark_model(\n",
    "    \"gpt-4o-mini\",\n",
    "    \"OpenAI\",\n",
    "    lambda p: llm.invoke(p),\n",
    "    BENCHMARK_PROMPT\n",
    ")\n",
    "results.append(result)\n",
    "print(f\"  OpenAI: {result.latency_ms:.0f}ms\")\n",
    "\n",
    "# Together AI benchmark (if available)\n",
    "if TOGETHER_API_KEY:\n",
    "    together_client = TogetherAIClient()\n",
    "    result = benchmark_model(\n",
    "        \"Llama-4-Scout-17B\",\n",
    "        \"Together AI\",\n",
    "        lambda p: together_client.chat(\n",
    "            model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "            messages=[{\"role\": \"user\", \"content\": p}],\n",
    "            max_tokens=200\n",
    "        ),\n",
    "        BENCHMARK_PROMPT\n",
    "    )\n",
    "    results.append(result)\n",
    "    print(f\"  Together AI: {result.latency_ms:.0f}ms\")\n",
    "\n",
    "# Ollama benchmark (if available)\n",
    "if OLLAMA_RUNNING:\n",
    "    ollama_llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    result = benchmark_model(\n",
    "        \"llama3.2\",\n",
    "        \"Ollama (local)\",\n",
    "        lambda p: ollama_llm.invoke(p),\n",
    "        BENCHMARK_PROMPT\n",
    "    )\n",
    "    results.append(result)\n",
    "    print(f\"  Ollama: {result.latency_ms:.0f}ms\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Provider':<20} {'Model':<25} {'Latency':>10} {'Tokens/s':>10}\")\n",
    "print(\"-\" * 70)\n",
    "for r in results:\n",
    "    if r.success:\n",
    "        print(f\"{r.provider:<20} {r.model:<25} {r.latency_ms:>8.0f}ms {r.tokens_per_second:>10.1f}\")\n",
    "    else:\n",
    "        print(f\"{r.provider:<20} {r.model:<25} FAILED: {r.error[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent-to-Agent Communication Patterns\n",
    "\n",
    "When building multi-agent systems at scale, you need robust communication patterns. This section covers three fundamental patterns:\n",
    "\n",
    "1. **Request-Response** — Synchronous call and wait\n",
    "2. **Pub-Sub** — Event-driven, decoupled communication\n",
    "3. **Event Sourcing** — Append-only log for audit trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentMessage(BaseModel):\n",
    "    \"\"\"Standard message format for agent communication.\"\"\"\n",
    "    sender: str\n",
    "    receiver: str\n",
    "    message_type: str  # \"request\", \"response\", \"event\"\n",
    "    payload: dict\n",
    "    correlation_id: str\n",
    "    timestamp: str = \"\"\n",
    "    \n",
    "    def __init__(self, **data):\n",
    "        if \"timestamp\" not in data or not data[\"timestamp\"]:\n",
    "            data[\"timestamp\"] = datetime.now().isoformat()\n",
    "        super().__init__(**data)\n",
    "\n",
    "\n",
    "# Example message\n",
    "msg = AgentMessage(\n",
    "    sender=\"coordinator\",\n",
    "    receiver=\"researcher\",\n",
    "    message_type=\"request\",\n",
    "    payload={\"task\": \"Find information about RAG\", \"max_results\": 5},\n",
    "    correlation_id=str(uuid.uuid4())\n",
    ")\n",
    "print(\"Example Agent Message:\")\n",
    "print(msg.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request-Response Pattern\n",
    "\n",
    "The simplest pattern: one agent sends a request, another processes it and sends back a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestResponseAgent:\n",
    "    \"\"\"Agent that handles request-response communication.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, handler=None):\n",
    "        self.name = name\n",
    "        self.handler = handler or (lambda payload: {\"result\": \"processed\"})\n",
    "        self.pending_requests: dict[str, AgentMessage] = {}\n",
    "    \n",
    "    def send_request(self, receiver: str, payload: dict) -> str:\n",
    "        \"\"\"Send a request and return correlation ID.\"\"\"\n",
    "        correlation_id = str(uuid.uuid4())\n",
    "        message = AgentMessage(\n",
    "            sender=self.name,\n",
    "            receiver=receiver,\n",
    "            message_type=\"request\",\n",
    "            payload=payload,\n",
    "            correlation_id=correlation_id\n",
    "        )\n",
    "        self.pending_requests[correlation_id] = message\n",
    "        print(f\"[{self.name}] Sent request {correlation_id[:8]}... to {receiver}\")\n",
    "        return correlation_id\n",
    "    \n",
    "    def handle_request(self, message: AgentMessage) -> AgentMessage:\n",
    "        \"\"\"Process an incoming request and return response.\"\"\"\n",
    "        print(f\"[{self.name}] Processing request from {message.sender}\")\n",
    "        result = self.handler(message.payload)\n",
    "        return AgentMessage(\n",
    "            sender=self.name,\n",
    "            receiver=message.sender,\n",
    "            message_type=\"response\",\n",
    "            payload=result,\n",
    "            correlation_id=message.correlation_id\n",
    "        )\n",
    "    \n",
    "    def receive_response(self, message: AgentMessage):\n",
    "        \"\"\"Process an incoming response.\"\"\"\n",
    "        if message.correlation_id in self.pending_requests:\n",
    "            del self.pending_requests[message.correlation_id]\n",
    "            print(f\"[{self.name}] Received response: {message.payload}\")\n",
    "            return message.payload\n",
    "        return None\n",
    "\n",
    "\n",
    "# Demonstration\n",
    "def research_handler(payload):\n",
    "    \"\"\"Simple research handler.\"\"\"\n",
    "    return {\"findings\": f\"Found 3 sources about {payload.get('topic', 'unknown')}\", \"confidence\": 0.85}\n",
    "\n",
    "\n",
    "coordinator = RequestResponseAgent(\"coordinator\")\n",
    "researcher = RequestResponseAgent(\"researcher\", handler=research_handler)\n",
    "\n",
    "# Simulate communication\n",
    "print(\"Request-Response Pattern Demo:\")\n",
    "print(\"-\" * 40)\n",
    "request_id = coordinator.send_request(\"researcher\", {\"topic\": \"RAG systems\"})\n",
    "\n",
    "# In a real system, this would go through a message queue\n",
    "request_msg = coordinator.pending_requests[request_id]\n",
    "response_msg = researcher.handle_request(request_msg)\n",
    "coordinator.receive_response(response_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pub-Sub for Event-Driven Coordination\n",
    "\n",
    "Publish-subscribe decouples agents: publishers emit events without knowing who listens, subscribers react to events they care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubSubCoordinator:\n",
    "    \"\"\"Event-driven coordination via publish-subscribe.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.subscribers: dict[str, list] = {}  # topic -> [callbacks]\n",
    "        self.event_log: list[AgentMessage] = []\n",
    "    \n",
    "    def subscribe(self, topic: str, callback):\n",
    "        \"\"\"Subscribe to events on a topic.\"\"\"\n",
    "        if topic not in self.subscribers:\n",
    "            self.subscribers[topic] = []\n",
    "        self.subscribers[topic].append(callback)\n",
    "        print(f\"Subscribed to '{topic}'\")\n",
    "    \n",
    "    def publish(self, topic: str, sender: str, payload: dict):\n",
    "        \"\"\"Publish an event to all subscribers.\"\"\"\n",
    "        message = AgentMessage(\n",
    "            sender=sender,\n",
    "            receiver=\"broadcast\",\n",
    "            message_type=\"event\",\n",
    "            payload={\"topic\": topic, **payload},\n",
    "            correlation_id=str(uuid.uuid4())\n",
    "        )\n",
    "        self.event_log.append(message)\n",
    "        \n",
    "        subscribers = self.subscribers.get(topic, [])\n",
    "        print(f\"[{sender}] Published to '{topic}' ({len(subscribers)} subscribers)\")\n",
    "        \n",
    "        for callback in subscribers:\n",
    "            callback(message)\n",
    "    \n",
    "    def get_event_count(self) -> int:\n",
    "        \"\"\"Return total events published.\"\"\"\n",
    "        return len(self.event_log)\n",
    "\n",
    "\n",
    "# Demonstration\n",
    "print(\"Pub-Sub Pattern Demo:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "coordinator = PubSubCoordinator()\n",
    "\n",
    "# Subscribe handlers\n",
    "def on_task_complete(msg):\n",
    "    print(f\"  [Logger] Task completed: {msg.payload.get('task_id')}\")\n",
    "\n",
    "def on_task_complete_notify(msg):\n",
    "    print(f\"  [Notifier] Sending notification for task {msg.payload.get('task_id')}\")\n",
    "\n",
    "coordinator.subscribe(\"task.complete\", on_task_complete)\n",
    "coordinator.subscribe(\"task.complete\", on_task_complete_notify)\n",
    "\n",
    "# Publish events\n",
    "coordinator.publish(\"task.complete\", \"worker-1\", {\"task_id\": \"T-001\", \"result\": \"success\"})\n",
    "coordinator.publish(\"task.complete\", \"worker-2\", {\"task_id\": \"T-002\", \"result\": \"success\"})\n",
    "\n",
    "print(f\"\\nTotal events: {coordinator.get_event_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Sourcing for Audit Trails\n",
    "\n",
    "Event sourcing stores every state change as an immutable event. This provides a complete audit trail and enables replay for debugging or recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventStore:\n",
    "    \"\"\"Append-only event store for audit trails.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events: list[dict] = []\n",
    "    \n",
    "    def append(self, event_type: str, agent: str, data: dict) -> dict:\n",
    "        \"\"\"Append an event to the store.\"\"\"\n",
    "        event = {\n",
    "            \"id\": len(self.events),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"type\": event_type,\n",
    "            \"agent\": agent,\n",
    "            \"data\": data\n",
    "        }\n",
    "        self.events.append(event)\n",
    "        return event\n",
    "    \n",
    "    def get_events_for_agent(self, agent: str) -> list[dict]:\n",
    "        \"\"\"Retrieve all events for a specific agent.\"\"\"\n",
    "        return [e for e in self.events if e[\"agent\"] == agent]\n",
    "    \n",
    "    def get_events_by_type(self, event_type: str) -> list[dict]:\n",
    "        \"\"\"Retrieve all events of a specific type.\"\"\"\n",
    "        return [e for e in self.events if e[\"type\"] == event_type]\n",
    "    \n",
    "    def replay(self, from_id: int = 0) -> list[dict]:\n",
    "        \"\"\"Replay events from a specific point.\"\"\"\n",
    "        return self.events[from_id:]\n",
    "\n",
    "\n",
    "# Demonstration\n",
    "print(\"Event Sourcing Demo:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "store = EventStore()\n",
    "\n",
    "# Simulate a workflow\n",
    "store.append(\"task.created\", \"coordinator\", {\"task\": \"Research LLM servers\", \"priority\": \"high\"})\n",
    "store.append(\"task.assigned\", \"coordinator\", {\"task_id\": 0, \"assignee\": \"researcher\"})\n",
    "store.append(\"task.started\", \"researcher\", {\"task_id\": 0})\n",
    "store.append(\"task.progress\", \"researcher\", {\"task_id\": 0, \"progress\": 50})\n",
    "store.append(\"task.completed\", \"researcher\", {\"task_id\": 0, \"result\": \"Found 5 relevant papers\"})\n",
    "\n",
    "print(f\"Total events: {len(store.events)}\")\n",
    "print(f\"\\nEvents by researcher:\")\n",
    "for event in store.get_events_for_agent(\"researcher\"):\n",
    "    print(f\"  [{event['type']}] {event['data']}\")\n",
    "\n",
    "print(f\"\\nReplay from event 2:\")\n",
    "for event in store.replay(2):\n",
    "    print(f\"  {event['id']}: [{event['type']}] {event['agent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposing Agents via MCP\n",
    "\n",
    "The Model Context Protocol (MCP) provides a standard way to expose your agent's capabilities to other agents and applications. An MCP server exposes:\n",
    "- **Tools** — Callable functions (like POST requests)\n",
    "- **Resources** — Read-only data (like GET requests)\n",
    "- **Prompts** — Reusable templates\n",
    "\n",
    "We'll build a simple Task Tracker MCP server to demonstrate these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mcp_servers directory\n",
    "os.makedirs(\"mcp_servers\", exist_ok=True)\n",
    "print(\"Created mcp_servers/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_servers/task_tracker_server.py\n",
    "\"\"\"Task Tracker MCP Server - Simple task management for agents.\"\"\"\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"TaskTracker\")\n",
    "\n",
    "# In-memory task storage\n",
    "TASKS = {}\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def create_task(title: str, description: str = \"\", priority: str = \"medium\") -> str:\n",
    "    \"\"\"Create a new task with title, description, and priority (low/medium/high).\"\"\"\n",
    "    task_id = f\"TASK-{len(TASKS) + 1:03d}\"\n",
    "    TASKS[task_id] = {\n",
    "        \"id\": task_id,\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"priority\": priority,\n",
    "        \"status\": \"pending\",\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"completed_at\": None\n",
    "    }\n",
    "    return json.dumps({\"created\": TASKS[task_id]})\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def list_tasks(status: str = \"all\") -> str:\n",
    "    \"\"\"List tasks filtered by status (all/pending/completed).\"\"\"\n",
    "    if status == \"all\":\n",
    "        tasks = list(TASKS.values())\n",
    "    else:\n",
    "        tasks = [t for t in TASKS.values() if t[\"status\"] == status]\n",
    "    return json.dumps({\"tasks\": tasks, \"count\": len(tasks)})\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def complete_task(task_id: str) -> str:\n",
    "    \"\"\"Mark a task as completed by its ID.\"\"\"\n",
    "    if task_id not in TASKS:\n",
    "        return json.dumps({\"error\": f\"Task {task_id} not found\"})\n",
    "    TASKS[task_id][\"status\"] = \"completed\"\n",
    "    TASKS[task_id][\"completed_at\"] = datetime.now().isoformat()\n",
    "    return json.dumps({\"completed\": TASKS[task_id]})\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_task(task_id: str) -> str:\n",
    "    \"\"\"Get details for a specific task.\"\"\"\n",
    "    if task_id not in TASKS:\n",
    "        return json.dumps({\"error\": f\"Task {task_id} not found\"})\n",
    "    return json.dumps(TASKS[task_id])\n",
    "\n",
    "\n",
    "@mcp.resource(\"tasks://summary\")\n",
    "def get_summary() -> str:\n",
    "    \"\"\"Get summary of all tasks (counts by status).\"\"\"\n",
    "    pending = sum(1 for t in TASKS.values() if t[\"status\"] == \"pending\")\n",
    "    completed = sum(1 for t in TASKS.values() if t[\"status\"] == \"completed\")\n",
    "    return json.dumps({\n",
    "        \"total\": len(TASKS),\n",
    "        \"pending\": pending,\n",
    "        \"completed\": completed\n",
    "    })\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the MCP Server\n",
    "\n",
    "We connect to our Task Tracker server using the MCP SDK and exercise its tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Task Tracker MCP server\n",
    "server_params = StdioServerParameters(\n",
    "    command=sys.executable,\n",
    "    args=[\"mcp_servers/task_tracker_server.py\"]\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "        \n",
    "        # Discover tools\n",
    "        tools = await session.list_tools()\n",
    "        print(\"Task Tracker MCP Tools:\")\n",
    "        for tool in tools.tools:\n",
    "            print(f\"  - {tool.name}: {tool.description[:60]}...\")\n",
    "        \n",
    "        # Create some tasks\n",
    "        print(\"\\n--- Creating tasks ---\")\n",
    "        result = await session.call_tool(\"create_task\", {\n",
    "            \"title\": \"Review PR #123\",\n",
    "            \"description\": \"Review the MCP integration PR\",\n",
    "            \"priority\": \"high\"\n",
    "        })\n",
    "        print(f\"Created: {result.content[0].text}\")\n",
    "        \n",
    "        result = await session.call_tool(\"create_task\", {\n",
    "            \"title\": \"Update documentation\",\n",
    "            \"priority\": \"medium\"\n",
    "        })\n",
    "        print(f\"Created: {result.content[0].text}\")\n",
    "        \n",
    "        # List all tasks\n",
    "        print(\"\\n--- All tasks ---\")\n",
    "        result = await session.call_tool(\"list_tasks\", {\"status\": \"all\"})\n",
    "        print(result.content[0].text)\n",
    "        \n",
    "        # Complete a task\n",
    "        print(\"\\n--- Completing task ---\")\n",
    "        result = await session.call_tool(\"complete_task\", {\"task_id\": \"TASK-001\"})\n",
    "        print(result.content[0].text)\n",
    "        \n",
    "        # Get summary\n",
    "        print(\"\\n--- Summary ---\")\n",
    "        result = await session.read_resource(\"tasks://summary\")\n",
    "        print(result.contents[0].text)\n",
    "\n",
    "print(\"\\nMCP server test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2A Protocol and Agent Cards\n",
    "\n",
    "The Agent-to-Agent (A2A) protocol enables agents to discover each other's capabilities. An Agent Card describes what an agent can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentCard(BaseModel):\n",
    "    \"\"\"Agent Card for capability discovery (A2A Protocol).\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    version: str\n",
    "    capabilities: list[str]\n",
    "    supported_modalities: list[str]  # text, audio, video, image\n",
    "    endpoint: str\n",
    "    authentication: dict\n",
    "    metadata: dict = {}\n",
    "\n",
    "\n",
    "# Task Tracker Agent Card\n",
    "task_tracker_card = AgentCard(\n",
    "    name=\"TaskTracker\",\n",
    "    description=\"Simple task management agent for tracking and organizing work\",\n",
    "    version=\"1.0.0\",\n",
    "    capabilities=[\n",
    "        \"create_task\",\n",
    "        \"list_tasks\",\n",
    "        \"complete_task\",\n",
    "        \"get_task\"\n",
    "    ],\n",
    "    supported_modalities=[\"text\"],\n",
    "    endpoint=\"mcp://localhost:8000/task-tracker\",\n",
    "    authentication={\"type\": \"none\"},\n",
    "    metadata={\n",
    "        \"author\": \"AI Engineering Bootcamp\",\n",
    "        \"tags\": [\"productivity\", \"tasks\", \"organization\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Agent Card:\")\n",
    "print(task_tracker_card.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2AClient:\n",
    "    \"\"\"Client for Agent-to-Agent communication via A2A protocol.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_card: AgentCard):\n",
    "        self.agent_card = agent_card\n",
    "    \n",
    "    def discover_capabilities(self) -> list[str]:\n",
    "        \"\"\"Discover what the remote agent can do.\"\"\"\n",
    "        return self.agent_card.capabilities\n",
    "    \n",
    "    def can_handle(self, capability: str) -> bool:\n",
    "        \"\"\"Check if agent supports a capability.\"\"\"\n",
    "        return capability in self.agent_card.capabilities\n",
    "    \n",
    "    def get_endpoint(self) -> str:\n",
    "        \"\"\"Get the agent's endpoint.\"\"\"\n",
    "        return self.agent_card.endpoint\n",
    "\n",
    "\n",
    "# Demonstration: Agent discovery\n",
    "print(\"A2A Discovery Demo:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "client = A2AClient(task_tracker_card)\n",
    "\n",
    "print(f\"Agent: {task_tracker_card.name}\")\n",
    "print(f\"Endpoint: {client.get_endpoint()}\")\n",
    "print(f\"Capabilities: {client.discover_capabilities()}\")\n",
    "print(f\"\\nCan handle 'create_task'? {client.can_handle('create_task')}\")\n",
    "print(f\"Can handle 'delete_task'? {client.can_handle('delete_task')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing It All Together\n",
    "\n",
    "Let's combine all the concepts from this chapter into a complete demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ADVANCED INFRASTRUCTURE: COMPLETE DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Model Selection\n",
    "print(\"\\n1. MODEL SELECTION\")\n",
    "print(\"-\" * 40)\n",
    "task = \"Generate a summary of LLM server options\"\n",
    "\n",
    "if OLLAMA_RUNNING:\n",
    "    model_choice = \"Ollama (local)\"\n",
    "    provider = \"local\"\n",
    "elif TOGETHER_API_KEY:\n",
    "    model_choice = \"Llama-4-Scout via Together AI\"\n",
    "    provider = \"together\"\n",
    "else:\n",
    "    model_choice = \"gpt-4o-mini via OpenAI\"\n",
    "    provider = \"openai\"\n",
    "\n",
    "print(f\"Task: {task}\")\n",
    "print(f\"Selected: {model_choice}\")\n",
    "\n",
    "# 2. Execute with Fallback\n",
    "print(\"\\n2. EXECUTION WITH FALLBACK\")\n",
    "print(\"-\" * 40)\n",
    "fallback = ModelFallbackChain(\n",
    "    primary_model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    fallback_model=\"gpt-4o-mini\"\n",
    ")\n",
    "response, model_used = fallback.invoke(\"List 3 popular LLM servers in one sentence.\")\n",
    "print(f\"Model used: {model_used}\")\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# 3. MCP Server Exposure\n",
    "print(\"\\n3. MCP SERVER EXPOSURE\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=sys.executable,\n",
    "    args=[\"mcp_servers/task_tracker_server.py\"]\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "        tools = await session.list_tools()\n",
    "        print(f\"Exposed {len(tools.tools)} tools via MCP:\")\n",
    "        for tool in tools.tools:\n",
    "            print(f\"  - {tool.name}\")\n",
    "\n",
    "# 4. Agent Communication\n",
    "print(\"\\n4. AGENT-TO-AGENT COORDINATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "event_store = EventStore()\n",
    "event_store.append(\"demo.started\", \"coordinator\", {\"chapter\": 13})\n",
    "event_store.append(\"model.selected\", \"coordinator\", {\"model\": model_used})\n",
    "event_store.append(\"mcp.exposed\", \"task-tracker\", {\"tools_count\": len(tools.tools)})\n",
    "event_store.append(\"demo.completed\", \"coordinator\", {\"success\": True})\n",
    "\n",
    "print(f\"Event log has {len(event_store.events)} events:\")\n",
    "for event in event_store.events:\n",
    "    print(f\"  [{event['type']}] {event['agent']}: {event['data']}\")\n",
    "\n",
    "# 5. Cost Summary\n",
    "print(\"\\n5. COST COMPARISON\")\n",
    "print(\"-\" * 40)\n",
    "analyze_costs(10_000_000, 5_000_000)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMO COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
