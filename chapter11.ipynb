{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Connectors\n",
    "\n",
    "## The Model Context Protocol\n",
    "\n",
    "This notebook explores the **Model Context Protocol (MCP)** — a standardized way for AI applications to connect to external data sources and tools. MCP defines a client-server architecture where AI agents (clients) connect to capability providers (servers) through a uniform protocol.\n",
    "\n",
    "MCP servers expose three types of capabilities:\n",
    "- **Resources** — Read-only data endpoints (like GET requests)\n",
    "- **Tools** — Callable functions with side effects (like POST requests)\n",
    "- **Prompts** — Reusable interaction templates\n",
    "\n",
    "We'll build MCP servers that expose real-world capabilities (product inventory, weather data, file systems, databases, and APIs), then connect to them using both the raw MCP SDK and LangChain adapters.\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────┐\n",
    "│                    AI Application                      │\n",
    "│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │\n",
    "│  │  LangChain   │  │   Direct     │  │  LangGraph   │  │\n",
    "│  │  MCP Adapter │  │  MCP Client  │  │    Agent     │  │\n",
    "│  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘  │\n",
    "└─────────┼─────────────────┼─────────────────┼──────────┘\n",
    "          │                 │                 │\n",
    "    ┌─────┴─────┐     ┌─────┴─────┐     ┌─────┴─────┐\n",
    "    │   stdio   │     │   stdio   │     │   HTTP    │\n",
    "    │ transport │     │ transport │     │ transport │\n",
    "    └─────┬─────┘     └─────┬─────┘     └─────┬─────┘\n",
    "          │                 │                 │\n",
    "   ┌──────┴──────┐    ┌─────┴──────┐   ┌──────┴──────┐\n",
    "   │  Inventory  │    │ File System│   │   Weather   │\n",
    "   │   Server    │    │   Server   │   │   Server    │\n",
    "   │ (tools +    │    │(resources) │   │ (tools +    │\n",
    "   │  resources) │    │            │   │  auth)      │\n",
    "   └─────────────┘    └────────────┘   └─────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Make sure you have the required packages installed:\n",
    "```\n",
    "uv sync\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# MCP SDK imports\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# LangChain MCP adapter imports\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create the directory for MCP server files\n",
    "os.makedirs(\"mcp_servers\", exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP Server Files\n",
    "\n",
    "MCP servers run as **separate processes** — the client launches them via stdio transport (stdin/stdout communication). We use `%%writefile` to create each server as a `.py` file in the `mcp_servers/` subdirectory that the MCP client can spawn as a subprocess.\n",
    "\n",
    "We'll create several servers throughout this notebook:\n",
    "1. **Inventory Server** — E-commerce product catalog (our primary example)\n",
    "2. **Weather Server** — Weather forecasts with API token authentication\n",
    "3. **Filesystem Server** — Local file browsing and search\n",
    "4. **Database Server** — SQLite-backed customer order management\n",
    "5. **GitHub API Server** — REST API integration with GitHub's public API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_servers/inventory_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_servers/inventory_server.py\n",
    "\"\"\"Product Inventory MCP Server — Electronics retailer catalog and stock management.\"\"\"\n",
    "import json\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Inventory\")\n",
    "\n",
    "# In-memory product database\n",
    "PRODUCTS = {\n",
    "    \"LAPTOP-001\": {\"name\": \"ProBook 15\", \"category\": \"laptops\", \"price\": 1299.99, \"stock\": 23,\n",
    "                   \"description\": \"15-inch professional laptop with 32GB RAM\"},\n",
    "    \"LAPTOP-002\": {\"name\": \"UltraLight Air\", \"category\": \"laptops\", \"price\": 999.99, \"stock\": 45,\n",
    "                   \"description\": \"Ultra-portable 13-inch laptop, 1kg weight\"},\n",
    "    \"HEADPH-001\": {\"name\": \"StudioPro X\", \"category\": \"audio\", \"price\": 349.99, \"stock\": 67,\n",
    "                   \"description\": \"Noise-cancelling over-ear headphones\"},\n",
    "    \"HEADPH-002\": {\"name\": \"BudFit Sport\", \"category\": \"audio\", \"price\": 129.99, \"stock\": 150,\n",
    "                   \"description\": \"Wireless sport earbuds, waterproof\"},\n",
    "    \"MONITOR-001\": {\"name\": \"ClearView 4K\", \"category\": \"monitors\", \"price\": 599.99, \"stock\": 12,\n",
    "                    \"description\": \"27-inch 4K IPS display, USB-C\"},\n",
    "    \"MONITOR-002\": {\"name\": \"UltraWide 34\", \"category\": \"monitors\", \"price\": 799.99, \"stock\": 8,\n",
    "                    \"description\": \"34-inch ultrawide curved monitor\"},\n",
    "    \"KEYBOARD-001\": {\"name\": \"MechType Pro\", \"category\": \"peripherals\", \"price\": 179.99, \"stock\": 89,\n",
    "                     \"description\": \"Mechanical keyboard with hot-swap switches\"},\n",
    "    \"MOUSE-001\": {\"name\": \"PrecisionGlide\", \"category\": \"peripherals\", \"price\": 79.99, \"stock\": 200,\n",
    "                  \"description\": \"Ergonomic wireless mouse, 8000 DPI\"},\n",
    "}\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def search_products(query: str, category: str | None = None) -> str:\n",
    "    \"\"\"Search the product catalog by keyword and optional category filter.\"\"\"\n",
    "    results = []\n",
    "    query_lower = query.lower()\n",
    "    for pid, product in PRODUCTS.items():\n",
    "        if category and product[\"category\"] != category:\n",
    "            continue\n",
    "        if query_lower in product[\"name\"].lower() or query_lower in product[\"description\"].lower():\n",
    "            results.append({\"id\": pid, **product})\n",
    "    if not results:\n",
    "        return json.dumps({\"results\": [], \"message\": \"No products found\"})\n",
    "    return json.dumps({\"results\": results})\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def check_stock(product_id: str) -> str:\n",
    "    \"\"\"Check current inventory stock level for a product by its ID.\"\"\"\n",
    "    product = PRODUCTS.get(product_id)\n",
    "    if not product:\n",
    "        return json.dumps({\"error\": f\"Product {product_id} not found\"})\n",
    "    return json.dumps({\n",
    "        \"product_id\": product_id, \"name\": product[\"name\"],\n",
    "        \"stock\": product[\"stock\"],\n",
    "        \"status\": \"in_stock\" if product[\"stock\"] > 0 else \"out_of_stock\"\n",
    "    })\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_price(product_id: str) -> str:\n",
    "    \"\"\"Get the current price for a product by its ID.\"\"\"\n",
    "    product = PRODUCTS.get(product_id)\n",
    "    if not product:\n",
    "        return json.dumps({\"error\": f\"Product {product_id} not found\"})\n",
    "    return json.dumps({\"product_id\": product_id, \"name\": product[\"name\"], \"price\": product[\"price\"]})\n",
    "\n",
    "\n",
    "@mcp.resource(\"inventory://catalog\")\n",
    "def get_catalog() -> str:\n",
    "    \"\"\"Get the full product catalog.\"\"\"\n",
    "    return json.dumps({\"products\": {pid: p for pid, p in PRODUCTS.items()}}, indent=2)\n",
    "\n",
    "\n",
    "@mcp.resource(\"inventory://product/{product_id}\")\n",
    "def get_product(product_id: str) -> str:\n",
    "    \"\"\"Get details for a specific product.\"\"\"\n",
    "    product = PRODUCTS.get(product_id)\n",
    "    if not product:\n",
    "        return json.dumps({\"error\": f\"Product {product_id} not found\"})\n",
    "    return json.dumps({\"id\": product_id, **product}, indent=2)\n",
    "\n",
    "\n",
    "@mcp.resource(\"inventory://categories\")\n",
    "def get_categories() -> str:\n",
    "    \"\"\"Get the list of product categories.\"\"\"\n",
    "    categories = list(set(p[\"category\"] for p in PRODUCTS.values()))\n",
    "    return json.dumps({\"categories\": sorted(categories)})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_servers/weather_server.py\n",
    "\"\"\"Weather MCP Server — Demonstrates authentication via environment variables.\"\"\"\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Weather\")\n",
    "\n",
    "# Simulated weather data\n",
    "CONDITIONS = [\"Sunny\", \"Partly Cloudy\", \"Cloudy\", \"Light Rain\", \"Heavy Rain\", \"Thunderstorm\", \"Snow\", \"Fog\"]\n",
    "\n",
    "\n",
    "def _check_auth() -> str | None:\n",
    "    \"\"\"Check if API_TOKEN is set. Returns error message or None.\"\"\"\n",
    "    token = os.environ.get(\"API_TOKEN\")\n",
    "    if not token:\n",
    "        return \"Authentication required: API_TOKEN environment variable not set\"\n",
    "    return None\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_forecast(city: str, days: int = 3) -> str:\n",
    "    \"\"\"Get a weather forecast for a city. Requires API_TOKEN to be set.\"\"\"\n",
    "    auth_error = _check_auth()\n",
    "    if auth_error:\n",
    "        return json.dumps({\"error\": auth_error})\n",
    "\n",
    "    random.seed(hash(city))  # Deterministic per city for demo\n",
    "    forecast = []\n",
    "    for day in range(days):\n",
    "        forecast.append({\n",
    "            \"day\": day + 1,\n",
    "            \"condition\": random.choice(CONDITIONS),\n",
    "            \"high_f\": random.randint(55, 85),\n",
    "            \"low_f\": random.randint(35, 60),\n",
    "            \"humidity_pct\": random.randint(30, 90),\n",
    "        })\n",
    "    return json.dumps({\"city\": city, \"forecast\": forecast})\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_alerts(region: str) -> str:\n",
    "    \"\"\"Get active weather alerts for a region. Requires API_TOKEN to be set.\"\"\"\n",
    "    auth_error = _check_auth()\n",
    "    if auth_error:\n",
    "        return json.dumps({\"error\": auth_error})\n",
    "\n",
    "    # Simulated alerts\n",
    "    alerts = [\n",
    "        {\"type\": \"Wind Advisory\", \"severity\": \"moderate\",\n",
    "         \"message\": f\"Gusty winds expected in {region} through tonight.\"},\n",
    "    ]\n",
    "    return json.dumps({\"region\": region, \"alerts\": alerts})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_servers/filesystem_server.py\n",
    "\"\"\"Filesystem MCP Server — Browse and read local files safely.\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob as globmod\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# Root directory is passed as a command-line argument\n",
    "ROOT_DIR = os.path.abspath(sys.argv[1]) if len(sys.argv) > 1 else os.getcwd()\n",
    "\n",
    "mcp = FastMCP(\"Filesystem\")\n",
    "\n",
    "\n",
    "def _safe_path(path: str) -> str | None:\n",
    "    \"\"\"Resolve path and ensure it's within ROOT_DIR. Returns None if unsafe.\"\"\"\n",
    "    resolved = os.path.normpath(os.path.join(ROOT_DIR, path))\n",
    "    if not resolved.startswith(ROOT_DIR):\n",
    "        return None\n",
    "    return resolved\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def list_directory(path: str = \".\") -> str:\n",
    "    \"\"\"List files and subdirectories at the given path (relative to root).\"\"\"\n",
    "    safe = _safe_path(path)\n",
    "    if not safe:\n",
    "        return json.dumps({\"error\": \"Path outside allowed directory\"})\n",
    "    if not os.path.isdir(safe):\n",
    "        return json.dumps({\"error\": f\"Not a directory: {path}\"})\n",
    "    entries = []\n",
    "    for name in sorted(os.listdir(safe)):\n",
    "        full = os.path.join(safe, name)\n",
    "        entries.append({\"name\": name, \"type\": \"dir\" if os.path.isdir(full) else \"file\",\n",
    "                        \"size\": os.path.getsize(full) if os.path.isfile(full) else None})\n",
    "    return json.dumps({\"path\": path, \"entries\": entries})\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def read_file(path: str) -> str:\n",
    "    \"\"\"Read the contents of a file (relative to root). Limited to 10KB.\"\"\"\n",
    "    safe = _safe_path(path)\n",
    "    if not safe:\n",
    "        return json.dumps({\"error\": \"Path outside allowed directory\"})\n",
    "    if not os.path.isfile(safe):\n",
    "        return json.dumps({\"error\": f\"Not a file: {path}\"})\n",
    "    try:\n",
    "        with open(safe, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            content = f.read(10240)  # 10KB limit\n",
    "        return json.dumps({\"path\": path, \"content\": content, \"truncated\": os.path.getsize(safe) > 10240})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def search_files(directory: str, pattern: str) -> str:\n",
    "    \"\"\"Search for files matching a glob pattern within a directory.\"\"\"\n",
    "    safe = _safe_path(directory)\n",
    "    if not safe:\n",
    "        return json.dumps({\"error\": \"Path outside allowed directory\"})\n",
    "    full_pattern = os.path.join(safe, \"**\", pattern)\n",
    "    matches = globmod.glob(full_pattern, recursive=True)\n",
    "    # Convert to relative paths\n",
    "    relative = [os.path.relpath(m, ROOT_DIR) for m in matches\n",
    "                if m.startswith(ROOT_DIR)]\n",
    "    return json.dumps({\"pattern\": pattern, \"directory\": directory, \"matches\": sorted(relative)[:50]})\n",
    "\n",
    "\n",
    "@mcp.resource(\"file://summary\")\n",
    "def get_summary() -> str:\n",
    "    \"\"\"Get a summary of the root directory.\"\"\"\n",
    "    file_count = 0\n",
    "    dir_count = 0\n",
    "    for _, dirs, files in os.walk(ROOT_DIR):\n",
    "        file_count += len(files)\n",
    "        dir_count += len(dirs)\n",
    "    return json.dumps({\"root\": ROOT_DIR, \"total_files\": file_count, \"total_directories\": dir_count})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_servers/database_server.py\n",
    "\"\"\"Database MCP Server — SQLite-backed customer order management.\"\"\"\n",
    "import json\n",
    "import sqlite3\n",
    "from contextlib import asynccontextmanager\n",
    "from collections.abc import AsyncIterator\n",
    "from dataclasses import dataclass\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DbContext:\n",
    "    conn: sqlite3.Connection\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def db_lifespan(server: FastMCP) -> AsyncIterator[DbContext]:\n",
    "    \"\"\"Manage database connection lifecycle.\"\"\"\n",
    "    conn = sqlite3.connect(\":memory:\")\n",
    "    conn.row_factory = sqlite3.Row\n",
    "\n",
    "    # Create tables and seed data\n",
    "    conn.executescript(\"\"\"\n",
    "        CREATE TABLE customers (\n",
    "            id INTEGER PRIMARY KEY, name TEXT, email TEXT\n",
    "        );\n",
    "        CREATE TABLE orders (\n",
    "            id TEXT PRIMARY KEY, customer_id INTEGER, product TEXT,\n",
    "            quantity INTEGER, total REAL, status TEXT,\n",
    "            FOREIGN KEY (customer_id) REFERENCES customers(id)\n",
    "        );\n",
    "        INSERT INTO customers VALUES (1, 'Alice Johnson', 'alice@example.com');\n",
    "        INSERT INTO customers VALUES (2, 'Bob Smith', 'bob@example.com');\n",
    "        INSERT INTO customers VALUES (3, 'Carol Williams', 'carol@example.com');\n",
    "        INSERT INTO orders VALUES ('ORD-001', 1, 'ProBook 15', 1, 1299.99, 'shipped');\n",
    "        INSERT INTO orders VALUES ('ORD-002', 1, 'MechType Pro', 2, 359.98, 'delivered');\n",
    "        INSERT INTO orders VALUES ('ORD-003', 2, 'UltraWide 34', 1, 799.99, 'processing');\n",
    "        INSERT INTO orders VALUES ('ORD-004', 3, 'BudFit Sport', 3, 389.97, 'shipped');\n",
    "        INSERT INTO orders VALUES ('ORD-005', 2, 'PrecisionGlide', 1, 79.99, 'delivered');\n",
    "    \"\"\")\n",
    "\n",
    "    try:\n",
    "        yield DbContext(conn=conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "mcp = FastMCP(\"Database\", lifespan=db_lifespan)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def query_orders(customer_name: str) -> str:\n",
    "    \"\"\"Look up all orders for a customer by name.\"\"\"\n",
    "    ctx: DbContext = mcp.get_context()\n",
    "    cursor = ctx.conn.execute(\n",
    "        \"SELECT o.id, o.product, o.quantity, o.total, o.status \"\n",
    "        \"FROM orders o JOIN customers c ON o.customer_id = c.id \"\n",
    "        \"WHERE c.name LIKE ?\", (f\"%{customer_name}%\",)\n",
    "    )\n",
    "    rows = [dict(row) for row in cursor.fetchall()]\n",
    "    return json.dumps({\"customer\": customer_name, \"orders\": rows})\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_order_status(order_id: str) -> str:\n",
    "    \"\"\"Check the status of a specific order by order ID.\"\"\"\n",
    "    ctx: DbContext = mcp.get_context()\n",
    "    cursor = ctx.conn.execute(\n",
    "        \"SELECT o.id, c.name as customer, o.product, o.status \"\n",
    "        \"FROM orders o JOIN customers c ON o.customer_id = c.id \"\n",
    "        \"WHERE o.id = ?\", (order_id,)\n",
    "    )\n",
    "    row = cursor.fetchone()\n",
    "    if not row:\n",
    "        return json.dumps({\"error\": f\"Order {order_id} not found\"})\n",
    "    return json.dumps(dict(row))\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_sales_summary() -> str:\n",
    "    \"\"\"Get an aggregate sales report across all orders.\"\"\"\n",
    "    ctx: DbContext = mcp.get_context()\n",
    "    cursor = ctx.conn.execute(\n",
    "        \"SELECT COUNT(*) as total_orders, SUM(total) as total_revenue, \"\n",
    "        \"COUNT(DISTINCT customer_id) as unique_customers \"\n",
    "        \"FROM orders\"\n",
    "    )\n",
    "    row = dict(cursor.fetchone())\n",
    "\n",
    "    # Get orders by status\n",
    "    status_cursor = ctx.conn.execute(\n",
    "        \"SELECT status, COUNT(*) as count FROM orders GROUP BY status\"\n",
    "    )\n",
    "    row[\"by_status\"] = {r[\"status\"]: r[\"count\"] for r in status_cursor.fetchall()}\n",
    "    return json.dumps(row)\n",
    "\n",
    "\n",
    "@mcp.resource(\"db://schema\")\n",
    "def get_schema() -> str:\n",
    "    \"\"\"Get the database schema description.\"\"\"\n",
    "    return json.dumps({\n",
    "        \"tables\": {\n",
    "            \"customers\": {\"columns\": [\"id (INTEGER PK)\", \"name (TEXT)\", \"email (TEXT)\"]},\n",
    "            \"orders\": {\"columns\": [\"id (TEXT PK)\", \"customer_id (FK)\", \"product (TEXT)\",\n",
    "                                    \"quantity (INTEGER)\", \"total (REAL)\", \"status (TEXT)\"]},\n",
    "        },\n",
    "        \"sample_statuses\": [\"processing\", \"shipped\", \"delivered\"],\n",
    "    }, indent=2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_servers/github_api_server.py\n",
    "\"\"\"GitHub API MCP Server — Wraps GitHub's public REST API as MCP tools.\"\"\"\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"GitHubAPI\")\n",
    "\n",
    "\n",
    "def _github_get(url: str) -> dict | list:\n",
    "    \"\"\"Make a GET request to the GitHub API.\"\"\"\n",
    "    req = urllib.request.Request(url, headers={\n",
    "        \"Accept\": \"application/vnd.github.v3+json\",\n",
    "        \"User-Agent\": \"MCP-GitHub-Server/1.0\",\n",
    "    })\n",
    "    try:\n",
    "        with urllib.request.urlopen(req, timeout=10) as resp:\n",
    "            return json.loads(resp.read().decode())\n",
    "    except urllib.error.HTTPError as e:\n",
    "        error_body = e.read().decode() if e.fp else \"\"\n",
    "        return {\"error\": f\"GitHub API returned {e.code}\", \"message\": error_body[:200]}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_repo_info(owner: str, repo: str) -> str:\n",
    "    \"\"\"Get metadata about a GitHub repository (stars, language, description, etc.).\"\"\"\n",
    "    data = _github_get(f\"https://api.github.com/repos/{owner}/{repo}\")\n",
    "    if \"error\" in data:\n",
    "        return json.dumps(data)\n",
    "    return json.dumps({\n",
    "        \"full_name\": data.get(\"full_name\"),\n",
    "        \"description\": data.get(\"description\"),\n",
    "        \"stargazers_count\": data.get(\"stargazers_count\"),\n",
    "        \"language\": data.get(\"language\"),\n",
    "        \"open_issues_count\": data.get(\"open_issues_count\"),\n",
    "        \"forks_count\": data.get(\"forks_count\"),\n",
    "        \"created_at\": data.get(\"created_at\"),\n",
    "        \"updated_at\": data.get(\"updated_at\"),\n",
    "    })\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def list_issues(owner: str, repo: str, state: str = \"open\") -> str:\n",
    "    \"\"\"List issues for a GitHub repository. State can be 'open', 'closed', or 'all'.\"\"\"\n",
    "    data = _github_get(f\"https://api.github.com/repos/{owner}/{repo}/issues?state={state}&per_page=5\")\n",
    "    if isinstance(data, dict) and \"error\" in data:\n",
    "        return json.dumps(data)\n",
    "    issues = [{\n",
    "        \"number\": i.get(\"number\"),\n",
    "        \"title\": i.get(\"title\"),\n",
    "        \"state\": i.get(\"state\"),\n",
    "        \"created_at\": i.get(\"created_at\"),\n",
    "        \"labels\": [l[\"name\"] for l in i.get(\"labels\", [])],\n",
    "    } for i in data[:5]]\n",
    "    return json.dumps(issues)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def search_repos(query: str) -> str:\n",
    "    \"\"\"Search GitHub repositories by keyword. Returns top 5 results.\"\"\"\n",
    "    encoded = urllib.parse.quote(query)\n",
    "    data = _github_get(f\"https://api.github.com/search/repositories?q={encoded}&per_page=5\")\n",
    "    if \"error\" in data:\n",
    "        return json.dumps(data)\n",
    "    items = [{\n",
    "        \"full_name\": r.get(\"full_name\"),\n",
    "        \"description\": (r.get(\"description\") or \"\")[:100],\n",
    "        \"stargazers_count\": r.get(\"stargazers_count\"),\n",
    "        \"language\": r.get(\"language\"),\n",
    "    } for r in data.get(\"items\", [])[:5]]\n",
    "    return json.dumps({\"items\": items, \"total_count\": data.get(\"total_count\", 0)})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import urllib.parse\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client-Side MCP Usage\n",
    "\n",
    "This section covers connecting to MCP servers from client code. We'll start with the raw MCP SDK for low-level control, then move to LangChain adapters for agent integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to MCP Servers\n",
    "\n",
    "The fundamental client connection pattern has three layers:\n",
    "1. **`StdioServerParameters`** — Defines the server command and arguments\n",
    "2. **`stdio_client()`** — Creates the transport (spawns the subprocess)\n",
    "3. **`ClientSession`** — Provides the API to list and call tools/resources\n",
    "\n",
    "These are nested async context managers. When the outer context exits, the server subprocess is automatically terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the inventory server using the raw MCP SDK\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_servers/inventory_server.py\"],\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        # Discover available tools\n",
    "        tools_result = await session.list_tools()\n",
    "        print(\"Available Tools:\")\n",
    "        for tool in tools_result.tools:\n",
    "            print(f\"  - {tool.name}: {tool.description}\")\n",
    "\n",
    "        # Discover available resources\n",
    "        resources_result = await session.list_resources()\n",
    "        print(\"\\nAvailable Resources:\")\n",
    "        for resource in resources_result.resources:\n",
    "            print(f\"  - {resource.uri}: {resource.name}\")\n",
    "\n",
    "        # Call a tool\n",
    "        print(\"\\n--- Calling search_products('laptop') ---\")\n",
    "        result = await session.call_tool(\"search_products\", {\"query\": \"laptop\"})\n",
    "        print(json.dumps(json.loads(result.content[0].text), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain MCP Adapters\n",
    "\n",
    "`MultiServerMCPClient` from `langchain-mcp-adapters` bridges MCP tools into LangChain's tool ecosystem. It can connect to multiple MCP servers at once, loading all their tools as `BaseTool` instances that work with any LangChain agent.\n",
    "\n",
    "The key value: you get MCP's standardized server interface **plus** LangChain's agent reasoning — the agent decides which tools to call and when."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Connect to the inventory server via LangChain adapter\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"inventory\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"mcp_servers/inventory_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        }\n",
    "    }\n",
    ") as client:\n",
    "    # Load MCP tools as LangChain tools\n",
    "    tools = client.get_tools()\n",
    "\n",
    "    print(\"LangChain Tools loaded from MCP:\")\n",
    "    for tool in tools:\n",
    "        print(f\"  - {tool.name}: {tool.description[:80]}...\")\n",
    "\n",
    "    # Create an agent that uses MCP tools\n",
    "    agent = create_agent(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=tools,\n",
    "        prompt=\"You are a helpful shopping assistant for an electronics store. \"\n",
    "               \"Use the available tools to help customers find products and check availability.\",\n",
    "    )\n",
    "\n",
    "    response = await agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"I need a lightweight laptop for travel. What do you have?\"}]}\n",
    "    )\n",
    "\n",
    "    # Display the agent's final response\n",
    "    for msg in response[\"messages\"]:\n",
    "        if hasattr(msg, \"type\") and msg.type == \"ai\" and msg.content and not getattr(msg, \"tool_calls\", []):\n",
    "            print(f\"\\nAgent Response:\\n{msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MCP for Enhanced Retrieval\n",
    "\n",
    "MCP resources and tools complement each other for retrieval:\n",
    "- **Resources** provide bulk data (like reading an entire catalog) — useful for broad context\n",
    "- **Tools** perform targeted queries (like searching for specific products) — useful for precision\n",
    "\n",
    "Combining both in a pipeline gives you a RAG-like pattern: first scan the broad context, then drill into specifics, then hand everything to the LLM for a grounded response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced retrieval: combine resources (bulk data) with tools (targeted queries)\n",
    "server_params = StdioServerParameters(command=\"python\", args=[\"mcp_servers/inventory_server.py\"])\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        # Step 1: Read the categories resource for catalog structure\n",
    "        print(\"Step 1: Read catalog structure via resource\")\n",
    "        categories = await session.read_resource(\"inventory://categories\")\n",
    "        cats = json.loads(categories.contents[0].text)\n",
    "        print(f\"  Categories: {cats['categories']}\")\n",
    "\n",
    "        # Step 2: Read full catalog resource for broad context\n",
    "        print(\"\\nStep 2: Read full catalog via resource\")\n",
    "        catalog = await session.read_resource(\"inventory://catalog\")\n",
    "        catalog_data = json.loads(catalog.contents[0].text)\n",
    "        print(f\"  Total products: {len(catalog_data['products'])}\")\n",
    "\n",
    "        # Step 3: Use tool for targeted search\n",
    "        print(\"\\nStep 3: Targeted search via tool\")\n",
    "        result = await session.call_tool(\"search_products\", {\"query\": \"wireless\", \"category\": \"audio\"})\n",
    "        search_results = json.loads(result.content[0].text)\n",
    "        print(f\"  Found {len(search_results['results'])} wireless audio products\")\n",
    "        for p in search_results[\"results\"]:\n",
    "            print(f\"    - {p['name']}: ${p['price']}\")\n",
    "\n",
    "        # Step 4: Feed combined context to LLM for a grounded recommendation\n",
    "        print(\"\\nStep 4: LLM recommendation grounded in MCP data\")\n",
    "        context = f\"Catalog has {len(catalog_data['products'])} products across categories: {cats['categories']}.\\n\"\n",
    "        context += f\"Wireless audio options: {json.dumps(search_results['results'], indent=2)}\"\n",
    "\n",
    "        recommendation = llm.invoke([\n",
    "            SystemMessage(content=\"You are a product advisor. Recommend a product based on the catalog data provided. Be concise.\"),\n",
    "            HumanMessage(content=f\"Customer wants wireless audio for running.\\n\\n{context}\")\n",
    "        ])\n",
    "        print(f\"  Recommendation: {recommendation.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication and Authorization\n",
    "\n",
    "MCP supports two authentication patterns:\n",
    "\n",
    "1. **Environment variables (stdio transport)** — Pass credentials via the `env` parameter of `StdioServerParameters`. The server checks `os.environ` on its side.\n",
    "2. **HTTP headers (HTTP/SSE transport)** — Pass `Authorization` headers in the `MultiServerMCPClient` configuration for remote servers.\n",
    "\n",
    "Our weather server requires an `API_TOKEN` environment variable, demonstrating the stdio pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Pattern 1: Auth via environment variables (stdio transport)\n",
    "\n",
    "# First, connect WITHOUT the token\n",
    "print(\"--- Connecting WITHOUT auth token ---\")\n",
    "server_params_no_auth = StdioServerParameters(\n",
    "    command=sys.executable,\n",
    "    args=[\"mcp_servers/weather_server.py\"],\n",
    "    env={\"PATH\": os.environ.get(\"PATH\", \"\")},  # No API_TOKEN\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params_no_auth) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "        result = await session.call_tool(\"get_forecast\", {\"city\": \"San Francisco\"})\n",
    "        data = json.loads(result.content[0].text)\n",
    "        print(f\"  Result: {data}\")\n",
    "        # Should show an auth error\n",
    "\n",
    "# Now connect WITH the token\n",
    "print(\"\\n--- Connecting WITH auth token ---\")\n",
    "server_params_auth = StdioServerParameters(\n",
    "    command=sys.executable,\n",
    "    args=[\"mcp_servers/weather_server.py\"],\n",
    "    env={\n",
    "        \"PATH\": os.environ.get(\"PATH\", \"\"),\n",
    "        \"API_TOKEN\": \"demo-weather-token-123\",\n",
    "    },\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params_auth) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "        result = await session.call_tool(\"get_forecast\", {\"city\": \"San Francisco\"})\n",
    "        data = json.loads(result.content[0].text)\n",
    "        print(f\"  Forecast: {json.dumps(data, indent=2)}\")\n",
    "\n",
    "# Pattern 2: Auth via HTTP headers (for remote servers)\n",
    "print(\"\\n--- HTTP transport auth pattern (configuration only) ---\")\n",
    "http_config = {\n",
    "    \"weather_remote\": {\n",
    "        \"url\": \"https://api.example.com/mcp\",\n",
    "        \"transport\": \"streamable_http\",\n",
    "        \"headers\": {\n",
    "            \"Authorization\": \"Bearer YOUR_API_KEY_HERE\",\n",
    "            \"X-Client-Id\": \"notebook-demo\",\n",
    "        },\n",
    "    }\n",
    "}\n",
    "print(f\"  Config: {json.dumps(http_config, indent=2)}\")\n",
    "print(\"  (Not connecting — this shows the configuration pattern for remote servers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building MCP Connectors\n",
    "\n",
    "This section covers building MCP servers from scratch — lifecycle management, exposing resources and tools, and handling errors robustly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connector Structure and Lifecycle\n",
    "\n",
    "A `FastMCP` server follows a clear lifecycle:\n",
    "1. **Creation** — `mcp = FastMCP(\"Name\")` with optional `lifespan` parameter\n",
    "2. **Registration** — `@mcp.tool()`, `@mcp.resource()`, `@mcp.prompt()` decorators register capabilities\n",
    "3. **Startup** — The `lifespan` async context manager runs, initializing shared state (database connections, caches, etc.)\n",
    "4. **Running** — Server processes requests; shared state is accessible via `mcp.get_context()`\n",
    "5. **Shutdown** — The `lifespan` context manager cleans up resources\n",
    "\n",
    "Let's review the inventory server code to see this structure in practice, then connect to it as a client to observe the lifecycle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the inventory server structure (written to disk in the Server Files section)\n",
    "with open(\"mcp_servers/inventory_server.py\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the inventory server and observe the lifecycle\n",
    "server_params = StdioServerParameters(command=\"python\", args=[\"mcp_servers/inventory_server.py\"])\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "        print(\"Connected to inventory server\\n\")\n",
    "\n",
    "        # Discover tools registered via @mcp.tool()\n",
    "        tools = await session.list_tools()\n",
    "        print(f\"Tools ({len(tools.tools)}):\")\n",
    "        for tool in tools.tools:\n",
    "            print(f\"  - {tool.name}: {tool.description}\")\n",
    "\n",
    "        # Discover resources registered via @mcp.resource()\n",
    "        resources = await session.list_resources()\n",
    "        print(f\"\\nResources ({len(resources.resources)}):\")\n",
    "        for r in resources.resources:\n",
    "            print(f\"  - {r.uri}\")\n",
    "\n",
    "        # Use tools — state is managed by the server process\n",
    "        result = await session.call_tool(\"search_products\", {\"query\": \"laptop\"})\n",
    "        print(f\"\\nSearch result: {result.content[0].text}\")\n",
    "\n",
    "        result = await session.call_tool(\"check_stock\", {\"product_id\": \"LAPTOP-001\"})\n",
    "        print(f\"Stock check: {result.content[0].text}\")\n",
    "\n",
    "        # Read a resource\n",
    "        result = await session.read_resource(\"inventory://categories\")\n",
    "        print(f\"Categories: {result.contents[0].text}\")\n",
    "\n",
    "# Exiting the context triggers shutdown (lifespan cleanup if configured)\n",
    "print(\"\\nServer shut down — session and subprocess terminated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Resource Access\n",
    "\n",
    "MCP resources expose read-only data through URI patterns:\n",
    "- **Static resources** have fixed URIs (e.g., `inventory://catalog`) — they appear in `list_resources()`\n",
    "- **Resource templates** have parameterized URIs (e.g., `inventory://product/{product_id}`) — they appear in `list_resource_templates()`\n",
    "\n",
    "Resources are defined with `@mcp.resource(\"uri://pattern\")` and return string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate resource access patterns using the inventory server\n",
    "server_params = StdioServerParameters(command=\"python\", args=[\"mcp_servers/inventory_server.py\"])\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        # List static resources\n",
    "        resources = await session.list_resources()\n",
    "        print(\"Static Resources:\")\n",
    "        for r in resources.resources:\n",
    "            print(f\"  {r.uri} — {r.name}\")\n",
    "\n",
    "        # List resource templates (parameterized)\n",
    "        templates = await session.list_resource_templates()\n",
    "        print(\"\\nResource Templates:\")\n",
    "        for t in templates.resource_templates:\n",
    "            print(f\"  {t.uri_template} — {t.name}\")\n",
    "\n",
    "        # Read a static resource\n",
    "        print(\"\\n--- Reading inventory://categories ---\")\n",
    "        result = await session.read_resource(\"inventory://categories\")\n",
    "        print(json.dumps(json.loads(result.contents[0].text), indent=2))\n",
    "\n",
    "        # Read a templated resource — product details\n",
    "        print(\"\\n--- Reading inventory://product/LAPTOP-001 ---\")\n",
    "        result = await session.read_resource(\"inventory://product/LAPTOP-001\")\n",
    "        print(json.dumps(json.loads(result.contents[0].text), indent=2))\n",
    "\n",
    "        print(\"\\n--- Reading inventory://product/HEADPH-001 ---\")\n",
    "        result = await session.read_resource(\"inventory://product/HEADPH-001\")\n",
    "        print(json.dumps(json.loads(result.contents[0].text), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Tool Exposure\n",
    "\n",
    "The `@mcp.tool()` decorator auto-generates a JSON Schema from the function's:\n",
    "- **Name** — the function name becomes the tool name\n",
    "- **Docstring** — becomes the tool's description (shown to LLMs)\n",
    "- **Type hints** — become the input schema (parameters, types, required/optional)\n",
    "\n",
    "Good tool design means clear names, typed parameters, and descriptive docstrings — because the LLM uses all of this to decide *when* and *how* to call each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect tool schemas and exercise each tool\n",
    "server_params = StdioServerParameters(command=\"python\", args=[\"mcp_servers/inventory_server.py\"])\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        # Inspect the auto-generated schemas\n",
    "        tools = await session.list_tools()\n",
    "        print(\"Tool Schemas (auto-generated from type hints):\")\n",
    "        for tool in tools.tools:\n",
    "            print(f\"\\n  {tool.name}:\")\n",
    "            print(f\"    Description: {tool.description}\")\n",
    "            print(f\"    Input Schema: {json.dumps(tool.inputSchema, indent=6)}\")\n",
    "\n",
    "        # Exercise: search with just a query\n",
    "        print(\"\\n--- search_products(query='monitor') ---\")\n",
    "        r = await session.call_tool(\"search_products\", {\"query\": \"monitor\"})\n",
    "        print(json.dumps(json.loads(r.content[0].text), indent=2))\n",
    "\n",
    "        # Exercise: search with query + category filter\n",
    "        print(\"\\n--- search_products(query='wireless', category='audio') ---\")\n",
    "        r = await session.call_tool(\"search_products\", {\"query\": \"wireless\", \"category\": \"audio\"})\n",
    "        print(json.dumps(json.loads(r.content[0].text), indent=2))\n",
    "\n",
    "        # Exercise: check_stock\n",
    "        print(\"\\n--- check_stock(product_id='MONITOR-002') ---\")\n",
    "        r = await session.call_tool(\"check_stock\", {\"product_id\": \"MONITOR-002\"})\n",
    "        print(json.dumps(json.loads(r.content[0].text), indent=2))\n",
    "\n",
    "        # Exercise: get_price\n",
    "        print(\"\\n--- get_price(product_id='KEYBOARD-001') ---\")\n",
    "        r = await session.call_tool(\"get_price\", {\"product_id\": \"KEYBOARD-001\"})\n",
    "        print(json.dumps(json.loads(r.content[0].text), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling and Retries\n",
    "\n",
    "MCP tool calls can fail in several ways:\n",
    "- **Server-side validation errors** — The tool runs but returns an error message (e.g., \"product not found\")\n",
    "- **Tool not found** — The client requests a tool that doesn't exist\n",
    "- **Transport errors** — The server process crashes or times out\n",
    "\n",
    "A retry wrapper with exponential backoff handles transient failures gracefully. The `result.isError` flag indicates whether the tool returned an error response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def mcp_call_with_retry(session, tool_name, arguments, max_retries=3, base_delay=0.5):\n",
    "    \"\"\"Call an MCP tool with exponential backoff retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = await session.call_tool(tool_name, arguments)\n",
    "            # Check for MCP-level errors\n",
    "            if result.isError:\n",
    "                print(f\"  [Attempt {attempt + 1}] Tool returned error: {result.content[0].text}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    delay = base_delay * (2 ** attempt)\n",
    "                    print(f\"  Retrying in {delay}s...\")\n",
    "                    await asyncio.sleep(delay)\n",
    "                    continue\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"  [Attempt {attempt + 1}] Exception: {type(e).__name__}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                delay = base_delay * (2 ** attempt)\n",
    "                print(f\"  Retrying in {delay}s...\")\n",
    "                await asyncio.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "    return None\n",
    "\n",
    "\n",
    "# Test error handling scenarios\n",
    "server_params = StdioServerParameters(command=\"python\", args=[\"mcp_servers/inventory_server.py\"])\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        # Case 1: Valid call (baseline)\n",
    "        print(\"--- Case 1: Valid call ---\")\n",
    "        result = await mcp_call_with_retry(session, \"check_stock\", {\"product_id\": \"LAPTOP-001\"})\n",
    "        print(f\"  Result: {result.content[0].text}\")\n",
    "\n",
    "        # Case 2: Invalid product ID (server returns error JSON, not a protocol error)\n",
    "        print(\"\\n--- Case 2: Invalid product ID ---\")\n",
    "        result = await mcp_call_with_retry(session, \"check_stock\", {\"product_id\": \"NONEXISTENT-999\"})\n",
    "        print(f\"  Result: {result.content[0].text}\")\n",
    "\n",
    "        # Case 3: Nonexistent tool (protocol-level error)\n",
    "        print(\"\\n--- Case 3: Nonexistent tool ---\")\n",
    "        try:\n",
    "            result = await mcp_call_with_retry(session, \"delete_everything\", {}, max_retries=1)\n",
    "        except Exception as e:\n",
    "            print(f\"  Caught expected error: {type(e).__name__}: {e}\")\n",
    "\n",
    "        print(\"\\nError handling demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Patterns\n",
    "\n",
    "This section covers common integration patterns — connecting MCP to file systems, databases, external APIs, creating composite tools, and composing multiple servers into a single agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File System Access\n",
    "\n",
    "A file system MCP server lets an AI agent browse and read local files. This is useful for document processing, code analysis, or any workflow that needs access to local data.\n",
    "\n",
    "**Security is critical**: the server validates all paths to prevent directory traversal attacks (`../../../etc/passwd`). Every path is resolved and checked against the allowed root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the filesystem server, pointed at the documents directory\n",
    "docs_path = os.path.abspath(\"documents\")\n",
    "if not os.path.isdir(docs_path):\n",
    "    print(f\"Warning: '{docs_path}' not found. Using current directory instead.\")\n",
    "    docs_path = os.getcwd()\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_servers/filesystem_server.py\", docs_path],\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        # List the root directory\n",
    "        print(\"--- List root directory ---\")\n",
    "        result = await session.call_tool(\"list_directory\", {\"path\": \".\"})\n",
    "        entries = json.loads(result.content[0].text)\n",
    "        for entry in entries.get(\"entries\", [])[:10]:\n",
    "            icon = \"📁\" if entry[\"type\"] == \"dir\" else \"📄\"\n",
    "            size = f\" ({entry['size']} bytes)\" if entry.get(\"size\") else \"\"\n",
    "            print(f\"  {icon} {entry['name']}{size}\")\n",
    "        if len(entries.get(\"entries\", [])) > 10:\n",
    "            print(f\"  ... and {len(entries['entries']) - 10} more\")\n",
    "\n",
    "        # Search for markdown files\n",
    "        print(\"\\n--- Search for .md files ---\")\n",
    "        result = await session.call_tool(\"search_files\", {\"directory\": \".\", \"pattern\": \"*.md\"})\n",
    "        files = json.loads(result.content[0].text)\n",
    "        for f in files.get(\"matches\", [])[:5]:\n",
    "            print(f\"  {f}\")\n",
    "\n",
    "        # Read a file\n",
    "        if files.get(\"matches\"):\n",
    "            first_file = files[\"matches\"][0]\n",
    "            print(f\"\\n--- Reading {first_file} ---\")\n",
    "            result = await session.call_tool(\"read_file\", {\"path\": first_file})\n",
    "            data = json.loads(result.content[0].text)\n",
    "            content = data.get(\"content\", \"\")\n",
    "            print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "\n",
    "        # Read directory summary resource\n",
    "        print(\"\\n--- Directory summary ---\")\n",
    "        summary = await session.read_resource(\"file://summary\")\n",
    "        print(summary.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Connections\n",
    "\n",
    "The database server demonstrates a production-ready pattern:\n",
    "- The **lifespan** function manages the database connection (create on startup, close on shutdown)\n",
    "- **Tools** expose safe query operations — no raw SQL from the client\n",
    "- **Resources** expose schema metadata so clients understand the data model\n",
    "\n",
    "Here we use SQLite, but the same pattern works for PostgreSQL, MySQL, or any database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database server\n",
    "server_params = StdioServerParameters(command=\"python\", args=[\"mcp_servers/database_server.py\"])\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        # Read the database schema\n",
    "        print(\"--- Database Schema ---\")\n",
    "        schema = await session.read_resource(\"db://schema\")\n",
    "        print(schema.contents[0].text)\n",
    "\n",
    "        # Query orders for a customer\n",
    "        print(\"\\n--- Orders for 'Alice Johnson' ---\")\n",
    "        result = await session.call_tool(\"query_orders\", {\"customer_name\": \"Alice Johnson\"})\n",
    "        print(json.dumps(json.loads(result.content[0].text), indent=2))\n",
    "\n",
    "        # Check a specific order status\n",
    "        print(\"\\n--- Order status for ORD-003 ---\")\n",
    "        result = await session.call_tool(\"get_order_status\", {\"order_id\": \"ORD-003\"})\n",
    "        print(json.dumps(json.loads(result.content[0].text), indent=2))\n",
    "\n",
    "        # Get the sales summary\n",
    "        print(\"\\n--- Sales Summary ---\")\n",
    "        result = await session.call_tool(\"get_sales_summary\", {})\n",
    "        print(json.dumps(json.loads(result.content[0].text), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Integrations (REST, GraphQL)\n",
    "\n",
    "Wrapping an external REST API as an MCP server creates a clean abstraction: the AI agent calls MCP tools, and the server handles the HTTP requests, authentication, rate limiting, and error handling.\n",
    "\n",
    "Our GitHub API server wraps GitHub's public REST API using only `urllib` (no extra dependencies). It demonstrates key considerations for API integrations: timeout handling, HTTP error codes, and response shaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the GitHub API server (makes live API calls)\n",
    "server_params = StdioServerParameters(command=\"python\", args=[\"mcp_servers/github_api_server.py\"])\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        # Search for repos\n",
    "        print(\"--- Search repos: 'langchain python' ---\")\n",
    "        result = await session.call_tool(\"search_repos\", {\"query\": \"langchain python\"})\n",
    "        repos = json.loads(result.content[0].text)\n",
    "        for repo in repos.get(\"items\", [])[:3]:\n",
    "            print(f\"  {repo['full_name']} ({repo['stargazers_count']} stars) — {repo.get('language', 'N/A')}\")\n",
    "\n",
    "        # Get repo info\n",
    "        print(\"\\n--- Repo info: langchain-ai/langchain ---\")\n",
    "        result = await session.call_tool(\"get_repo_info\", {\"owner\": \"langchain-ai\", \"repo\": \"langchain\"})\n",
    "        info = json.loads(result.content[0].text)\n",
    "        print(f\"  Stars: {info.get('stargazers_count')}\")\n",
    "        print(f\"  Language: {info.get('language')}\")\n",
    "        print(f\"  Forks: {info.get('forks_count')}\")\n",
    "        print(f\"  Description: {(info.get('description') or '')[:100]}\")\n",
    "\n",
    "        # List recent issues\n",
    "        print(\"\\n--- Recent open issues: modelcontextprotocol/python-sdk ---\")\n",
    "        result = await session.call_tool(\n",
    "            \"list_issues\", {\"owner\": \"modelcontextprotocol\", \"repo\": \"python-sdk\", \"state\": \"open\"}\n",
    "        )\n",
    "        issues = json.loads(result.content[0].text)\n",
    "        if isinstance(issues, list):\n",
    "            for issue in issues[:3]:\n",
    "                labels = \", \".join(issue.get(\"labels\", [])) or \"no labels\"\n",
    "                print(f\"  #{issue['number']}: {issue['title'][:60]} [{labels}]\")\n",
    "        else:\n",
    "            print(f\"  {issues}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tool Creation\n",
    "\n",
    "MCP tools become especially powerful when composed by an LLM agent. Instead of a human deciding which tools to call in what order, the agent reasons about the user's intent and orchestrates multiple tool calls automatically.\n",
    "\n",
    "Here we load the inventory server's tools into a LangChain agent and give it a *comparison* task — the agent must call multiple tools and synthesize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Load inventory tools via LangChain adapter\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"inventory\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"mcp_servers/inventory_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        }\n",
    "    }\n",
    ") as client:\n",
    "    tools = client.get_tools()\n",
    "\n",
    "    # Create an agent specialized for product comparison\n",
    "    agent = create_agent(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=tools,\n",
    "        prompt=(\n",
    "            \"You are a product comparison assistant. When a customer asks to compare products:\\n\"\n",
    "            \"1. Search for relevant products\\n\"\n",
    "            \"2. Get prices for each\\n\"\n",
    "            \"3. Check stock availability\\n\"\n",
    "            \"4. Provide a structured comparison with a clear recommendation\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    response = await agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"Compare your two laptop options. Which is better for someone on a budget?\"}]}\n",
    "    )\n",
    "\n",
    "    # Show tool calls and final response\n",
    "    for msg in response[\"messages\"]:\n",
    "        if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "            for tc in msg.tool_calls:\n",
    "                print(f\"[Tool Call] {tc['name']}({tc['args']})\")\n",
    "        if hasattr(msg, \"type\") and msg.type == \"ai\" and msg.content and not getattr(msg, \"tool_calls\", []):\n",
    "            print(f\"\\nComparison:\\n{msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing Multiple MCP Servers\n",
    "\n",
    "`MultiServerMCPClient` can connect to **any number** of MCP servers simultaneously, loading all their tools into a single pool. The LangChain agent then picks the right tools for each query.\n",
    "\n",
    "This is MCP's key value proposition: **uniform access to diverse data sources**. Add a new data source by configuring one more MCP server — no changes to the agent code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Connect to multiple MCP servers simultaneously\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"inventory\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"mcp_servers/inventory_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"github\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"mcp_servers/github_api_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "    }\n",
    ") as multi_client:\n",
    "    all_tools = multi_client.get_tools()\n",
    "\n",
    "    print(f\"Total tools loaded from all servers: {len(all_tools)}\")\n",
    "    for tool in all_tools:\n",
    "        print(f\"  - {tool.name}: {tool.description[:60]}...\")\n",
    "\n",
    "    # Create a multi-source agent\n",
    "    agent = create_agent(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=all_tools,\n",
    "        prompt=(\n",
    "            \"You are a helpful assistant with access to a product inventory system \"\n",
    "            \"and GitHub. Use the appropriate tools based on the user's question.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Test 1: Product question (should use inventory tools)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Question about products:\")\n",
    "    print(\"=\" * 50)\n",
    "    r1 = await agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"What monitors do you have in stock and how much do they cost?\"}]}\n",
    "    )\n",
    "    for msg in r1[\"messages\"]:\n",
    "        if hasattr(msg, \"type\") and msg.type == \"ai\" and msg.content and not getattr(msg, \"tool_calls\", []):\n",
    "            print(msg.content)\n",
    "\n",
    "    # Test 2: GitHub question (should use GitHub tools)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Question about GitHub:\")\n",
    "    print(\"=\" * 50)\n",
    "    r2 = await agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"How many stars does the modelcontextprotocol/python-sdk repo have?\"}]}\n",
    "    )\n",
    "    for msg in r2[\"messages\"]:\n",
    "        if hasattr(msg, \"type\") and msg.type == \"ai\" and msg.content and not getattr(msg, \"tool_calls\", []):\n",
    "            print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing MCP Connectors\n",
    "\n",
    "This section covers strategies for testing MCP servers and client integrations — from local protocol-level tests to full end-to-end integration tests to mock servers for development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Testing Strategies\n",
    "\n",
    "The most reliable way to test an MCP server is to connect to it as a real client and verify every tool and resource. This tests the full protocol round-trip — serialization, transport, execution, and response parsing.\n",
    "\n",
    "Each test case follows the pattern: call a tool, parse the result, assert on the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_inventory_server():\n",
    "    \"\"\"Test suite for the inventory MCP server.\"\"\"\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "\n",
    "    server_params = StdioServerParameters(command=\"python\", args=[\"mcp_servers/inventory_server.py\"])\n",
    "\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "\n",
    "            # Test 1: Tool discovery\n",
    "            try:\n",
    "                tools = await session.list_tools()\n",
    "                tool_names = [t.name for t in tools.tools]\n",
    "                assert \"search_products\" in tool_names\n",
    "                assert \"check_stock\" in tool_names\n",
    "                assert \"get_price\" in tool_names\n",
    "                passed += 1\n",
    "                print(\"[PASS] Tool discovery — found all 3 expected tools\")\n",
    "            except AssertionError as e:\n",
    "                failed += 1\n",
    "                print(f\"[FAIL] Tool discovery: {e}\")\n",
    "\n",
    "            # Test 2: Search returns results for known products\n",
    "            try:\n",
    "                r = await session.call_tool(\"search_products\", {\"query\": \"laptop\"})\n",
    "                data = json.loads(r.content[0].text)\n",
    "                assert len(data[\"results\"]) >= 1, \"Expected at least one laptop result\"\n",
    "                assert any(\"laptop\" in p[\"name\"].lower() or \"laptop\" in p[\"description\"].lower()\n",
    "                           for p in data[\"results\"]), \"Results should mention 'laptop'\"\n",
    "                passed += 1\n",
    "                print(f\"[PASS] Search products — returned {len(data['results'])} laptop(s)\")\n",
    "            except (AssertionError, Exception) as e:\n",
    "                failed += 1\n",
    "                print(f\"[FAIL] Search products: {e}\")\n",
    "\n",
    "            # Test 3: Stock data is valid\n",
    "            try:\n",
    "                r = await session.call_tool(\"check_stock\", {\"product_id\": \"LAPTOP-001\"})\n",
    "                data = json.loads(r.content[0].text)\n",
    "                assert \"stock\" in data, \"Response must contain 'stock' field\"\n",
    "                assert isinstance(data[\"stock\"], int), \"Stock must be an integer\"\n",
    "                assert data[\"stock\"] > 0, \"LAPTOP-001 should be in stock\"\n",
    "                passed += 1\n",
    "                print(f\"[PASS] Check stock — LAPTOP-001 has {data['stock']} units\")\n",
    "            except (AssertionError, Exception) as e:\n",
    "                failed += 1\n",
    "                print(f\"[FAIL] Check stock: {e}\")\n",
    "\n",
    "            # Test 4: Resources are readable\n",
    "            try:\n",
    "                result = await session.read_resource(\"inventory://categories\")\n",
    "                cats = json.loads(result.contents[0].text)\n",
    "                assert \"categories\" in cats, \"Must contain 'categories' key\"\n",
    "                assert len(cats[\"categories\"]) >= 2, \"Must have at least 2 categories\"\n",
    "                passed += 1\n",
    "                print(f\"[PASS] Resource access — {len(cats['categories'])} categories\")\n",
    "            except (AssertionError, Exception) as e:\n",
    "                failed += 1\n",
    "                print(f\"[FAIL] Resource access: {e}\")\n",
    "\n",
    "            # Test 5: Invalid product returns error gracefully\n",
    "            try:\n",
    "                r = await session.call_tool(\"check_stock\", {\"product_id\": \"INVALID-999\"})\n",
    "                data = json.loads(r.content[0].text)\n",
    "                assert \"error\" in data, \"Invalid product should return error\"\n",
    "                passed += 1\n",
    "                print(\"[PASS] Error handling — invalid product returns error message\")\n",
    "            except (AssertionError, Exception) as e:\n",
    "                failed += 1\n",
    "                print(f\"[FAIL] Error handling: {e}\")\n",
    "\n",
    "    print(f\"\\nResults: {passed} passed, {failed} failed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "# Run the tests\n",
    "await test_inventory_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration Testing\n",
    "\n",
    "Integration testing verifies the full chain: **user question → LangChain agent → MCP tool call → server execution → response**. This catches issues that protocol-level tests miss — like the LLM failing to select the right tool, or the agent misinterpreting tool output.\n",
    "\n",
    "We verify both that the agent *used* tools and that the final response contains expected information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "async def test_agent_integration():\n",
    "    \"\"\"Integration test: LLM agent + MCP server end-to-end.\"\"\"\n",
    "    print(\"--- Integration Test: Agent + Inventory Server ---\\n\")\n",
    "\n",
    "    async with MultiServerMCPClient(\n",
    "        {\n",
    "            \"inventory\": {\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"mcp_servers/inventory_server.py\"],\n",
    "                \"transport\": \"stdio\",\n",
    "            }\n",
    "        }\n",
    "    ) as client:\n",
    "        tools = client.get_tools()\n",
    "\n",
    "        agent = create_agent(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            tools=tools,\n",
    "            prompt=\"You are a store assistant. Use tools to answer product questions accurately.\",\n",
    "        )\n",
    "\n",
    "        response = await agent.ainvoke(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": \"What headphones do you sell and how much do they cost?\"}]}\n",
    "        )\n",
    "\n",
    "        # Check: did the agent use tools?\n",
    "        tool_calls_made = [\n",
    "            msg for msg in response[\"messages\"]\n",
    "            if hasattr(msg, \"tool_calls\") and msg.tool_calls\n",
    "        ]\n",
    "        print(f\"Tool calls made: {len(tool_calls_made)}\")\n",
    "        for msg in tool_calls_made:\n",
    "            for tc in msg.tool_calls:\n",
    "                print(f\"  Called: {tc['name']}({tc['args']})\")\n",
    "\n",
    "        # Extract the final answer\n",
    "        final_answer = \"\"\n",
    "        for msg in response[\"messages\"]:\n",
    "            if hasattr(msg, \"type\") and msg.type == \"ai\" and msg.content and not getattr(msg, \"tool_calls\", []):\n",
    "                final_answer = msg.content\n",
    "\n",
    "        print(f\"\\nAgent response:\\n{final_answer[:400]}...\\n\" if len(final_answer) > 400 else f\"\\nAgent response:\\n{final_answer}\\n\")\n",
    "\n",
    "        # Verify the response\n",
    "        checks = {\n",
    "            \"Agent used tools\": len(tool_calls_made) > 0,\n",
    "            \"Mentions headphones\": any(w in final_answer.lower() for w in [\"headphone\", \"studiopro\", \"budfit\", \"earbud\"]),\n",
    "            \"Mentions a price\": any(p in final_answer for p in [\"$\", \"349\", \"129\", \"price\"]),\n",
    "        }\n",
    "\n",
    "        print(\"Integration checks:\")\n",
    "        all_passed = True\n",
    "        for check_name, check_passed in checks.items():\n",
    "            status = \"PASS\" if check_passed else \"FAIL\"\n",
    "            print(f\"  [{status}] {check_name}\")\n",
    "            if not check_passed:\n",
    "                all_passed = False\n",
    "\n",
    "        return all_passed\n",
    "\n",
    "\n",
    "success = await test_agent_integration()\n",
    "print(f\"\\nOverall: {'PASS' if success else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock Servers for Development\n",
    "\n",
    "When developing a client against an MCP server that calls expensive external APIs (or that hasn't been built yet), create a **mock server** with the same tool interface but hardcoded responses.\n",
    "\n",
    "This allows rapid client development and testing:\n",
    "- No network calls (fast, free, offline)\n",
    "- Predictable responses (easy to assert on)\n",
    "- Same interface (swap mock for real with a config change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_servers/mock_github_server.py\n",
    "\"\"\"Mock GitHub API server — returns canned data for development and testing.\"\"\"\n",
    "import json\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"MockGitHub\")\n",
    "\n",
    "MOCK_REPO = {\n",
    "    \"full_name\": \"mock-org/mock-repo\",\n",
    "    \"description\": \"A mock repository for testing\",\n",
    "    \"stargazers_count\": 1234,\n",
    "    \"language\": \"Python\",\n",
    "    \"open_issues_count\": 5,\n",
    "    \"forks_count\": 42,\n",
    "    \"created_at\": \"2024-01-01T00:00:00Z\",\n",
    "    \"updated_at\": \"2025-06-15T12:00:00Z\",\n",
    "}\n",
    "\n",
    "MOCK_ISSUES = [\n",
    "    {\"number\": 1, \"title\": \"Mock issue one\", \"state\": \"open\", \"created_at\": \"2025-06-01\", \"labels\": [\"bug\"]},\n",
    "    {\"number\": 2, \"title\": \"Mock issue two\", \"state\": \"open\", \"created_at\": \"2025-06-10\", \"labels\": [\"feature\"]},\n",
    "    {\"number\": 3, \"title\": \"Mock issue three\", \"state\": \"closed\", \"created_at\": \"2025-05-20\", \"labels\": []},\n",
    "]\n",
    "\n",
    "MOCK_SEARCH = [\n",
    "    {\"full_name\": \"mock-org/result-1\", \"description\": \"First search result\", \"stargazers_count\": 500, \"language\": \"Python\"},\n",
    "    {\"full_name\": \"mock-org/result-2\", \"description\": \"Second search result\", \"stargazers_count\": 200, \"language\": \"TypeScript\"},\n",
    "]\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_repo_info(owner: str, repo: str) -> str:\n",
    "    \"\"\"Get metadata about a GitHub repository (stars, language, description, etc.).\"\"\"\n",
    "    return json.dumps({**MOCK_REPO, \"full_name\": f\"{owner}/{repo}\"})\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def list_issues(owner: str, repo: str, state: str = \"open\") -> str:\n",
    "    \"\"\"List issues for a GitHub repository. State can be 'open', 'closed', or 'all'.\"\"\"\n",
    "    filtered = [i for i in MOCK_ISSUES if state == \"all\" or i[\"state\"] == state]\n",
    "    return json.dumps(filtered)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def search_repos(query: str) -> str:\n",
    "    \"\"\"Search GitHub repositories by keyword. Returns top 5 results.\"\"\"\n",
    "    return json.dumps({\"items\": MOCK_SEARCH, \"total_count\": len(MOCK_SEARCH)})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the mock and real GitHub servers\n",
    "async def run_against_server(label: str, server_args: list[str]):\n",
    "    \"\"\"Run the same client code against any server with the GitHub tool interface.\"\"\"\n",
    "    server_params = StdioServerParameters(command=\"python\", args=server_args)\n",
    "\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "\n",
    "            tools = await session.list_tools()\n",
    "            tool_names = sorted([t.name for t in tools.tools])\n",
    "            print(f\"  Tools: {tool_names}\")\n",
    "\n",
    "            result = await session.call_tool(\"get_repo_info\", {\"owner\": \"langchain-ai\", \"repo\": \"langchain\"})\n",
    "            data = json.loads(result.content[0].text)\n",
    "            print(f\"  Repo: {data.get('full_name')} ({data.get('stargazers_count')} stars)\")\n",
    "\n",
    "            result = await session.call_tool(\"search_repos\", {\"query\": \"mcp python\"})\n",
    "            search = json.loads(result.content[0].text)\n",
    "            print(f\"  Search results: {search.get('total_count', len(search.get('items', [])))} repos found\")\n",
    "\n",
    "\n",
    "# Mock server — instant, offline, predictable\n",
    "print(\"--- Mock GitHub Server ---\")\n",
    "await run_against_server(\"mock\", [\"mcp_servers/mock_github_server.py\"])\n",
    "\n",
    "# Real server — live API calls (may be rate-limited)\n",
    "print(\"\\n--- Real GitHub Server ---\")\n",
    "await run_against_server(\"real\", [\"mcp_servers/github_api_server.py\"])\n",
    "\n",
    "print(\"\\nBoth servers expose identical tool interfaces.\")\n",
    "print(\"Use mock during development, swap to real for integration testing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-bootcamp-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
