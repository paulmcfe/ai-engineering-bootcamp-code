{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Memory\n",
    "\n",
    "This notebook implements the memory systems from Chapter 6 — persistent stores, memory tools, formation strategies, integration patterns, and caching. Both the chapter and this notebook use SQLite in-memory so every cell runs without external dependencies. PostgreSQL setup for production is covered as a reference section.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before running this notebook, make sure you have:\n",
    "- An OpenAI API key set as the `OPENAI_API_KEY` environment variable\n",
    "- The required packages installed: `uv sync`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, UTC\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import (\n",
    "    Column, String, Integer, DateTime, JSON, Index,\n",
    "    create_engine, cast\n",
    ")\n",
    "from sqlalchemy.orm import Session, sessionmaker, declarative_base\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database and Memory Model\n",
    "\n",
    "Before building our memory store, we need the database infrastructure. We use SQLite in-memory here for portability — no external services required. The `Memory` model uses a namespace/key/value structure with a composite index for efficient lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "class Memory(Base):\n",
    "    \"\"\"User memory storage organized by namespace.\"\"\"\n",
    "    __tablename__ = \"memories\"\n",
    "\n",
    "    id = Column(String(36), primary_key=True)\n",
    "    user_id = Column(String(50), nullable=False, index=True)\n",
    "    namespace = Column(String(100), nullable=False, index=True)\n",
    "    key = Column(String(200), nullable=False)\n",
    "    value = Column(JSON, nullable=False)\n",
    "    created_at = Column(DateTime, default=lambda: datetime.now(UTC))\n",
    "    updated_at = Column(DateTime, default=lambda: datetime.now(UTC), onupdate=lambda: datetime.now(UTC))\n",
    "\n",
    "    __table_args__ = (Index(\"ix_memory_user_namespace\", \"user_id\", \"namespace\"),)\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "db_session = SessionLocal()\n",
    "current_user_id = \"student_01\"\n",
    "\n",
    "print(\"Database and Memory model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Memory Store Pattern\n",
    "\n",
    "The core of any memory system is a store abstraction that handles put, get, and search. This lightweight class wraps SQLAlchemy queries using a namespace/key/value pattern — no external memory libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryStore:\n",
    "    \"\"\"Simple memory store backed by SQLAlchemy.\"\"\"\n",
    "\n",
    "    def __init__(self, db: Session):\n",
    "        self.db = db\n",
    "\n",
    "    def put(self, user_id: str, namespace: str, key: str, value: dict) -> Memory:\n",
    "        \"\"\"Store or update a memory.\"\"\"\n",
    "        existing = (\n",
    "            self.db.query(Memory)\n",
    "            .filter_by(user_id=user_id, namespace=namespace, key=key)\n",
    "            .first()\n",
    "        )\n",
    "\n",
    "        if existing:\n",
    "            existing.value = value\n",
    "            existing.updated_at = datetime.now(UTC)\n",
    "            self.db.commit()\n",
    "            return existing\n",
    "\n",
    "        memory = Memory(\n",
    "            id=str(uuid.uuid4()),\n",
    "            user_id=user_id,\n",
    "            namespace=namespace,\n",
    "            key=key,\n",
    "            value=value,\n",
    "        )\n",
    "        self.db.add(memory)\n",
    "        self.db.commit()\n",
    "        return memory\n",
    "\n",
    "    def get(self, user_id: str, namespace: str, key: str) -> dict | None:\n",
    "        \"\"\"Get a specific memory by key.\"\"\"\n",
    "        memory = (\n",
    "            self.db.query(Memory)\n",
    "            .filter_by(user_id=user_id, namespace=namespace, key=key)\n",
    "            .first()\n",
    "        )\n",
    "        return memory.value if memory else None\n",
    "\n",
    "    def search(self, user_id: str, namespace: str | None = None, limit: int = 10) -> list[dict]:\n",
    "        \"\"\"Search memories, ordered by recency.\"\"\"\n",
    "        query = self.db.query(Memory).filter_by(user_id=user_id)\n",
    "\n",
    "        if namespace:\n",
    "            query = query.filter_by(namespace=namespace)\n",
    "\n",
    "        query = query.order_by(Memory.updated_at.desc()).limit(limit)\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"namespace\": m.namespace,\n",
    "                \"key\": m.key,\n",
    "                \"value\": m.value,\n",
    "                \"updated_at\": m.updated_at.isoformat() if m.updated_at else None,\n",
    "            }\n",
    "            for m in query.all()\n",
    "        ]\n",
    "\n",
    "store = MemoryStore(db_session)\n",
    "print(\"MemoryStore ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giving Agents Memory Tools\n",
    "\n",
    "Agent memory tools wrap store operations as LangChain tools so the agent can decide when to remember something and when to recall past context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def remember(namespace: str, key: str, fact: str) -> str:\n",
    "    \"\"\"Store an important fact about the user or conversation.\n",
    "\n",
    "    Args:\n",
    "        namespace: Category like 'preferences', 'goals', or 'struggles'\n",
    "        key: Identifier for this specific memory\n",
    "        fact: The information to remember\n",
    "    \"\"\"\n",
    "    store.put(\n",
    "        user_id=current_user_id,\n",
    "        namespace=namespace,\n",
    "        key=key,\n",
    "        value={\"content\": fact, \"timestamp\": datetime.now(UTC).isoformat()}\n",
    "    )\n",
    "    return f\"I'll remember that: {fact}\"\n",
    "\n",
    "@tool\n",
    "def recall(namespace: str) -> str:\n",
    "    \"\"\"Recall memories from a specific category.\n",
    "\n",
    "    Args:\n",
    "        namespace: Category to search, like 'preferences' or 'goals'\n",
    "    \"\"\"\n",
    "    memories = store.search(current_user_id, namespace=namespace, limit=5)\n",
    "\n",
    "    if not memories:\n",
    "        return f\"No memories found in {namespace}.\"\n",
    "\n",
    "    return \"\\n\".join([\n",
    "        f\"- {m['key']}: {m['value'].get('content', m['value'])}\"\n",
    "        for m in memories\n",
    "    ])\n",
    "\n",
    "print(\"Memory tools defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Namespace Organization\n",
    "\n",
    "Namespaces categorize memories into logical groups for efficient, scoped retrieval. Within each user's memories, namespaces separate preferences from facts, goals from session summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common namespace conventions\n",
    "\"preferences\"   # Learning style, communication preferences, settings\n",
    "\"facts\"         # Known facts about the user (profession, background)\n",
    "\"goals\"         # What they're trying to achieve\n",
    "\"struggles\"     # Topics or concepts they find difficult\n",
    "\"sessions\"      # Summaries of past interactions\n",
    "\n",
    "print(\"Namespace conventions shown above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Search\n",
    "\n",
    "For more sophisticated retrieval, you can search by content within the JSON value using text matching. This approach works well for keyword-based recall without requiring vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_content(self, user_id: str, search_term: str, limit: int = 10) -> list[dict]:\n",
    "    \"\"\"Search memories by content within the JSON value.\"\"\"\n",
    "    query = (\n",
    "        self.db.query(Memory)\n",
    "        .filter_by(user_id=user_id)\n",
    "        .filter(cast(Memory.value, String).ilike(f\"%{search_term}%\"))\n",
    "        .order_by(Memory.updated_at.desc())\n",
    "        .limit(limit)\n",
    "    )\n",
    "\n",
    "    return [{\"key\": m.key, \"value\": m.value} for m in query.all()]\n",
    "\n",
    "MemoryStore.search_by_content = search_by_content\n",
    "print(\"search_by_content method added to MemoryStore!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Memory: Facts and Knowledge\n",
    "\n",
    "Semantic memory stores stable facts about the user — their profession, preferences, and background. These drive personalization and tend to remain relevant over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of semantic memory\n",
    "store.put(\n",
    "    user_id=current_user_id,\n",
    "    namespace=\"facts\",\n",
    "    key=\"profession\",\n",
    "    value={\"content\": \"Senior software engineer at a healthcare company\"}\n",
    ")\n",
    "\n",
    "store.put(\n",
    "    user_id=current_user_id,\n",
    "    namespace=\"preferences\",\n",
    "    key=\"explanation_style\",\n",
    "    value={\"content\": \"Prefers code examples over abstract descriptions\"}\n",
    ")\n",
    "\n",
    "print(\"Semantic memories stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic Memory: Past Experiences\n",
    "\n",
    "Episodic memory records specific interactions with timestamps and context. This enables learning from experience — if an explanation didn't land last time, try a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"session_001\"\n",
    "\n",
    "# Examples of episodic memory\n",
    "store.put(\n",
    "    user_id=current_user_id,\n",
    "    namespace=\"episodes\",\n",
    "    key=f\"session_{session_id}\",\n",
    "    value={\n",
    "        \"content\": \"Discussed RAG implementation patterns\",\n",
    "        \"timestamp\": datetime.now(UTC).isoformat(),\n",
    "        \"outcome\": \"User understood chunking but needed more help with retrieval\",\n",
    "        \"topics\": [\"RAG\", \"chunking\", \"vector search\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Episodic memory stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot Path: Active Memory Formation\n",
    "\n",
    "In the hot path approach, memory formation happens during the conversation. The agent decides in real-time that something is worth remembering and stores it immediately via a tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def remember_hotpath(fact: str) -> str:\n",
    "    \"\"\"\n",
    "    Store an important fact about the user or conversation.\n",
    "    Use when the user shares preferences, goals, or important context.\n",
    "    \"\"\"\n",
    "    store.put(\n",
    "        user_id=current_user_id,\n",
    "        namespace=\"facts\",\n",
    "        key=str(uuid.uuid4()),\n",
    "        value={\n",
    "            \"content\": fact,\n",
    "            \"type\": \"semantic\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    )\n",
    "    return f\"I'll remember that: {fact}\"\n",
    "\n",
    "print(\"Hot-path remember tool defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Passive Memory Extraction\n",
    "\n",
    "After a conversation ends, a separate process analyzes the transcript and extracts memories. This adds zero latency to conversations and can analyze the full conversation holistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_extraction(extraction) -> list[dict]:\n",
    "    \"\"\"Parse LLM extraction into memory dicts.\"\"\"\n",
    "    try:\n",
    "        return json.loads(extraction.content)\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        return []\n",
    "\n",
    "def extract_memories_background(conversation: list[dict]):\n",
    "    \"\"\"Run after conversation ends to extract memories.\"\"\"\n",
    "\n",
    "    prompt = \"\"\"Analyze this conversation and extract:\n",
    "    1. User preferences mentioned\n",
    "    2. Important facts shared\n",
    "    3. Topics discussed\n",
    "    4. Any commitments or follow-ups needed\n",
    "\n",
    "    Return as a JSON array of objects with \"type\" and \"content\" fields.\n",
    "\n",
    "    Conversation:\n",
    "    {conversation}\n",
    "    \"\"\"\n",
    "\n",
    "    extraction = llm.invoke(prompt.format(conversation=json.dumps(conversation)))\n",
    "    memories = parse_extraction(extraction)\n",
    "\n",
    "    for memory in memories:\n",
    "        store.put(\n",
    "            user_id=current_user_id,\n",
    "            namespace=memory.get(\"type\", \"facts\"),\n",
    "            key=str(uuid.uuid4()),\n",
    "            value=memory\n",
    "        )\n",
    "\n",
    "print(\"Background extraction function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Memories with Metadata\n",
    "\n",
    "When you store a memory, include rich metadata — timestamps, type labels, topic tags, and confidence scores. This metadata enables precise filtering during retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rich memory storage with metadata\n",
    "store.put(\n",
    "    user_id=current_user_id,\n",
    "    namespace=\"semantic\",\n",
    "    key=str(uuid.uuid4()),\n",
    "    value={\n",
    "        \"content\": \"User is preparing for AWS Solutions Architect exam\",\n",
    "        \"type\": \"semantic\",\n",
    "        \"source\": \"explicit_mention\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"topics\": [\"AWS\", \"certification\", \"career\"],\n",
    "        \"confidence\": 1.0\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Rich memory stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search for Relevant Memories\n",
    "\n",
    "Given the current conversation context, find memories that might be relevant. In production you'd use vector similarity; here we use keyword matching via `search_by_content` as a stand-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_memories(query: str, user_id: str, k: int = 5):\n",
    "    \"\"\"Retrieve memories semantically related to the query.\"\"\"\n",
    "    # In production, use vector similarity search\n",
    "    # Here we use keyword matching as a simplified stand-in\n",
    "    results = store.search_by_content(user_id, query, limit=k)\n",
    "    return [result[\"value\"] for result in results]\n",
    "\n",
    "print(\"get_relevant_memories defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recency and Relevance Weighting\n",
    "\n",
    "Pure semantic similarity isn't always optimal — a memory from yesterday is often more relevant than one from six months ago. This function combines recency weighting with retrieval, using a decay function tunable to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memories_with_recency(query: str, user_id: str):\n",
    "    \"\"\"Get memories weighted by both relevance and recency.\"\"\"\n",
    "    results = store.search(user_id, limit=20)  # Get more candidates for reranking\n",
    "\n",
    "    scored = []\n",
    "    now = datetime.now(UTC)\n",
    "\n",
    "    for r in results:\n",
    "        updated = r.get(\"updated_at\")\n",
    "        if updated:\n",
    "            timestamp = datetime.fromisoformat(updated)\n",
    "            if timestamp.tzinfo is None:\n",
    "                timestamp = timestamp.replace(tzinfo=UTC)\n",
    "        else:\n",
    "            timestamp = now\n",
    "        age_hours = (now - timestamp).total_seconds() / 3600\n",
    "        recency_weight = 1 / (1 + age_hours / 24)  # Decay over days\n",
    "        scored.append((recency_weight, r))\n",
    "\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [r[\"value\"] for _, r in scored[:5]]\n",
    "\n",
    "print(\"get_memories_with_recency defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Injection at Start\n",
    "\n",
    "The simplest integration pattern loads relevant memories at conversation start and injects them into the system prompt. The agent always has memory context available without needing to decide when to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_system_prompt(user_id: str, current_query: str):\n",
    "    \"\"\"Build a personalized system prompt with relevant memories.\"\"\"\n",
    "\n",
    "    memories = get_relevant_memories(current_query, user_id)\n",
    "\n",
    "    memory_context = \"\\n\".join([\n",
    "        f\"- {m.get('content', m)}\" for m in memories\n",
    "    ])\n",
    "\n",
    "    return f\"\"\"You are a helpful assistant.\n",
    "\n",
    "What you know about this user:\n",
    "{memory_context}\n",
    "\n",
    "Use this context to personalize your responses.\"\"\"\n",
    "\n",
    "print(\"build_system_prompt defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-Demand Retrieval\n",
    "\n",
    "An alternative to injection: give the agent a memory search tool and let it decide when to retrieve. More token-efficient, but requires the agent to recognize when memory would help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def recall_on_demand(topic: str) -> str:\n",
    "    \"\"\"\n",
    "    Search memory for information about a topic.\n",
    "    Use when you need to remember something about the user\n",
    "    or from past conversations.\n",
    "    \"\"\"\n",
    "    memories = store.search_by_content(current_user_id, topic, limit=5)\n",
    "\n",
    "    if not memories:\n",
    "        return \"No relevant memories found.\"\n",
    "\n",
    "    return \"\\n\".join([f\"- {m['value'].get('content', m['value'])}\" for m in memories])\n",
    "\n",
    "print(\"On-demand recall tool defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proactive Memory\n",
    "\n",
    "Proactive memory surfaces relevant context automatically based on the conversation, without being asked. In production, a relevance score threshold filters out weak matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proactive_memories(conversation_context: str, user_id: str):\n",
    "    \"\"\"Find memories the user might not ask about but would help.\"\"\"\n",
    "\n",
    "    related = store.search_by_content(user_id, conversation_context, limit=5)\n",
    "\n",
    "    # In production, only surface highly relevant memories (score > 0.75)\n",
    "    return [r[\"value\"] for r in related]\n",
    "\n",
    "print(\"get_proactive_memories defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Local PostgreSQL\n",
    "\n",
    "For production, you’d use PostgreSQL instead of SQLite. For a complete working example, see the Building StudyBuddy v6 section, which uses PostgreSQL for all memory persistence. On macOS, Homebrew makes installation simple:\n",
    "\n",
    "```bash\n",
    "# Install PostgreSQL\n",
    "brew install postgresql@16\n",
    "\n",
    "# Add to PATH (add this to your ~/.zshrc for persistence)\n",
    "export PATH=\"/opt/homebrew/opt/postgresql@16/bin:$PATH\"\n",
    "\n",
    "# If you add the path to ~/.zshrc, run the following or open a new terminal\n",
    "source ~/.zshrc\n",
    "\n",
    "# Start PostgreSQL as a background service\n",
    "brew services start postgresql@16\n",
    "```\n",
    "\n",
    "Create a database and verify:\n",
    "\n",
    "```bash\n",
    "# Create a database (use your app's name)\n",
    "createdb myapp\n",
    "\n",
    "# Connect to local PostgreSQL\n",
    "psql myapp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Schema Design\n",
    "\n",
    "The production schema includes tables for users, memories, sessions, and a content cache. The memories table uses the same namespace/key/value structure as our Python model. Paste the following SQL into your psql session to create the schema:\n",
    "\n",
    "```sql\n",
    "-- Users table with preferences\n",
    "CREATE TABLE users (\n",
    "    id VARCHAR(50) PRIMARY KEY,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    preferences JSON DEFAULT '{}'\n",
    ");\n",
    "\n",
    "-- Memories table for agent context (organized by namespace)\n",
    "CREATE TABLE memories (\n",
    "    id VARCHAR(36) PRIMARY KEY,\n",
    "    user_id VARCHAR(50) REFERENCES users(id) NOT NULL,\n",
    "    namespace VARCHAR(100) NOT NULL,\n",
    "    key VARCHAR(200) NOT NULL,\n",
    "    value JSON NOT NULL,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "CREATE INDEX ix_memory_user_namespace ON memories(user_id, namespace);\n",
    "\n",
    "-- Sessions table for conversation history\n",
    "CREATE TABLE sessions (\n",
    "    id VARCHAR(36) PRIMARY KEY,\n",
    "    user_id VARCHAR(50) REFERENCES users(id) NOT NULL,\n",
    "    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    ended_at TIMESTAMP,\n",
    "    summary TEXT,\n",
    "    metadata JSON DEFAULT '{}'\n",
    ");\n",
    "CREATE INDEX ix_session_user ON sessions(user_id);\n",
    "\n",
    "-- Generated content cache (content-addressed)\n",
    "CREATE TABLE content_cache (\n",
    "    id VARCHAR(36) PRIMARY KEY,\n",
    "    content_hash VARCHAR(64) UNIQUE NOT NULL,\n",
    "    content_type VARCHAR(50) NOT NULL,\n",
    "    content JSON NOT NULL,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    access_count INTEGER DEFAULT 0,\n",
    "    last_accessed TIMESTAMP\n",
    ");\n",
    "CREATE INDEX ix_cache_hash ON content_cache(content_hash);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring SQLAlchemy\n",
    "\n",
    "In production, SQLAlchemy connects to PostgreSQL via the `POSTGRES_URL` environment variable (e.g., `POSTGRES_URL=postgresql://localhost/myapp` in your `.env` file). The URL translation from `postgres://` to `postgresql://` handles a Vercel quirk. We wrap this in a factory function since we're using SQLite in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_production_engine():\n",
    "    \"\"\"Configure SQLAlchemy for production PostgreSQL (reference implementation).\"\"\"\n",
    "\n",
    "    POSTGRES_URL = os.environ.get(\"POSTGRES_URL\")\n",
    "\n",
    "    if not POSTGRES_URL:\n",
    "        raise RuntimeError(\n",
    "            \"POSTGRES_URL environment variable is required. \"\n",
    "            \"Set it in your .env file, e.g.: POSTGRES_URL=postgresql://localhost/myapp\"\n",
    "        )\n",
    "\n",
    "    # Vercel uses 'postgres://' but SQLAlchemy requires 'postgresql://'\n",
    "    DATABASE_URL = POSTGRES_URL.replace(\"postgres://\", \"postgresql://\", 1)\n",
    "\n",
    "    prod_engine = create_engine(\n",
    "        DATABASE_URL,\n",
    "        pool_pre_ping=True,\n",
    "        pool_size=5,\n",
    "        max_overflow=10,\n",
    "    )\n",
    "\n",
    "    ProdSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=prod_engine)\n",
    "    return prod_engine, ProdSessionLocal\n",
    "\n",
    "print(f\"Production factory defined (notebook using: {engine.url})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with SQLAlchemy Models\n",
    "\n",
    "The `Memory` model we defined earlier implements the namespace/key/value structure with a composite index on `(user_id, namespace)` for efficient lookups. Here's a summary of its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory model defined earlier — display its schema\n",
    "print(\"Memory table columns:\")\n",
    "for column in Memory.__table__.columns:\n",
    "    print(f\"  {column.name:15} {str(column.type):20} nullable={column.nullable}\")\n",
    "print(f\"\\nIndexes:\")\n",
    "for idx in Memory.__table__.indexes:\n",
    "    print(f\"  {idx.name}: ({', '.join(c.name for c in idx.columns)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Addressed Caching Strategies\n",
    "\n",
    "Content-addressed caching uses the input content itself as the cache key. Hash the input, use that hash to look up cached results. If the hash matches, return the stored result; if not, generate fresh content and store it under that hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_hash(content: str, params: dict) -> str:\n",
    "    \"\"\"Generate a deterministic hash for cache lookup.\"\"\"\n",
    "    cache_key_data = {\n",
    "        \"content\": content,\n",
    "        \"params\": params\n",
    "    }\n",
    "    key_string = json.dumps(cache_key_data, sort_keys=True)\n",
    "    return hashlib.sha256(key_string.encode()).hexdigest()\n",
    "\n",
    "print(\"get_content_hash defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database-Backed Performance Wins\n",
    "\n",
    "Your database doubles as a cache store. Check it before calling the LLM — cache hits are simple SELECT queries, orders of magnitude faster than generation. We define an `ExplanationCache` model and a stub generator for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplanationCache(Base):\n",
    "    __tablename__ = \"explanation_cache\"\n",
    "    id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))\n",
    "    content_hash = Column(String(64), unique=True, index=True)\n",
    "    topic = Column(String(200))\n",
    "    user_level = Column(String(50))\n",
    "    explanation = Column(String)\n",
    "    access_count = Column(Integer, default=0)\n",
    "    last_accessed = Column(DateTime)\n",
    "\n",
    "ExplanationCache.__table__.create(engine, checkfirst=True)\n",
    "\n",
    "def generate_explanation_with_llm(topic: str, context: str, user_level: str) -> str:\n",
    "    \"\"\"Stub: in production, this calls an LLM to generate an explanation.\"\"\"\n",
    "    return f\"{topic} ({user_level} level): {context}\"\n",
    "\n",
    "def get_or_generate_explanation(topic: str, context: str, user_level: str) -> str:\n",
    "    \"\"\"Return cached explanation if available, otherwise generate new one.\"\"\"\n",
    "\n",
    "    # Check cache first\n",
    "    content_hash = get_content_hash(topic + context, {\"level\": user_level})\n",
    "    cached = db_session.query(ExplanationCache).filter_by(content_hash=content_hash).first()\n",
    "\n",
    "    if cached:\n",
    "        cached.access_count += 1\n",
    "        cached.last_accessed = datetime.now(UTC)\n",
    "        db_session.commit()\n",
    "        print(\"Cache hit! Returning stored explanation.\")\n",
    "        return cached.explanation\n",
    "\n",
    "    # Cache miss - generate new explanation\n",
    "    print(\"Cache miss. Generating new explanation...\")\n",
    "    explanation = generate_explanation_with_llm(topic, context, user_level)\n",
    "\n",
    "    # Store for future requests\n",
    "    db_session.add(ExplanationCache(\n",
    "        id=str(uuid.uuid4()),\n",
    "        content_hash=content_hash,\n",
    "        topic=topic,\n",
    "        user_level=user_level,\n",
    "        explanation=explanation\n",
    "    ))\n",
    "    db_session.commit()\n",
    "\n",
    "    return explanation\n",
    "\n",
    "print(\"Caching functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Inputs for Cache Keys\n",
    "\n",
    "Your cache key must capture everything that affects the output — topic, source material, and parameters like expertise level. Miss any input and you'll serve stale or incorrect cached results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cache_key(topic: str, source_docs: list[str], params: dict) -> str:\n",
    "    \"\"\"Build a comprehensive cache key.\"\"\"\n",
    "\n",
    "    # Sort everything for deterministic ordering\n",
    "    key_components = {\n",
    "        \"topic\": topic,\n",
    "        \"sources\": sorted(source_docs),\n",
    "        \"params\": dict(sorted(params.items()))\n",
    "    }\n",
    "\n",
    "    key_string = json.dumps(key_components, sort_keys=True)\n",
    "    return hashlib.sha256(key_string.encode()).hexdigest()\n",
    "\n",
    "print(\"build_cache_key defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Generated Outputs\n",
    "\n",
    "Store generated content with enough metadata to be useful later — when it was created, what inputs produced it, access counts, and content type. The access tracking helps identify popular content worth keeping and stale content worth evicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedContent(Base):\n",
    "    __tablename__ = \"generated_content\"\n",
    "\n",
    "    id = Column(String, primary_key=True)\n",
    "    content_hash = Column(String, unique=True, index=True)\n",
    "    content_type = Column(String)  # \"explanation\", \"summary\", \"extraction\", etc.\n",
    "    content = Column(JSON)\n",
    "    input_summary = Column(String)  # For debugging\n",
    "    created_at = Column(DateTime, default=lambda: datetime.now(UTC))\n",
    "    access_count = Column(Integer, default=0)\n",
    "    last_accessed = Column(DateTime)\n",
    "\n",
    "GeneratedContent.__table__.create(engine, checkfirst=True)\n",
    "print(\"GeneratedContent model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance Improvements\n",
    "\n",
    "Instrument your cache with metrics — track hits vs. misses, measure latency savings, and calculate cost savings from avoided LLM calls. Good caching often yields 60-80% hit rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheMetrics:\n",
    "    def __init__(self):\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.total_latency_saved_ms = 0\n",
    "        self.estimated_cost_saved = 0.0\n",
    "\n",
    "    def record_hit(self, latency_saved_ms: int, cost_saved: float):\n",
    "        self.hits += 1\n",
    "        self.total_latency_saved_ms += latency_saved_ms\n",
    "        self.estimated_cost_saved += cost_saved\n",
    "\n",
    "    def record_miss(self):\n",
    "        self.misses += 1\n",
    "\n",
    "    @property\n",
    "    def hit_rate(self) -> float:\n",
    "        total = self.hits + self.misses\n",
    "        return self.hits / total if total > 0 else 0.0\n",
    "\n",
    "metrics = CacheMetrics()\n",
    "print(\"CacheMetrics ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "Let's tie the major systems together: store memories about a user, retrieve them with different strategies, build a personalized system prompt, get a personalized LLM response, and demonstrate caching with metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Store memories ---\n",
    "print(\"=\" * 60)\n",
    "print(\"1. STORING MEMORIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "store.put(current_user_id, \"facts\", \"name\", {\"content\": \"Alex Chen\"})\n",
    "store.put(current_user_id, \"goals\", \"certification\",\n",
    "          {\"content\": \"Preparing for AWS Solutions Architect exam\",\n",
    "           \"timestamp\": datetime.now(UTC).isoformat()})\n",
    "store.put(current_user_id, \"episodes\", \"session_002\",\n",
    "          {\"content\": \"Studied S3 bucket policies and IAM roles\",\n",
    "           \"timestamp\": datetime.now(UTC).isoformat(),\n",
    "           \"outcome\": \"Solid on S3, needs more work on IAM\"})\n",
    "print(\"Memories stored!\\n\")\n",
    "\n",
    "# --- 2. Use the remember tool ---\n",
    "print(\"=\" * 60)\n",
    "print(\"2. USING THE REMEMBER TOOL\")\n",
    "print(\"=\" * 60)\n",
    "result = remember.invoke({\"namespace\": \"preferences\", \"key\": \"study_time\",\n",
    "                          \"fact\": \"Prefers studying in the morning\"})\n",
    "print(result, \"\\n\")\n",
    "\n",
    "# --- 3. Search memories ---\n",
    "print(\"=\" * 60)\n",
    "print(\"3. SEARCHING MEMORIES\")\n",
    "print(\"=\" * 60)\n",
    "all_memories = store.search(current_user_id)\n",
    "print(f\"Total memories: {len(all_memories)}\")\n",
    "for m in all_memories[:5]:\n",
    "    print(f\"  [{m['namespace']}] {m['key']}: {m['value'].get('content', '')[:60]}\")\n",
    "\n",
    "print(f\"\\nKeyword search for 'AWS':\")\n",
    "aws_memories = store.search_by_content(current_user_id, \"AWS\")\n",
    "for m in aws_memories:\n",
    "    print(f\"  {m['key']}: {m['value'].get('content', '')[:60]}\")\n",
    "\n",
    "# --- 4. Use the recall tool ---\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"4. USING THE RECALL TOOL\")\n",
    "print(\"=\" * 60)\n",
    "print(recall.invoke({\"namespace\": \"goals\"}))\n",
    "\n",
    "# --- 5. Recency-weighted retrieval ---\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"5. RECENCY-WEIGHTED RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "recent = get_memories_with_recency(\"AWS\", current_user_id)\n",
    "for m in recent:\n",
    "    print(f\"  {m.get('content', m)[:70]}\")\n",
    "\n",
    "# --- 6. Build personalized system prompt ---\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"6. PERSONALIZED SYSTEM PROMPT\")\n",
    "print(\"=\" * 60)\n",
    "prompt = build_system_prompt(current_user_id, \"AWS\")\n",
    "print(prompt)\n",
    "\n",
    "# --- 7. Get a personalized LLM response ---\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"7. PERSONALIZED LLM RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "response = llm.invoke([\n",
    "    {\"role\": \"system\", \"content\": prompt},\n",
    "    {\"role\": \"user\", \"content\": \"What should I focus on next for my exam prep?\"}\n",
    "])\n",
    "print(response.content)\n",
    "\n",
    "# --- 8. Caching demonstration ---\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"8. CACHING DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "exp1 = get_or_generate_explanation(\"AWS S3\", \"Object storage service for the cloud\", \"intermediate\")\n",
    "print(f\"First call result: {exp1}\")\n",
    "metrics.record_miss()\n",
    "\n",
    "exp2 = get_or_generate_explanation(\"AWS S3\", \"Object storage service for the cloud\", \"intermediate\")\n",
    "print(f\"Second call result: {exp2}\")\n",
    "metrics.record_hit(latency_saved_ms=1500, cost_saved=0.01)\n",
    "\n",
    "print(f\"\\nCache hit rate: {metrics.hit_rate:.0%}\")\n",
    "print(f\"Latency saved: {metrics.total_latency_saved_ms}ms\")\n",
    "print(f\"Cost saved: ${metrics.estimated_cost_saved:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-bootcamp-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
