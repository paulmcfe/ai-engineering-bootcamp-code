{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbb92e6",
   "metadata": {},
   "source": [
    "# Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Call the API\n",
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=\"Embed this string, please.\"\n",
    ")\n",
    "\n",
    "# Extract the vector from the response\n",
    "embedding = response.data[0].embedding\n",
    "\n",
    "# len() returns the number of dimensions\n",
    "print(f\"Embedding dimensions: {len(embedding)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b62ad0",
   "metadata": {},
   "source": [
    "# Building a Custom Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "class SimpleVectorDB:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add(self, vector: List[float], metadata: dict):\n",
    "        \"\"\"Add a vector and its metadata to the database\"\"\"\n",
    "        self.vectors.append(np.array(vector))\n",
    "        self.metadata.append(metadata)\n",
    "    \n",
    "    def search(self, query_vector: List[float], k: int = 5) -> List[Tuple[dict, float]]:\n",
    "        \"\"\"Search for the k most similar vectors\"\"\"\n",
    "        query = np.array(query_vector)\n",
    "        \n",
    "        # Calculate cosine similarity for all vectors\n",
    "        similarities = []\n",
    "        for i, vec in enumerate(self.vectors):\n",
    "            similarity = self._cosine_similarity(query, vec)\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (highest first) and get top k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_k = similarities[:k]\n",
    "        \n",
    "        # Return metadata and scores\n",
    "        results = [(self.metadata[idx], score) for idx, score in top_k]\n",
    "        return results\n",
    "    \n",
    "    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        return dot_product / (norm1 * norm2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b92b9c",
   "metadata": {},
   "source": [
    "# Distance Metrics\n",
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Example usage\n",
    "vec_a = np.array([1.0, 2.0, 3.0])\n",
    "vec_b = np.array([2.0, 4.0, 6.0])\n",
    "similarity = cosine_similarity(vec_a, vec_b)\n",
    "print(f\"Cosine similarity: {similarity:.4f}\")  # Output: 1.0000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b664e",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8aa3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calculate Euclidean distance between two vectors\"\"\"\n",
    "    return np.sqrt(np.sum((vec1 - vec2) ** 2))\n",
    "\n",
    "# Example usage\n",
    "vec_a = np.array([1.0, 2.0, 3.0])\n",
    "vec_b = np.array([4.0, 5.0, 6.0])\n",
    "distance = euclidean_distance(vec_a, vec_b)\n",
    "print(f\"Euclidean distance: {distance:.4f}\")\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = 1 / (1 + distance)\n",
    "print(f\"Similarity: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9e9f4",
   "metadata": {},
   "source": [
    "## Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff642e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calculate dot product between two vectors\"\"\"\n",
    "    return np.dot(vec1, vec2)\n",
    "\n",
    "# Example usage\n",
    "vec_a = np.array([1.0, 2.0, 3.0])\n",
    "vec_b = np.array([2.0, 4.0, 6.0])\n",
    "product = dot_product(vec_a, vec_b)\n",
    "print(f\"Dot product: {product:.4f}\")\n",
    "\n",
    "# For normalized vectors:\n",
    "normalized_a = vec_a / np.linalg.norm(vec_a)\n",
    "normalized_b = vec_b / np.linalg.norm(vec_b)\n",
    "# This equals cosine similarity\n",
    "product_normalized = dot_product(normalized_a, normalized_b)\n",
    "print(f\"Normalized dot product: {product_normalized:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c52e5",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504191c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three metrics\n",
    "vec_a = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "vec_b = np.array([1.5, 2.5, 3.5, 4.5])\n",
    "\n",
    "cos_sim = cosine_similarity(vec_a, vec_b)\n",
    "euc_dist = euclidean_distance(vec_a, vec_b)\n",
    "dot_prod = dot_product(vec_a, vec_b)\n",
    "\n",
    "print(f\"Cosine Similarity: {cos_sim:.4f}\")\n",
    "print(f\"Euclidean Distance: {euc_dist:.4f}\")\n",
    "print(f\"Dot Product: {dot_prod:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704dc04",
   "metadata": {},
   "source": [
    "# Fixed-Size vs Semantic Chunking\n",
    "## Fixed-Size Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0158ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_size_chunking(text: str, chunk_size: int = 500,\n",
    "                        overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Split text into fixed-size chunks with overlap\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c838eb",
   "metadata": {},
   "source": [
    "## Semantic Chunking (Paragraph Boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39344ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "def semantic_chunking(text: str, max_chunk_size: int = 1000) -> List[str]:\n",
    "    \"\"\"Split text into semantic chunks at paragraph boundaries\"\"\"\n",
    "    # Split on double newlines (paragraph breaks)\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph = paragraph.strip()\n",
    "        if not paragraph:\n",
    "            continue\n",
    "        \n",
    "        para_size = len(paragraph.split())\n",
    "        \n",
    "        # If adding this paragraph exceeds max size, start new chunk\n",
    "        if current_size + para_size > max_chunk_size and current_chunk:\n",
    "            chunks.append('\\n\\n'.join(current_chunk))\n",
    "            current_chunk = [paragraph]\n",
    "            current_size = para_size\n",
    "        else:\n",
    "            current_chunk.append(paragraph)\n",
    "            current_size += para_size\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n\\n'.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "document = \"\"\"First paragraph about topic A.\n",
    "\n",
    "Second paragraph also about topic A.\n",
    "\n",
    "Third paragraph about topic B. \n",
    "\n",
    "Fourth paragraph about topic A.\n",
    "\n",
    "Fifth paragraph also about topic A.\n",
    "\n",
    "Sixth paragraph about topic B.\"\"\"\n",
    "\n",
    "chunks = semantic_chunking(document, max_chunk_size=20)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {len(chunk.split())} words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9209c0d",
   "metadata": {},
   "source": [
    "## Semantic Chunking (Section Boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sections(text: str, header_pattern: str = r'^#{1,3} ') -> List[str]:\n",
    "    \"\"\"Chunk markdown documents by section headers\"\"\"\n",
    "    import re\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Check if line is a header\n",
    "        if re.match(header_pattern, line):\n",
    "            # Save previous chunk if it exists\n",
    "            if current_chunk:\n",
    "                chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = [line]\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be906e",
   "metadata": {},
   "source": [
    "# Building RAG From First Principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.vector_db = SimpleVectorDB()\n",
    "        self.embedding_model = \"text-embedding-3-small\"\n",
    "        self.llm_model = \"gpt-5-mini\"\n",
    "    \n",
    "    def index_documents(self, documents: List[str], chunk_size: int = 500,\n",
    "                        overlap: int = 50):\n",
    "        \"\"\"Index documents by chunking and embedding them\"\"\"\n",
    "        print(f\"Indexing {len(documents)} documents...\")\n",
    "        \n",
    "        for doc_id, document in enumerate(documents):\n",
    "            # Chunk the document\n",
    "            chunks = self._chunk_text(document, chunk_size, overlap)\n",
    "            \n",
    "            # Embed each chunk\n",
    "            for chunk_id, chunk in enumerate(chunks):\n",
    "                embedding = self._get_embedding(chunk)\n",
    "                \n",
    "                # Store in vector database\n",
    "                metadata = {\n",
    "                    'doc_id': doc_id,\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'text': chunk\n",
    "                }\n",
    "                self.vector_db.add(embedding, metadata)\n",
    "        \n",
    "        print(f\"Indexed {len(self.vector_db.vectors)} chunks\")\n",
    "    \n",
    "    def query(self, question: str, k: int = 5) -> str:\n",
    "        \"\"\"Query the RAG system with a question\"\"\"\n",
    "        # Get embedding for the question\n",
    "        question_embedding = self._get_embedding(question)\n",
    "        \n",
    "        # Retrieve most similar chunks\n",
    "        results = self.vector_db.search(question_embedding, k=k)\n",
    "        \n",
    "        # Build context from retrieved chunks\n",
    "        context = \"\\n\\n\".join([result[0]['text'] for result in results])\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Get response from LLM\n",
    "        response = self.client.responses.create(\n",
    "            model=self.llm_model,\n",
    "            input=prompt,\n",
    "        )\n",
    "        \n",
    "        return response.output_text\n",
    "    \n",
    "    def _get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Get embedding from OpenAI\"\"\"\n",
    "        response = self.client.embeddings.create(\n",
    "            model=self.embedding_model,\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def _chunk_text(self, text: str, chunk_size: int,\n",
    "                    overlap: int) -> List[str]:\n",
    "        \"\"\"Split text into overlapping chunks\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            if chunk:  # Only add non-empty chunks\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "# Initialize the system\n",
    "rag = RAGSystem()\n",
    "\n",
    "# Index some documents\n",
    "documents = [\n",
    "    \"\"\"Python is a high-level, general-purpose programming language used for a vast range of applications, including\n",
    "web development, data science, artificial intelligence (AI), machine learning, and automation. Its versatility and beginner-friendly, readable syntax have made it one of the most popular languages in the world. \n",
    "Key uses for Python include:\n",
    "\n",
    "    Data Analysis and Machine Learning Python has become a staple in data science due to its extensive libraries (like NumPy, pandas, and TensorFlow), which enable complex statistical calculations, data visualization, and the building of machine learning algorithms.\n",
    "    Web Development It is often used for the back-end (server-side) of websites and applications, processing logic, interacting with databases, and ensuring security. Companies like Google, Netflix, and Instagram use Python for their services.\n",
    "    Automation and Scripting Python can automate repetitive tasks, such as renaming files, finding and downloading online content, or processing errors across multiple files. This makes work more efficient for both programmers and non-programmers.\n",
    "    Software Testing and Prototyping In software development, Python aids in build control, bug tracking, and automating tests for new products or features, and its fast prototyping capabilities are valuable for startups.\n",
    "    Scientific Computing and Research Python's robust libraries are used extensively in academic research across various fields, including bioinformatics, biology, and mathematics.\n",
    "    Embedded Systems and Gaming While not its primary use, Python can be found in embedded systems and graphic design applications (e.g., as a scripting language for Maya, a 3D modeling tool). \n",
    "\n",
    "Python's design emphasizes code readability and simplicity, which contributes to its wide adoption across various industries and job functions, from developers and software engineers to data analysts and AI researchers.\"\"\",\n",
    "    \"\"\"Machine learning (ML) is a subset of artificial intelligence focused on building systems that learn patterns from data and then use those patterns to make predictions, decisions, or recommendations. Instead of writing explicit rules for every scenario, you provide examples (data) and a learning algorithm finds a model that generalizes beyond those examples.\n",
    "\n",
    "A typical ML workflow includes collecting and cleaning data, splitting it into training/validation/test sets, selecting features (or learning them automatically), training a model, evaluating it with appropriate metrics, and then deploying it to make predictions on new inputs. A key challenge is avoiding overfitting—when a model performs well on training data but poorly on new data—so techniques like regularization, cross-validation, and careful monitoring are essential.\n",
    "\n",
    "Common learning paradigms include:\n",
    "    Supervised learning: learn from labeled examples (e.g., spam detection, price prediction).\n",
    "    Unsupervised learning: discover structure without labels (e.g., clustering customers, dimensionality reduction).\n",
    "    Reinforcement learning: learn by trial and error with rewards (e.g., game-playing agents, robotics).\n",
    "Deep learning is a family of ML methods (neural networks with many layers) that has been especially successful in vision, speech, and language tasks.\n",
    "\n",
    "ML is used across many industries: recommendation systems, search ranking, fraud detection, anomaly detection, medical imaging, predictive maintenance, and personalization. In practice, ML success depends as much on data quality, problem framing, and evaluation as on the specific algorithm, and teams must also consider issues like bias, data leakage, privacy, and ongoing model drift after deployment.\"\"\",\n",
    "    \"\"\"Vector databases store and search high-dimensional vector embeddings—numeric representations of text, images, audio, or other data produced by embedding models. The core idea is that “similar” items end up close together in vector space, allowing semantic search: you can retrieve relevant content even when it doesn’t share the same exact keywords as the query.\n",
    "\n",
    "A vector database typically supports:\n",
    "    Ingestion: store an embedding plus an ID and metadata (source, timestamp, tags, etc.).\n",
    "    Similarity search: find the nearest neighbors to a query vector using metrics like cosine similarity, dot product, or Euclidean distance.\n",
    "    Indexing for speed: use approximate nearest neighbor (ANN) structures (e.g., HNSW or IVF-based indexes) to search efficiently at scale, trading a small amount of accuracy for large performance gains.\n",
    "    Metadata filtering: constrain results by structured fields (e.g., only documents from a certain product, user, or date range).\n",
    "Hybrid approaches often combine semantic retrieval (vectors) with lexical retrieval (BM25/keyword search) to improve precision for proper nouns, codes, and rare terms.\n",
    "\n",
    "Vector databases are widely used in Retrieval-Augmented Generation (RAG) systems: documents are chunked, embedded, and stored; at query time the system embeds the question, retrieves the most similar chunks, and provides them as grounded context to a language model. This reduces hallucinations and enables answering questions using up-to-date or proprietary knowledge. Practical considerations include chunking strategy, embedding model choice, deduplication, index rebuild/update behavior, and evaluation of retrieval quality (recall, precision, latency, and cost).\"\"\",\n",
    " ]\n",
    "\n",
    "rag.index_documents(documents)\n",
    "\n",
    "# Query the system\n",
    "question = \"What is Python used for?\"\n",
    "answer = rag.query(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
